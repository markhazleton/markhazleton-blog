extends ../layouts/modern-layout

block pagehead
  title AI Observability Is No Joke
  meta(name='description', content='A  humorous look at AI observability through the lens of a joke-fetching agent. Learn why knowing what your AI did matters.')
  meta(name='keywords', content='AI, observability, debugging, agent, logging, best practices, transparency, accountability, Mark Hazleton')
  meta(name='author', content='Mark Hazleton')

block canonical
  link(rel='canonical', href='https://markhazleton.com/articles/ai-observability-is-no-joke.html')

block og_overrides
  meta(property='og:title', content='AI Observability Is No Joke')
  meta(property='og:description', content='A  humorous look at AI observability through the lens of a joke-fetching agent. Learn why knowing what your AI did matters.')
  meta(property='og:url', content='https://markhazleton.com/articles/ai-observability-is-no-joke.html')
  meta(property='og:type', content='article')
  meta(property='og:video', content='https://www.youtube.com/embed/j5Hm-iceT_M')
  meta(property='og:video:type', content='text/html')
  meta(property='og:video:width', content='560')
  meta(property='og:video:height', content='315')
  meta(property='og:video:url', content='https://www.youtube.com/embed/j5Hm-iceT_M')
  meta(property='og:video:secure_url', content='https://www.youtube.com/embed/j5Hm-iceT_M')
  meta(property='og:image', content='https://img.youtube.com/vi/j5Hm-iceT_M/maxresdefault.jpg')
  meta(property='og:image:alt', content='AI Observability Is No Joke - Video thumbnail showing discussion about AI agent transparency')

block twitter_overrides
  meta(name='twitter:title', content='AI Observability Is No Joke')
  meta(name='twitter:description', content='A  humorous look at AI observability through the lens of a joke-fetching agent. Learn why knowing what your AI did matters.')
  meta(name='twitter:card', content='player')
  meta(name='twitter:player', content='https://www.youtube.com/embed/j5Hm-iceT_M')
  meta(name='twitter:player:width', content='560')
  meta(name='twitter:player:height', content='315')
  meta(name='twitter:image', content='https://img.youtube.com/vi/j5Hm-iceT_M/maxresdefault.jpg')
  meta(name='twitter:image:alt', content='AI Observability Is No Joke - Video thumbnail showing discussion about AI agent transparency')

block layout-content
  br

  // Hero Section
  section.bg-gradient-primary.py-5
    .container
      .row.align-items-center
        .col-lg-10.mx-auto.text-center
          h1.display-4.fw-bold.mb-3
            i.bi.bi-lightbulb.me-3 AI Observability Is No Joke
          p.lead.mb-4.
            A humorous look at AI observability through the lens of a joke-fetching agent.
            Learn why knowing what your AI did matters.

          // Featured Video
          .card.mb-4.shadow-sm
            .card-header.bg-dark.text-white
              h5.card-title.mb-0
                i.bi.bi-play-circle.me-2
                | Watch: AI Observability Deep Dive
            .card-body.p-0
              .ratio.ratio-16x9
                iframe(
                  src="https://www.youtube.com/embed/j5Hm-iceT_M?si=edToyAVbaG0FX_Fs"
                  title="AI Observability Is No Joke - Deep Dive Discussion"
                  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                  referrerpolicy="strict-origin-when-cross-origin"
                  allowfullscreen
                )
            .card-footer.text-muted
              small
                i.bi.bi-info-circle.me-1
                | Discover the challenges and solutions for AI agent transparency

          // Article Metadata
          .mb-4.text-muted.d-flex.justify-content-center.align-items-center.gap-4
            span
              i.bi.bi-person.me-1
              | Mark Hazleton
            span
              i.bi.bi-calendar.me-1
              | June 16, 2025
            span
              i.bi.bi-tag.me-1
              | AI, Observability

  // Main Article Content
  article#main-article
    .container
      .row
        .col-lg-9.mx-auto

          // Article Content Section
          section#article-content.mb-5

            // Introduction
            .alert.alert-info.mb-4
              p.mb-0
                i.bi.bi-info-circle.me-2
                strong What you'll learn:
                | How a simple AI joke request revealed critical observability gaps, why transparency matters in AI systems, and practical steps to implement better monitoring in your AI agents.

            h2.h3.mb-3.text-primary.d-flex.align-items-center
              i.bi.bi-emoji-laughing.me-2
              | A funny request turned into a lesson on AI observability
            p.lead.mb-4.
              I thought I was just asking for a joke.
              Turns out, I stumbled into one of the challenges facing AI development today.
              And honestly? Here's what unfoldedâ€”and why it matters.
            p.mb-3.
              Picture this: I'm tinkering with my latest AI agent setup, feeling pretty proud of myself.
              I've got my simple MCP (Model Context Protocol) server running, connected to jokeapi.com,
              ready to fetch the freshest jokes from the internet.
              My agent is configured, the tools are registered,
              everything looks perfect.
              So I typed: "Hey agent, tell me a joke."
            blockquote.blockquote.bg-light.p-3.border-start.border-4.border-primary.mb-3
              i.bi.bi-quote.me-2.text-primary
              | "Why don't scientists trust atoms? Because they make up everything!"
            p.mb-3.
              I chuckle. Mission accomplished, right? My agent used the API, grabbed a joke,
              delivered the goods. Time to pat myself on the back and move on to the next project.
              But then that little voice in my head started whispering. "Just where did that joke come from?"
            p.mb-3.
              I mean, sure, my agent said it fetched it from jokeapi.com.
              But did it really? Or did it just pull that gem from the vast
              repository of dad jokes floating around in its training data? How would I even know?
            p.mb-3.
              This is when I realized that while I had written the code, I had no idea of how it worked.
              Everything looks fine from the outside, but I had no clue as to what was happening under the hood.
            p.mb-3.
              The agent could claim it called the API.
              It could even format its response to look like it came from an external source.
              But proving it actually made that HTTP request? That was surprisingly difficult.
            p.mb-3.
              Trying to answer the question of what was going on, I decided to dig deeper.
              I started looking at my logs,
              my network traffic, my agent's behavior patterns. And what I found was...
              well, not much. My fancy AI agent was essentially a black box wrapped in promises.

            h2.h4.mb-3.text-info.d-flex.align-items-center
              i.bi.bi-eye.me-2
              | The Bigger Picture
            p.mb-3.
              This seemingly simple joke request opened my eyes to a massive problem in AI agent development:
              observability. Not the most exciting word, I'll admit, but stick with me here.
            p.mb-3.
              Here's the thing about black boxes - they're designed to hide their internal workings.
              You feed something in, you get something out, but what happens in between?
              That's the mystery. My AI agent had become this opaque system where I could see the inputs
              and outputs, but the actual processing, the decision-making, the tool usage?
              All hidden from view.
            p.mb-3.
              If you've ever worked in Quality Assurance,
              you'll recognize this dilemma immediately.
              In traditional software testing, we have two fundamental approaches:
            dl
              dt Black Box Testing
              dd.
                Black box testing focuses solely on inputs and outputs - you don't care how the code works internally,
                you just verify that when you put X in, you get Y out.
                It's perfect for testing user experiences and catching obvious bugs.

              dt White Box Testing
              dd.
                Where you examine the internal structure, the code paths, the decision logic.
                You can see exactly which functions are called, which branches are taken,
                which variables are modified.
                This visibility is crucial for catching edge cases, verifying security measures,
                and understanding performance bottlenecks.

            p.mb-3.
              The problem with AI agents? We're stuck in permanent black box mode. Even when we write the orchestration code ourselves, the AI's decision-making process remains opaque. We can't step through its reasoning, can't see which tools it actually invokes versus which ones it pretends to use, can't verify its internal logic.
            p.mb-3.
              The agent could claim it called the API. It could even format its response to look like it came from an external source. But proving it actually made that HTTP request? That was surprisingly difficult. I was staring at a black box that could be doing anything inside - fetching fresh jokes, recycling old ones, or making up entirely new material - and I had no way to peek inside and verify.
            p.mb-3.
              The more I thought about this, the funnier (and scarier) it became. This black box could be operating in countless ways I couldn't see, and I started imagining all the different internal processes that might be happening without my knowledge...
            p.mb-3.
              Imagine these scenarios:
            ul.list-group.mb-4
              li.list-group-item.d-flex.align-items-center
                i.bi.bi-rocket-takeoff.me-2.text-primary
                strong.me-2 The Overachiever:
                | Calls five APIs when one would do.
              li.list-group-item.d-flex.align-items-center
                i.bi.bi-moon.me-2.text-secondary
                strong.me-2 The Lazy Agent:
                | Fakes it with old data.
              li.list-group-item.d-flex.align-items-center
                i.bi.bi-question-circle.me-2.text-info
                strong.me-2 The Confused Agent:
                | Gives up silently but still logs success.
              li.list-group-item.d-flex.align-items-center
                i.bi.bi-person-gear.me-2.text-success
                strong.me-2 The Identity Crisis Agent:
                | Thinks it made a callâ€¦ but didnâ€™t.

            h2.h4.mb-3.text-danger.d-flex.align-items-center
              i.bi.bi-exclamation-triangle.me-2
              | Why This Matters
            p.mb-3.
              This isn't just about jokes or my particular brand of overthinking.
              As AI agents become more sophisticated and handle more critical tasks,
              this observability gap becomes genuinely problematic:
            dl.row.mb-4
              dt.col-sm-3.fw-bold.d-flex.align-items-center
                i.bi.bi-people.me-2.text-primary
                | Customer Service:
              dd.col-sm-9.mb-2
                | Did the agent actually check your account status, or is it giving you a generic response?
              dt.col-sm-3.fw-bold.d-flex.align-items-center
                i.bi.bi-currency-dollar.me-2.text-success
                | Financial Applications:
              dd.col-sm-9.mb-2
                | When the agent says it's pulling real-time market data, is it really, or are you making decisions based on stale information?
              dt.col-sm-3.fw-bold.d-flex.align-items-center
                i.bi.bi-heart-pulse.me-2.text-danger
                | Healthcare:
              dd.col-sm-9.mb-2
                | If an AI assistant claims to have checked the latest research, you'd better hope it actually did.
              dt.col-sm-3.fw-bold.d-flex.align-items-center
                i.bi.bi-code-slash.me-2.text-info
                | Development:
              dd.col-sm-9.mb-2
                | When your coding assistant says it's following best practices from the latest documentation, did it actually access that documentation or just wing it?
            p.mb-3.
              AI agents touch everythingâ€”from customer service to healthcare. We must know what they actually do, not just what they say they do.

            p.mb-3.
              Agents chain actions across tools and APIs. Without logs and proofs, it's a game of telephone with hallucinations.

            h2.h4.mb-3.text-primary.d-flex.align-items-center
              i.bi.bi-shield-check.me-2
              | Observability Best Practices
            p.mb-3.
              So, what can we do? Here are some best practices I'm adopting to ensure my AI agents are more transparent and accountable:
            ul.list-group.mb-4
              li.list-group-item.d-flex.align-items-center
                i.bi.bi-patch-check.me-2.text-success
                strong.me-2 Trust, but Verify:
                | Don't just askâ€”prove it.
              li.list-group-item.d-flex.align-items-center
                i.bi.bi-journal-text.me-2.text-info
                strong.me-2 Embrace Logs:
                | Log intent, attempts, and failures.
              li.list-group-item.d-flex.align-items-center
                i.bi.bi-bounding-box.me-2.text-warning
                strong.me-2 Test Boundaries:
                | Disable tools and see if the agent notices.
              li.list-group-item.d-flex.align-items-center
                i.bi.bi-tools.me-2.text-primary
                strong.me-2 Build for Observability:
                | Make it foundational, not an afterthought.
            p.mb-3.
              That joke? Still funny. But if my agent were making business decisions, I'd want proof. The joke's on us if we don't demand it.

            h2.h4.mb-3.text-secondary.d-flex.align-items-center
              i.bi.bi-book.me-2
              | Model Context Protocol (MCP) Glossary
            p.mb-3.
              Since MCP is central to this discussion, here's a comprehensive glossary of key terms and concepts
              that will help you understand how this protocol enables AI agent observability.

            .accordion#mcpGlossary.mb-5
              .accordion-item
                h3.accordion-header#headingMCP
                  button.accordion-button.collapsed(
                    type='button'
                    data-bs-toggle='collapse'
                    data-bs-target='#collapseMCP'
                    aria-expanded='false'
                    aria-controls='collapseMCP'
                  )
                    i.bi.bi-diagram-3.me-2
                    | Model Context Protocol (MCP)
                .accordion-collapse.collapse#collapseMCP(
                  aria-labelledby='headingMCP'
                  data-bs-parent='#mcpGlossary'
                )
                  .accordion-body
                    p.
                      MCP is an open standard that enables secure, bidirectional communication between AI language models
                      and external data sources and tools. Think of it as a universal translator that allows AI agents
                      to safely interact with your databases, APIs, file systems, and other resources without exposing
                      sensitive information or compromising security. The protocol standardizes how AI systems request
                      access to resources, how they authenticate, and how data flows between the AI and external systems.

              .accordion-item
                h3.accordion-header#headingServer
                  button.accordion-button.collapsed(
                    type='button'
                    data-bs-toggle='collapse'
                    data-bs-target='#collapseServer'
                    aria-expanded='false'
                    aria-controls='collapseServer'
                  )
                    i.bi.bi-server.me-2
                    | MCP Server
                .accordion-collapse.collapse#collapseServer(
                  aria-labelledby='headingServer'
                  data-bs-parent='#mcpGlossary'
                )
                  .accordion-body
                    p.
                      An MCP server is a standalone program that exposes specific capabilities (tools, resources, or prompts)
                      to AI clients through the MCP protocol. Servers act as secure gateways between AI agents and external
                      systems. For example, a database MCP server might provide read-only access to customer records,
                      while a file system server could allow an AI to read documentation files. Each server defines what
                      operations are allowed and implements appropriate security controls and logging.

              .accordion-item
                h3.accordion-header#headingClient
                  button.accordion-button.collapsed(
                    type='button'
                    data-bs-toggle='collapse'
                    data-bs-target='#collapseClient'
                    aria-expanded='false'
                    aria-controls='collapseClient'
                  )
                    i.bi.bi-cpu.me-2
                    | MCP Client
                .accordion-collapse.collapse#collapseClient(
                  aria-labelledby='headingClient'
                  data-bs-parent='#mcpGlossary'
                )
                  .accordion-body
                    p.
                      The MCP client is typically an AI application or agent that connects to one or more MCP servers to
                      access external capabilities. Clients discover available tools and resources, make requests to servers,
                      and handle responses. Popular AI platforms like Claude Desktop, ChatGPT Plus, and custom AI applications
                      can act as MCP clients. The client is responsible for managing connections, handling authentication,
                      and presenting available tools to the AI model in a format it can understand and use.

              .accordion-item
                h3.accordion-header#headingTools
                  button.accordion-button.collapsed(
                    type='button'
                    data-bs-toggle='collapse'
                    data-bs-target='#collapseTools'
                    aria-expanded='false'
                    aria-controls='collapseTools'
                  )
                    i.bi.bi-tools.me-2
                    | Tools
                .accordion-collapse.collapse#collapseTools(
                  aria-labelledby='headingTools'
                  data-bs-parent='#mcpGlossary'
                )
                  .accordion-body
                    p.
                      In MCP terminology, tools are specific functions or capabilities that an MCP server exposes to AI clients.
                      Tools define what actions an AI can performâ€”like querying a database, calling an API, or processing a file.
                      Each tool has a defined schema that specifies required parameters, expected inputs, and output formats.
                      Tools are stateless and designed to be called multiple times safely. Examples include "search_database",
                      "send_email", "create_calendar_event", or "fetch_weather_data". The tool abstraction allows AI agents
                      to understand and use complex external systems through simple, well-defined interfaces.

              .accordion-item
                h3.accordion-header#headingResources
                  button.accordion-button.collapsed(
                    type='button'
                    data-bs-toggle='collapse'
                    data-bs-target='#collapseResources'
                    aria-expanded='false'
                    aria-controls='collapseResources'
                  )
                    i.bi.bi-folder.me-2
                    | Resources
                .accordion-collapse.collapse#collapseResources(
                  aria-labelledby='headingResources'
                  data-bs-parent='#mcpGlossary'
                )
                  .accordion-body
                    p.
                      Resources in MCP represent data or content that AI agents can access through the protocol. Unlike tools
                      (which perform actions), resources provide information. They can be static (like configuration files)
                      or dynamic (like real-time sensor data). Resources have URIs for identification and can include metadata
                      about their content type, size, and freshness. Examples include log files, documentation, database snapshots,
                      or API responses. Resources enable AI agents to access contextual information needed to make informed
                      decisions or provide accurate responses.

              .accordion-item
                h3.accordion-header#headingPrompts
                  button.accordion-button.collapsed(
                    type='button'
                    data-bs-toggle='collapse'
                    data-bs-target='#collapsePrompts'
                    aria-expanded='false'
                    aria-controls='collapsePrompts'
                  )
                    i.bi.bi-chat-text.me-2
                    | Prompts
                .accordion-collapse.collapse#collapsePrompts(
                  aria-labelledby='headingPrompts'
                  data-bs-parent='#mcpGlossary'
                )
                  .accordion-body
                    p.
                      MCP prompts are reusable templates or instructions that servers can provide to AI clients. They help
                      standardize how AI agents interact with specific systems or perform particular tasks. Prompts can include
                      context about how to use tools effectively, what information to gather, or how to format responses.
                      They act as "best practice guides" built into the protocol, ensuring that AI agents use external systems
                      correctly and consistently. For example, a customer service MCP server might provide prompts that guide
                      an AI on how to handle different types of customer inquiries.

              .accordion-item
                h3.accordion-header#headingTransport
                  button.accordion-button.collapsed(
                    type='button'
                    data-bs-toggle='collapse'
                    data-bs-target='#collapseTransport'
                    aria-expanded='false'
                    aria-controls='collapseTransport'
                  )
                    i.bi.bi-arrow-left-right.me-2
                    | Transport Layer
                .accordion-collapse.collapse#collapseTransport(
                  aria-labelledby='headingTransport'
                  data-bs-parent='#mcpGlossary'
                )
                  .accordion-body
                    p.
                      The transport layer in MCP handles the actual communication between clients and servers. MCP supports
                      multiple transport mechanisms including stdio (standard input/output for local processes), SSE (Server-Sent Events
                      for web-based communication), and WebSocket connections for real-time bidirectional communication. The transport
                      layer is responsible for message delivery, connection management, and basic error handling. Different transports
                      are optimized for different deployment scenariosâ€”stdio for local development, SSE for simple web integrations,
                      and WebSockets for high-performance applications requiring real-time updates.

              .accordion-item
                h3.accordion-header#headingObservability
                  button.accordion-button.collapsed(
                    type='button'
                    data-bs-toggle='collapse'
                    data-bs-target='#collapseObservability'
                    aria-expanded='false'
                    aria-controls='collapseObservability'
                  )
                    i.bi.bi-eye.me-2
                    | MCP Observability Features
                .accordion-collapse.collapse#collapseObservability(
                  aria-labelledby='headingObservability'
                  data-bs-parent='#mcpGlossary'
                )
                  .accordion-body
                    p.
                      MCP includes built-in features that support observability and monitoring of AI agent interactions.
                      The protocol includes request/response logging, error reporting, and progress tracking mechanisms.
                      Servers can emit detailed logs about tool calls, resource access, and performance metrics. The standardized
                      message format makes it easier to implement monitoring dashboards, audit trails, and debugging tools.
                      This is exactly why MCP is so valuable for solving the observability challenges discussed in this articleâ€”it
                      provides a structured way to track what AI agents are actually doing when they interact with external systems.            // Header with icon
            .text-center.mb-5
              i.bi.bi-eye.display-4.text-primary.mb-3
              h2.fw-bold.mb-3 How MCP Addresses the Black Box Problem
              p.lead.text-muted.
                While my joke-fetching agent was essentially flying blind, the Model Context Protocol offers a promising solution to AI observability challenges.

            // Key Benefits Cards
            .row.g-4.mb-5
              .col-md-6.col-lg-4
                .card.h-100.border-0.shadow-sm
                  .card-body.text-center.p-4
                    i.bi.bi-journal-text.display-5.text-primary.mb-3
                    h5.card-title Structured Logging
                    p.card-text.text-muted.
                      MCP provides standardized logging with eight severity levels, making it easy to track what your AI agent is actually doing.

              .col-md-6.col-lg-4
                .card.h-100.border-0.shadow-sm
                  .card-body.text-center.p-4
                    i.bi.bi-diagram-3.display-5.text-success.mb-3
                    h5.card-title Request Tracing
                    p.card-text.text-muted.
                      Every tool call is captured as JSON-RPC messages, providing complete audit trails of agent interactions.

              .col-md-6.col-lg-4
                .card.h-100.border-0.shadow-sm
                  .card-body.text-center.p-4
                    i.bi.bi-search.display-5.text-warning.mb-3
                    h5.card-title Debug Tools
                    p.card-text.text-muted.
                      Built-in MCP Inspector and real-time monitoring help developers see exactly what's happening under the hood.

            // From Black Box to Gray Box
            .row.align-items-center.mb-5
              .col-lg-6
                h3.fw-bold.mb-4
                  i.bi.bi-box.text-dark.me-2
                  | From Black Box to Gray Box
                p.mb-3.
                  In my joke scenario, I had no way to verify whether my agent actually called the API or just pulled from training data. With MCP, this changes dramatically:

                ul.list-unstyled
                  li.mb-2
                    i.bi.bi-check-circle-fill.text-success.me-2
                    strong Explicit Tool Calls:
                    span.text-muted.ms-1 See exactly when external APIs are invoked
                  li.mb-2
                    i.bi.bi-check-circle-fill.text-success.me-2
                    strong Complete Audit Trail:
                    span.text-muted.ms-1 Track HTTP requests, responses, and timing
                  li.mb-2
                    i.bi.bi-check-circle-fill.text-success.me-2
                    strong Real-time Visibility:
                    span.text-muted.ms-1 Monitor agent behavior as it happens

              .col-lg-6
                .bg-white.rounded.shadow-sm.p-4
                  h6.text-muted.small.mb-3
                    i.bi.bi-code-square.me-1
                    | Example MCP Log Output
                  pre.bg-dark.text-light.p-3.rounded.small.language-json
                    code.language-json.
                      {
                        "jsonrpc": "2.0",
                        "method": "tools/call",
                        "params": {
                          "name": "fetch_joke",
                          "arguments": {
                            "source": "jokeapi.com"
                          }
                        },
                        "id": 1
                      }

            // Enterprise Solutions
            .alert.alert-info.border-0.rounded-3.mb-5
              .row.align-items-center
                .col-auto
                  i.bi.bi-building.display-6.text-info
                .col
                  h5.alert-heading.mb-2
                    | Enterprise Observability Solutions
                  p.mb-2.
                    Major platforms like New Relic, Dynatrace, and Moesif now offer specialized MCP monitoring, providing waterfall diagrams, performance metrics, and usage analytics for production AI systems.
                  small.text-muted
                    i.bi.bi-info-circle.me-1
                    | The ecosystem is rapidly evolving with OpenTelemetry integration and advanced tracing capabilities in development.

            // Bottom Note
            .text-center.mt-5.mb-5
              .bg-primary.bg-opacity-10.rounded-3.p-4
                h5.text-primary.mb-3
                  i.bi.bi-lightbulb.me-2
                  | The Takeaway
                p.mb-0.text-dark.
                  While MCP doesn't solve AI decision-making transparency completely, it transforms AI systems from completely opaque black boxes into "gray boxes" â€“ giving developers the visibility they need to debug, monitor, and trust their AI agents.

            h2.h4.mb-3.text-dark.d-flex.align-items-center
              i.bi.bi-lightning-charge.me-2
              | Code Samples
            p.mb-3.
              For those interested in diving deeper, here is the simple C# code snippet that demonstrates
              how to implement MCP servers and tools for my basic joke-fetching tool.
              With no observability.
              Notice the lack of logging or proof of API calls.
            h3.h5.mb-3.text-primary.d-flex.align-items-center MCP Server
            pre.language-csharp
              code.language-csharp.
                // Create a generic host builder for
                // dependency injection, logging, and configuration.
                var builder = Host.CreateApplicationBuilder(args);

                // Configure logging for better integration with MCP clients.
                builder.Logging.AddConsole(consoleLogOptions =>
                {
                  consoleLogOptions.LogToStandardErrorThreshold = LogLevel.Trace;
                });

                // Register the MCP server and configure it to use stdio transport.
                // Scan the assembly for tool definitions.
                builder.Services
                  .AddMcpServer()
                  .WithStdioServerTransport()
                  .WithToolsFromAssembly();

                // Register HttpClient for API calls
                builder.Services.AddHttpClient();

                // Build and run the host. This starts the MCP server.
                await builder.Build().RunAsync();

            h3.h5.mb-3.text-secondary.d-flex.align-items-center Joke Tool
            pre.language-csharp
              code.language-csharp.
                /// <summary>
                /// Tool implementations using static methods
                /// </summary>
                public static class Tools
                {

                  /// <summary>
                  /// Fetches a random joke from JokeAPI
                  /// </summary>
                  /// <returns>A random programming joke</returns>
                  [Description("Fetches a random joke from JokeAPI")]
                  public static async Task<string> GetJoke()
                  {
                    using var client = new HttpClient();
                    try
                    {
                      var response = await client.GetFromJsonAsync<JokeResponse>(
                      "https://v2.jokeapi.dev/joke/Programming?safe-mode");

                      string joke = response?.Type == "single"
                      ? response.Joke ?? "No joke available"
                      : $"{response?.Setup}\n{response?.Delivery}";

                      return $"JOKE: {joke}";
                    }
                    catch (Exception ex)
                    {
                      return $"Error fetching joke: {ex.Message}";
                    }
                  }
                }

            h2.h4.mb-3.text-dark.d-flex.align-items-center
              i.bi.bi-lightning-charge.me-2
              | Final Thoughts
            p.mb-3.
              We need to ask harder questions of our AI systems. Not "can they do it?"â€”but "did they actually do it?" Because in the end, observability isnâ€™t optional. Itâ€™s essential.

