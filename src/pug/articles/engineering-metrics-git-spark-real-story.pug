extends ../layouts/modern-layout

block layout-content
  br

  section.py-4
    .container
      .row
        .col-lg-10.mx-auto
          h1.mb-3
            | Engineering Metrics: The Git Spark Story
          p.lead.mb-4.
            Traditional performance metrics weren't designed for the age of AI-assisted development.
            They track visible outputs like commits and tickets closed, but miss the work that
            AI tools do behind the scenes. This is the story of trying to measure AI contributions
            through Git data—and why it's harder than it looks.
          // Article Metadata
          .mb-4.text-muted
            span.me-3
              i.bi.bi-person.me-1
              | Mark Hazleton
            span.me-3
              i.bi.bi-calendar.me-1
              | October 2025
            span
              i.bi.bi-tag.me-1
              | Engineering Metrics, Git Analytics

  // Main Article Content
  article#main-article
    .container
      .row
        .col-lg-9.mx-auto
          // Article Content Section
          section#article-content.mb-5

            // Opening Story
            h2.mb-3
              | When "Objective Data" Leads You Astray

            p.
              After writing about measuring AI's contribution to code, I was frustrated.
              I couldn't quantify how much AI tools were actually helping my development process.
              Git history seemed like the obvious answer—objective, comprehensive, and already
              being tracked. Every commit, every change, every contributor permanently recorded.
              Surely, I could extract meaningful insights from this data, right?

            p.
              Wrong. Very wrong. That weekend, instead of just analyzing Git data, I decided
              to build something better. I wanted to create a tool that could honestly report
              what Git history actually contains—without the misleading "health scores" and
              fake productivity metrics that plague existing tools. This became git-spark,
              my first npm package.

            p.
              I thought building the tool would give me answers. Instead, it taught me about
              all the questions Git history simply cannot answer. But that journey—from
              frustrated developer to package publisher—revealed something more valuable:
              the fundamental limitations of measuring software development through commit logs.

            p.
              This is the real story of creating git-spark: a weekend project born from
              frustration, shaped by AI tools, refined through trial and error, and ultimately
              designed to admit what it doesn't know. If you're trying to measure developer
              productivity or AI contributions through Git data, here's what I learned
              about what you can and cannot reliably discover.

            // Table of Contents
            nav#table-of-contents.mb-4(aria-label='Table of Contents')
              .card
                .card-header
                  h3.h5.mb-0
                    | Table of Contents
                .card-body
                  ul.list-unstyled
                    li.mb-1: a.text-decoration-none(href='#anti-metrics') The Problem with Current Metrics
                    li.mb-1: a.text-decoration-none(href='#missing-data') What Git History Can't Tell You
                    li.mb-1: a.text-decoration-none(href='#activity-index') Building Honest Metrics
                    li.mb-1: a.text-decoration-none(href='#social-structure') Team Patterns in Code
                    li.mb-1: a.text-decoration-none(href='#git-spark') What Git Spark Does
                    li.mb-1: a.text-decoration-none(href='#try-git-spark') Try It Yourself
                    li: a.text-decoration-none(href='#conclusion') Lessons Learned

          section#git-spark.mb-4
            h2.mb-3
              | Building Git Spark: My First npm Package

            p.
              That weekend, I decided to build what I wished existed: an honest Git analytics tool.
              Having used npm packages for years, I'd always wanted to create my own. Git-spark
              became that opportunity—and my crash course in package development.

            .card.mb-4
              .card-body
                h3.h5.mb-3
                  | The Weekend Build: Reality Check

                .row
                  .col-md-6
                    h4.h6.text-success Saturday Morning Goals
                    ul
                      li Use AI tools to generate initial code
                      li Parse Git history and extract metrics
                      li Create visualizations and reports
                      li Publish to npm by Sunday evening

                  .col-md-6
                    h4.h6.text-warning Sunday Night Results
                    ul
                      li.text-success AI tools gave me a great first build
                      li.text-warning Had to revise formulas multiple times
                      li.text-warning Realized most metrics were misleading
                      li.text-danger Still couldn't measure AI contributions
                      li.text-success Learned what honest reporting means

            p.
              The AI tools I used (ironically, the same ones I was trying to measure) gave
              me an impressive initial implementation. The code worked, the visualizations
              looked professional, and the metrics seemed authoritative. That's when I made
              a critical mistake: I actually read the formulas behind the numbers.

            .alert.alert-warning
              strong The Problem with "Health Scores"
              p.mb-2.
                My first version calculated a "Repository Health Score" based on commit
                frequency, author distribution, and code churn. It looked scientific.
                It generated impressive charts. And it was complete nonsense.
              p.mb-0.
                The formula assigned arbitrary weights to metrics we couldn't meaningfully
                interpret. A score of 87% health? What does that even mean? Healthy compared
                to what? Based on whose definition of healthy?

            p.
              This realization forced multiple rewrites. Each iteration stripped away another
              layer of pretense, another attempt to derive meaning from data that simply
              didn't contain it. By version 0.5, I'd removed every evaluative metric and
              replaced them with honest observations.

          section#anti-metrics.mb-4
            h2.mb-3
              | The Problem with Current Metrics

            p.
              The most popular Git-based metrics aren't just unhelpful—they're actively harmful.
              They measure motion instead of progress and incentivize behaviors that damage
              both code quality and team culture.

            .card.mb-4
              .card-header
                h3.h5.mb-0
                  | The Three Problematic Metrics
              .card-body
                .row
                  .col-md-4
                    h4.h6 Commit Count
                    p.
                      Rewards developers who split logical changes into artificially small commits.
                      Punishes those who make well-structured, comprehensive changes.
                    small.text-muted.
                      Result: Noisy history and developers optimizing for metrics instead of quality.

                  .col-md-4
                    h4.h6 Lines of Code
                    p.
                      Measures verbosity, not value. Punishes refactoring and simplification.
                      Rewards copy-paste programming and bloated implementations.
                    small.text-muted.
                      Result: Growing codebases that become harder to maintain.

                  .col-md-4
                    h4.h6 Weekend Commits
                    p.
                      Often interpreted as "dedication" when it actually signals burnout,
                      poor work-life balance, or unrealistic deadlines.
                    small.text-muted.
                      Result: Normalized overwork and exhausted team members.

            p.
              I learned this lesson the hard way. Within a month of introducing commit-based metrics,
              our most disciplined engineer—someone who routinely made comprehensive, well-tested commits—
              appeared to be our least productive. Meanwhile, a junior developer who committed after
              every minor change topped the charts. The metrics were giving us exactly the wrong signal.

          section#missing-data.mb-4
            h2.mb-3
              | What Git History Cannot Tell You

            p.
              Building git-spark taught me something humbling: the most valuable aspects
              of software development leave absolutely no trace in commit logs. This was
              my biggest surprise and the most important lesson from the entire project.

            p.
              I started this journey trying to answer one specific question:
              <strong>"How much of my code is generated by AI prompts?"</strong> After weeks
              of analysis, creating formulas, and examining every metric Git history offers,
              I have to admit defeat. Git simply doesn't—and cannot—record that information.

            .row.mb-4
              .col-md-6
                .card.h-100
                  .card-header
                    h4.h5.mb-0
                      | What Git Records
                  .card-body
                    ul.list-unstyled
                      li.mb-2 Files changed
                      li.mb-2 Lines added/removed
                      li.mb-2 Timestamp of changes
                      li.mb-2 Commit author (always human)
                      li Commit message (human-written)

              .col-md-6
                .card.h-100
                  .card-header
                    h4.h5.mb-0
                      | What I Needed (But Doesn't Exist)
                  .card-body
                    ul.list-unstyled
                      li.mb-2 AI assistance level per commit
                      li.mb-2 Prompt-to-code traceability
                      li.mb-2 Human vs. AI authorship percentage
                      li.mb-2 Code review quality and outcomes
                      li.mb-2 Design decisions and trade-offs
                      li.mb-2 Testing effort and coverage
                      li.mb-2 Deployment success and impact
                      li Refactoring vs. new feature work

            p.
              This realization completely changed how I thought about developer productivity
              measurement. Every existing tool I'd examined—the ones claiming to measure
              "health" or "productivity"—suffered from the same fundamental flaw: they
              measured motion, not value. They counted commits but ignored impact.

          section#activity-index.mb-4
            h2.mb-3
              | Building Honest Metrics

            p.
              Here's where I started to understand what Git analytics could legitimately provide.
              The problem wasn't using Git data—it was pretending it measured things it didn't.
              "Repository health" is marketing speak. "Activity patterns" is honest reporting.

            p.
              Many tools generate "health scores" or "productivity ratings" that sound authoritative
              but are fundamentally subjective. They take the limited data Git provides, apply
              arbitrary weights and thresholds, then package it as objective truth.

            .card.mb-4
              .card-header
                h3.h5.mb-0
                  | Activity Index: What We Can Honestly Measure
              .card-body
                p.mb-3.
                  Instead of fake health scores, we can measure observable activity patterns
                  and let humans interpret what they mean in context:

                dl.mb-0
                  dt.fw-bold.mb-2 Commit Frequency (Normalized)
                  dd.mb-3.
                    How often commits happen, adjusted for team size and project phase.
                    Signals whether development is active, stalled, or sporadic.

                  dt.fw-bold.mb-2 Author Participation Breadth
                  dd.mb-3.
                    How many team members contribute relative to total volume.
                    Reveals whether work is distributed or concentrated.

                  dt.fw-bold.mb-2 Change Size Variability
                  dd.mb-3.
                    Coefficient of variation in commit sizes over time.
                    Indicates consistency in development rhythm and working style.

                  dt.fw-bold.mb-2 File Touch Patterns
                  dd.mb-3.
                    Which files change together frequently and who works on them.
                    Exposes coupling, specialization, and potential bottlenecks.

            p.
              This shift from evaluation to observation is crucial. We're not saying the team
              is productive or unproductive. We're saying "here are the patterns we observe;
              you decide what they mean for your context."

          section#social-structure.mb-4
            h2.mb-3
              | Team Patterns in Code

            p.
              This is where Git analytics gets genuinely interesting: not for measuring
              individual productivity, but for revealing patterns that would otherwise remain
              invisible. Your code structure mirrors your team structure, and Git history
              exposes that relationship.

            p.
              Conway's Law states that organizations build systems that mirror their
              communication structure. Git analytics makes this visible through patterns
              that emerge from how developers interact with the codebase. These patterns
              don't tell you if your team is "good" or "bad"—they tell you how your team
              actually works, which is far more valuable.

            .row.mb-4
              .col-md-6
                .card.h-100
                  .card-body
                    h4.h5
                      | File Specialization Index
                    p.
                      Measures how concentrated code ownership is across files.
                      High FSI means few people touch each file; low FSI means
                      broad collaboration.

                    small What It Reveals:
                    ul.mt-2
                      li Potential knowledge bottlenecks
                      li Areas of deep expertise vs. shared ownership
                      li Bus factor risks

              .col-md-6
                .card.h-100
                  .card-body
                    h4.h5
                      | Ownership Entropy
                    p.
                      Measures how evenly contributions are distributed across
                      authors. High entropy means balanced collaboration; low entropy
                      means concentrated ownership.

                    small What It Reveals:
                    ul.mt-2
                      li Whether "collaboration" is real or superficial
                      li Dominant contributors vs. peripheral participants
                      li Team knowledge distribution patterns

            .card.mb-4
              .card-header
                h4.h5.mb-0
                  | Co-Change Coupling: Hidden Dependencies
              .card-body
                p.mb-3.
                  Files that change together frequently reveal architectural coupling
                  that may not be obvious from static code analysis. This helps identify:

                ul
                  li <strong>Architecture Friction:</strong> Files that shouldn't be coupled but always change together
                  li <strong>Hidden Dependencies:</strong> Coupling between seemingly unrelated modules
                  li <strong>Team Coordination Needs:</strong> Files requiring coordination between their maintainers

            p.
              I discovered this accidentally when analyzing why certain features always
              took longer than estimated. Git analytics revealed that three supposedly
              independent modules had high co-change coupling—they couldn't be modified
              independently in practice, even though the architecture said they could.
              This explained why every "simple" change rippled through the system.

          section#try-git-spark.mb-4
            h2.mb-3
              | What Git Spark Does

            p.
              Despite failing to measure AI contributions, I succeeded in creating something
              useful: an honest Git analytics tool that respects what the data can and
              cannot tell us. This became git-spark, my first npm package.

            .card.mb-4
              .card-header
                h4.h5.mb-0
                  | The Honest Approach
              .card-body
                p.mb-3.
                  After multiple rewrites, here's what git-spark actually does:

                .row
                  .col-md-6
                    h5.h6.mb-3 What It Reports
                    ul
                      li Observable patterns in commit history
                      li File coupling and change frequencies
                      li Author contribution distributions
                      li Temporal development trends
                      li Code structure evolution over time

                  .col-md-6
                    h5.h6.mb-3 What It Refuses to Infer
                    ul
                      li Fake "health scores" or "productivity ratings"
                      li Developer rankings or comparisons
                      li Code quality judgments from LOC
                      li AI contribution percentages
                      li Anything not directly observable in Git

            p.
              Git-spark doesn't tell you whether your repository is "healthy" or your team
              is "productive." Instead, it shows you patterns and lets you interpret them
              with your domain knowledge.

            .card.mb-4
              .card-body
                h4.h5.mb-3 Try It Out

                pre.bg-dark.text-light.p-3.rounded.mb-3
                  code.
                    # Install globally
                    npm install -g git-spark

                    # Or use with npx
                    npx git-spark analyze

                p.mb-0.
                  Get transparent insights into your repository's activity patterns.
                  No magic formulas, no fake scores—just observable facts from your Git history.

          section#conclusion.mb-4
            h2.mb-3
              | Lessons Learned

            p.
              I set out to answer a simple question: "How much of my code is generated by
              AI prompts?" I built an entire analytics tool, published my first npm package,
              and learned more about Git internals than I ever expected. And I still can't
              answer that question.

            p.
              But that failure taught me something more valuable: the discipline of honest
              measurement. Not every question has a data-driven answer. Not every metric
              is meaningful. And tools that claim to measure everything often measure nothing
              reliably.

            .card.mb-4
              .card-body
                h3.h5.mb-3
                  | What This Journey Produced

                .row
                  .col-md-6
                    h4.h6.mb-3 Technical Skills
                    ul
                      li My first published npm package
                      li Understanding of Git internals
                      li Experience with testing and validation
                      li Package organization skills
                      li TypeScript/JavaScript development

                  .col-md-6
                    h4.h6.mb-3 Important Insights
                    ul
                      li Limits of Git-based metrics
                      li Difference between motion and progress
                      li Value of honest vs. authoritative reporting
                      li Why AI contributions remain invisible
                      li What makes metrics trustworthy

            p.
              Git-spark doesn't tell you if your team is productive. It doesn't measure
              code quality. It can't identify AI-generated code. What it does is report
              observable patterns in your repository's history and then—crucially—shuts up.
              No fake scores. No pretend insights. Just data and the humility to admit its
              limitations.

            p.
              Building git-spark taught me that the best metrics tools don't pretend to
              have all the answers. They provide honest data and trust you to ask better
              questions. That's the philosophy behind every line of code in git-spark,
              and I hope it's useful to others wrestling with these same measurement challenges.

            .text-center.mt-4
              a.btn.btn-primary.me-3(
                href='https://github.com/MarkHazleton/git-spark',
                target='_blank',
                rel='noopener noreferrer'
              ) Try git-spark
              a.btn.btn-outline-primary(href='/#contact') Discuss Metrics

