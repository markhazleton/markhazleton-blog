extends ../layouts/modern-layout

block layout-content
  br

  section.bg-gradient-primary.py-5
    .container
      .row.align-items-center
        .col-lg-10.mx-auto.text-center
          h1.display-4.fw-bold.mb-3
            i.bi.bi-graph-up-arrow.me-3
            | Engineering Metrics: Git Spark Story
          h2.h3.mb-4 Why Git History Tells the Wrong Story About Developer Productivity
          p.lead.mb-5.
            Traditional performance metrics were not built for AI Agents.
            They track visible outputs—commits, comments, tickets closed—but miss the
            silent labor of agents optimizing workflows, refactoring code,
            or triaging issues before they escalate.
            The result? A skewed picture of productivity that obfuscates the work of AI agents.
          // Article Metadata
          .mb-4.d-flex.justify-content-center.align-items-center.gap-4
            span
              i.bi.bi-person.me-1
              | Mark Hazleton
            span
              i.bi.bi-calendar.me-1
              | October 2025
            span
              i.bi.bi-tag.me-1
              | Engineering Metrics, Git Analytics, Team Performance

  // Main Article Content
  article#main-article
    .container
      .row
        .col-lg-9.mx-auto
          // Article Content Section
          section#article-content.mb-5

            // Opening Story
            h2.h3.mb-3.text-primary.d-flex.align-items-center
              i.bi.bi-lightning-charge.me-2
              | When "Objective Data" Leads You Astray

            p.lead.mb-4.
              After writing about
              <a href='measuring-ais-contribution-to-code.html' target='_blank'>measuring AI's contribution to code</a>
              , I was frustrated. I couldn't quantify how much AI agents were actually helping
              my development process. Git history seemed like the obvious answer—objective,
              comprehensive, and already being tracked. Every commit, every change, every
              contributor permanently recorded. Surely, I could extract meaningful insights
              from this data, right?

            p.mb-4.
              Wrong. Very wrong. That weekend, instead of just analyzing Git data, I decided
              to build something better. I wanted to create a tool that could honestly report
              what Git history actually contains—without the misleading "health scores" and
              fake productivity metrics that plague existing tools. This became git-spark,
              my first npm package.

            p.mb-4.
              I thought building the tool would give me answers. Instead, it taught me about
              all the questions Git history simply cannot answer. But that journey—from
              frustrated developer to package publisher—revealed something more valuable:
              the fundamental limitations of measuring software development through commit logs.

            p.mb-4.
              This is the real story of creating git-spark: a weekend project born from
              frustration, shaped by AI agents, refined through brutal honesty, and ultimately
              designed to admit what it doesn't know. If you're trying to measure developer
              productivity or AI contributions through Git data, I learned the hard way
              what you can and cannot reliably discover.

            // Table of Contents
            nav#table-of-contents.mb-5(aria-label='Table of Contents')
              .card.bg-light
                .card-header
                  h3.card-title.mb-0.fw-bold
                    i.bi.bi-list-ul.me-2
                    | Table of Contents
                .card-body
                  ul.list-group.list-group-flush
                    li.list-group-item: a.text-decoration-none(href='#anti-metrics') The Dangerous Allure of Anti-Metrics
                    li.list-group-item: a.text-decoration-none(href='#missing-data') What Git History Can't Tell You
                    li.list-group-item: a.text-decoration-none(href='#activity-index') From Health Scores to Honest Metrics
                    li.list-group-item: a.text-decoration-none(href='#social-structure') Code as a Window Into Team Dynamics
                    li.list-group-item: a.text-decoration-none(href='#git-spark') What Git Spark Gets Right
                    li.list-group-item: a.text-decoration-none(href='#try-git-spark') Try Git Spark: My First npm Package
                    li.list-group-item: a.text-decoration-none(href='#conclusion') Conclusion: From Simple Answers to Better Questions

          section#git-spark.mb-5
            h2.h3.mb-4.text-primary.d-flex.align-items-center
              i.bi.bi-stars.me-2
              | Building Git Spark: My First npm Package

            p.lead.mb-4.
              That weekend, fueled by frustration and too much coffee, I decided to build
              what I wished existed: an honest Git analytics tool. Having used npm packages
              for years, I'd always wanted to create my own. Git-spark became that opportunity—and
              my crash course in package development.

            .card.mb-4.bg-light.border-primary
              .card-body
                h3.h5.fw-bold.mb-3
                  i.bi.bi-clock-history.me-2
                  | The Weekend Build: Expectations vs. Reality

                .row.g-3
                  .col-md-6
                    h4.h6.text-success Saturday Morning: The Vision
                    ul.mb-0
                      li.mb-2 Use AI agents to generate initial code
                      li.mb-2 Parse Git history and extract metrics
                      li.mb-2 Create visualizations and reports
                      li.mb-2 Publish to npm by Sunday evening
                      li.mb-2 Finally measure AI contributions!

                  .col-md-6
                    h4.h6.text-warning Sunday Night: The Reality
                    ul.mb-0
                      li.mb-2 ✅ AI agents gave me a great first build
                      li.mb-2 ⚠️ Had to revise formulas multiple times
                      li.mb-2 ⚠️ Realized most metrics were misleading
                      li.mb-2 ❌ Still couldn't measure AI contributions
                      li.mb-2 ✅ But learned what honest reporting means

            p.mb-4.
              The AI agents I used (ironically, the same ones I was trying to measure) gave
              me an impressive initial implementation. The code worked, the visualizations
              looked professional, and the metrics seemed authoritative. That's when I made
              a critical mistake: I actually read the formulas behind the numbers.

            .alert.alert-warning.mb-4
              strong.d-block.mb-2
                i.bi.bi-exclamation-triangle.me-2
                | The Formulas Don't Lie (But They Can Mislead)
              p.mb-2.
                My first version calculated a "Repository Health Score" based on commit
                frequency, author distribution, and code churn. It looked scientific.
                It generated impressive charts. And it was complete nonsense.
              p.mb-0.
                The formula assigned arbitrary weights to metrics we couldn't meaningfully
                interpret. A score of 87% health? What does that even mean? Healthy compared
                to what? Based on whose definition of healthy?

            p.mb-4.
              This realization forced multiple rewrites. Each iteration stripped away another
              layer of pretense, another attempt to derive meaning from data that simply
              didn't contain it. By version 0.5, I'd removed every evaluative metric and
              replaced them with honest observations.

            .card.mb-4
              .card-header.bg-success.text-white
                h3.card-title.mb-0.fw-bold
                  i.bi.bi-shield-check.me-2
                  | The Git Spark Philosophy: What I Learned to Value
              .card-body
                p.mb-3.
                  Building this tool taught me that enterprise-worthy software—something I'd
                  want my name on—requires brutal honesty about limitations:

                dl.mb-0
                  dt.fw-bold.mb-2 Testing Over Assumptions
                  dd.mb-3.
                    Every metric needed validation against real repositories. Does this
                    pattern actually mean what I think it means? Can I defend this formula?
                    If I can't explain it to a developer, it doesn't belong in the tool.

                  dt.fw-bold.mb-2 Transparency Over Authority
                  dd.mb-3.
                    Users should see exactly how metrics are calculated. No magic formulas,
                    no proprietary algorithms, no "trust us" scoring. Show your work or
                    don't show the score.

                  dt.fw-bold.mb-2 Observation Over Evaluation
                  dd.mb-3.
                    Report what Git history actually contains. Don't invent productivity
                    scores from commit counts. Don't pretend to measure code quality from
                    line changes. State facts; let users interpret them.

                  dt.fw-bold.mb-2 Limitations Over Pretense
                  dd.mb-3.
                    Admit what you can't measure. Git history doesn't contain deployment
                    outcomes, code review quality, or AI assistance levels. Acknowledging
                    these gaps builds trust; pretending they don't exist destroys it.

            p.mb-4.
              This philosophy transformed git-spark from just another analytics tool into
              something I'm genuinely proud of: an honest reporter that respects both the
              data and the user's intelligence.

          section#anti-metrics.mb-5
            h2.h3.mb-4.text-primary.d-flex.align-items-center
              i.bi.bi-exclamation-triangle.me-2
              | The Dangerous Allure of Anti-Metrics

            p.lead.mb-4.
              The most popular Git-based metrics aren't just unhelpful—they're actively harmful.
              I call them "anti-metrics" because they measure motion instead of progress
              and incentivize behaviors that damage both code quality and team culture.

            .card.mb-4
              .card-header.bg-danger.text-white
                h3.card-title.mb-0.fw-bold
                  i.bi.bi-x-circle.me-2
                  | The Three Deadly Anti-Metrics
              .card-body
                .row.g-4
                  .col-md-4
                    .card.h-100.border-danger
                      .card-body
                        h4.card-title.h5
                          i.bi.bi-git.me-2
                          | Commit Count
                        p.card-text.
                          Rewards developers who split logical changes into artificially small commits.
                          Punishes those who make well-structured, comprehensive changes.
                        .alert.alert-danger.mb-0
                          small.
                            <strong>Result:</strong> Noisy history, meaningless granularity,
                            and developers optimizing for metrics instead of quality.

                  .col-md-4
                    .card.h-100.border-danger
                      .card-body
                        h4.card-title.h5
                          i.bi.bi-code-slash.me-2
                          | Lines of Code
                        p.card-text.
                          Measures verbosity, not value. Punishes refactoring and simplification.
                          Rewards copy-paste programming and bloated implementations.
                        .alert.alert-danger.mb-0
                          small.
                            <strong>Result:</strong> Growing codebases that become harder to maintain,
                            with developers afraid to delete unnecessary code.

                  .col-md-4
                    .card.h-100.border-danger
                      .card-body
                        h4.card-title.h5
                          i.bi.bi-clock.me-2
                          | Weekend Commits
                        p.card-text.
                          Often interpreted as "dedication" when it actually signals burnout,
                          poor work-life balance, or unrealistic deadlines.
                        .alert.alert-danger.mb-0
                          small.
                            <strong>Result:</strong> Normalized overwork, exhausted team members,
                            and the false impression that productivity requires sacrifice.

            p.mb-4.
              I learned this lesson the hard way. Within a month of introducing commit-based metrics,
              our most disciplined engineer—someone who routinely made comprehensive, well-tested commits—
              appeared to be our least productive. Meanwhile, a junior developer who committed after
              every minor change topped the charts. The metrics were giving us exactly the wrong signal.

            pre.language-javascript
              code.language-javascript.
                // 🚫 The anti-metric trap
                const productivityScore = {
                  commits: developer.commits.length,          // Meaningless
                  linesAdded: developer.additions,            // Worse than meaningless
                  weekendWork: developer.weekendCommits       // Actively harmful
                };
                // This measures motion, not progress

          section#missing-data.mb-5
            h2.h3.mb-4.text-primary.d-flex.align-items-center
              i.bi.bi-eye-slash.me-2
              | The Biggest Surprise: What Git History Cannot Tell You

            p.lead.mb-4.
              Building git-spark taught me something humbling: the most valuable aspects
              of software development leave absolutely no trace in commit logs. This was
              my biggest surprise and the most important lesson from the entire project.

            p.mb-4.
              I started this journey trying to answer one specific question:
              <strong>"How much of my code is generated by AI prompts?"</strong> After weeks
              of analysis, creating formulas, and examining every metric Git history offers,
              I have to admit defeat. Git simply doesn't—and cannot—record that information.

            .card.mb-4.bg-light.border-warning
              .card-body
                h3.h5.fw-bold.mb-3.text-warning
                  i.bi.bi-question-circle.me-2
                  | The Original Question Remains Unanswered

                p.mb-3.
                  Despite all the interesting patterns git-spark revealed, I failed to achieve
                  my primary goal: pinpointing AI agent contributions to my codebase. And
                  that failure taught me more than success would have.

                .alert.alert-warning.mb-0
                  strong Why Git Can't Answer This:
                  ul.mb-0.mt-2
                    li Git records commits, not process
                    li AI assistance happens before the commit
                    li No standard way to tag AI-generated code
                    li Human editing obscures AI origins
                    li Pair programming (human + AI) is invisible

            .row.g-4.mb-4
              .col-md-6
                .card.h-100
                  .card-header.bg-info.text-white
                    h4.card-title.mb-0.h5
                      i.bi.bi-search.me-2
                      | What Git Records
                  .card-body
                    ul.list-unstyled
                      li.mb-2
                        i.bi.bi-check-circle-fill.text-success.me-2
                        | Files changed
                      li.mb-2
                        i.bi.bi-check-circle-fill.text-success.me-2
                        | Lines added/removed
                      li.mb-2
                        i.bi.bi-check-circle-fill.text-success.me-2
                        | Timestamp of changes
                      li.mb-2
                        i.bi.bi-check-circle-fill.text-success.me-2
                        | Commit author (always human)
                      li.mb-2
                        i.bi.bi-check-circle-fill.text-success.me-2
                        | Commit message (human-written)

              .col-md-6
                .card.h-100
                  .card-header.bg-warning.text-dark
                    h4.card-title.mb-0.h5
                      i.bi.bi-question-circle.me-2
                      | What I Needed (But Doesn't Exist)
                  .card-body
                    ul.list-unstyled
                      li.mb-2
                        i.bi.bi-x-circle-fill.text-danger.me-2
                        | AI assistance level per commit
                      li.mb-2
                        i.bi.bi-x-circle-fill.text-danger.me-2
                        | Prompt-to-code traceability
                      li.mb-2
                        i.bi.bi-x-circle-fill.text-danger.me-2
                        | Human vs. AI authorship percentage
                      li.mb-2
                        i.bi.bi-x-circle-fill.text-danger.me-2
                        | Code review quality and outcomes
                      li.mb-2
                        i.bi.bi-x-circle-fill.text-danger.me-2
                        | Design decisions and trade-offs
                      li.mb-2
                        i.bi.bi-x-circle-fill.text-danger.me-2
                        | Testing effort and coverage
                      li.mb-2
                        i.bi.bi-x-circle-fill.text-danger.me-2
                        | Deployment success and impact
                      li.mb-2
                        i.bi.bi-x-circle-fill.text-danger.me-2
                        | Refactoring vs. new feature work

            p.mb-4.
              This realization completely changed how I thought about developer productivity
              measurement. Every existing tool I'd examined—the ones claiming to measure
              "health" or "productivity"—suffered from the same fundamental flaw: they
              measured motion, not value. They counted commits but ignored impact.

            .card.mb-4.border-danger
              .card-header.bg-danger.text-white
                h4.card-title.mb-0
                  i.bi.bi-exclamation-triangle.me-2
                  | The Weakness of Existing Tools
              .card-body
                p.mb-3.
                  Building git-spark made me deeply skeptical of every Git analytics tool
                  that promises to measure productivity, code quality, or team health.
                  Here's what I learned they're actually doing:

                .row.g-3.mb-3
                  .col-md-4
                    .alert.alert-danger.mb-0
                      strong.d-block.mb-2 Taking Limited Data
                      small Commits, LOC, file changes—only what Git records

                  .col-md-4
                    .alert.alert-danger.mb-0
                      strong.d-block.mb-2 Applying Arbitrary Weights
                      small "We've determined that X is 3.7x more important than Y"

                  .col-md-4
                    .alert.alert-danger.mb-0
                      strong.d-block.mb-2 Generating Fake Scores
                      small "Your repository health is 87% (excellent!)"

                p.mb-0.
                  These tools don't measure what they claim to measure. They measure what's
                  easy to count and pretend it correlates with what actually matters. Building
                  git-spark taught me to recognize—and reject—that approach.

            pre.language-javascript
              code.language-javascript.
                // What I hoped to measure
                const aiContribution = {
                  promptsUsed: 47,
                  linesGenerated: 2840,
                  percentageOfCode: "42%",
                  productivityBoost: "3.2x"
                };

                // What Git actually tells me
                const gitReality = {
                  commits: 156,
                  totalChanges: 6723,
                  author: "Mark Hazleton",
                  aiAssistance: undefined  // Git doesn't know
                };

                // The gap between what I need and what exists
                console.warn("AI contribution: Not measurable from Git history");

          section#activity-index.mb-5
            h2.h3.mb-4.text-primary.d-flex.align-items-center
              i.bi.bi-bar-chart-line.me-2
              | From Health Scores to Honest Metrics

            p.lead.mb-4.
              Here's where I started to understand what Git analytics could legitimately provide.
              The problem wasn't using Git data—it was pretending it measured things it didn't.
              "Repository health" is marketing speak. "Activity patterns" is honest reporting.

            p.mb-4.
              Many tools generate "health scores" or "productivity ratings" that sound authoritative
              but are fundamentally subjective. They take the limited data Git provides, apply
              arbitrary weights and thresholds, then package it as objective truth. It's
              pseudoscience wrapped in dashboards.

            .card.mb-4
              .card-header.bg-success.text-white
                h3.card-title.mb-0.fw-bold
                  i.bi.bi-check-circle.me-2
                  | Activity Index: What We Can Honestly Measure
              .card-body
                p.mb-3.
                  Instead of fake health scores, we can measure observable activity patterns
                  and let humans interpret what they mean in context:

                dl.mb-0
                  dt.fw-bold.mb-2 Commit Frequency (Normalized)
                  dd.mb-3.
                    How often commits happen, adjusted for team size and project phase.
                    Signals whether development is active, stalled, or sporadic.
                    <span class="text-muted">This is a fact, not a judgment.</span>

                  dt.fw-bold.mb-2 Author Participation Breadth
                  dd.mb-3.
                    How many team members contribute relative to total volume.
                    Reveals whether work is distributed or concentrated.
                    <span class="text-muted">Shows patterns, doesn't evaluate them.</span>

                  dt.fw-bold.mb-2 Change Size Variability
                  dd.mb-3.
                    Coefficient of variation in commit sizes over time.
                    Indicates consistency in development rhythm and working style.
                    <span class="text-muted">Context determines if this is good or bad.</span>

                  dt.fw-bold.mb-2 File Touch Patterns
                  dd.mb-3.
                    Which files change together frequently and who works on them.
                    Exposes coupling, specialization, and potential bottlenecks.
                    <span class="text-muted">Information, not evaluation.</span>

            pre.language-javascript
              code.language-javascript.
                // ❌ Fake health score (subjective, misleading)
                const healthScore = calculateMagicFormula(commits, loc, authors);
                // Pretends to know what "healthy" means

                // ✅ Activity index (objective, honest)
                const activityIndex = {
                  frequency: {
                    commitsPerWeek: 47,
                    trend: "stable",
                    normalized: 0.85
                  },
                  breadth: {
                    activeContributors: 8,
                    participationRatio: 0.67,
                    concentration: "moderate"
                  },
                  variance: {
                    commitSizeCV: 1.2,
                    pattern: "consistent",
                    outliers: 3
                  }
                };
                // Reports facts, lets humans interpret

            p.mb-4.
              This shift from evaluation to observation is crucial. We're not saying the team
              is productive or unproductive. We're saying "here are the patterns we observe;
              you decide what they mean for your context." That honesty is what makes
              the data actually useful.

          section#social-structure.mb-5
            h2.h3.mb-4.text-primary.d-flex.align-items-center
              i.bi.bi-diagram-3.me-2
              | Code as a Window Into Team Dynamics

            p.lead.mb-4.
              This is where Git analytics gets genuinely interesting: not for measuring
              individual productivity, but for revealing socio-technical patterns that
              would otherwise remain invisible. Your code structure mirrors your team
              structure, and Git history exposes that relationship.

            p.mb-4.
              Conway's Law states that organizations build systems that mirror their
              communication structure. Git analytics makes this visible through patterns
              that emerge from how developers interact with the codebase. These patterns
              don't tell you if your team is "good" or "bad"—they tell you how your team
              actually works, which is far more valuable.

            .row.g-4.mb-4
              .col-md-6
                .card.h-100.border-primary
                  .card-body
                    h4.card-title.h5
                      i.bi.bi-file-earmark-code.me-2
                      | File Specialization Index (FSI)
                    p.card-text.
                      Measures how concentrated code ownership is across files.
                      High FSI means few people touch each file; low FSI means
                      broad collaboration.

                    .alert.alert-info.mb-3
                      small.fw-bold What It Reveals:
                      ul.mb-0.mt-2
                        li Potential knowledge bottlenecks
                        li Areas of deep expertise vs. shared ownership
                        li Bus factor risks

                    pre.language-python
                      code.language-python.
                        # Calculate FSI per file
                        fsi = 1 / len(unique_authors_per_file)

                        # High FSI (0.8+): Specialist ownership
                        # Low FSI (0.3-): Collaborative ownership

              .col-md-6
                .card.h-100.border-primary
                  .card-body
                    h4.card-title.h5
                      i.bi.bi-shuffle.me-2
                      | Ownership Entropy
                    p.card-text.
                      Measures how evenly contributions are distributed across
                      authors. High entropy means balanced collaboration; low entropy
                      means concentrated ownership.

                    .alert.alert-info.mb-3
                      small.fw-bold What It Reveals:
                      ul.mb-0.mt-2
                        li Whether "collaboration" is real or superficial
                        li Dominant contributors vs. peripheral participants
                        li Team knowledge distribution patterns

                    pre.language-python
                      code.language-python.
                        # Calculate entropy of contributions
                        from math import log
                        entropy = -sum(p * log(p)
                          for p in commit_shares)

                        # High entropy: Balanced team
                        # Low entropy: Concentrated ownership

            .card.mb-4
              .card-header.bg-primary.text-white
                h4.card-title.mb-0.h5
                  i.bi.bi-link-45deg.me-2
                  | Co-Change Coupling: The Hidden Dependencies
              .card-body
                p.mb-3.
                  Files that change together frequently reveal architectural coupling
                  that may not be obvious from static code analysis. This is particularly
                  valuable for identifying:

                .row.g-3
                  .col-md-4
                    .alert.alert-warning.mb-0
                      strong.d-block.mb-2 Architecture Friction
                      small.
                        Files that shouldn't be coupled but always change together
                        signal architectural problems or missing abstractions.

                  .col-md-4
                    .alert.alert-warning.mb-0
                      strong.d-block.mb-2 Hidden Dependencies
                      small.
                        Coupling between seemingly unrelated modules reveals
                        technical debt and refactoring opportunities.

                  .col-md-4
                    .alert.alert-warning.mb-0
                      strong.d-block.mb-2 Team Coordination Needs
                      small.
                        Frequently co-changed files require coordination between
                        their maintainers, indicating collaboration requirements.

            p.mb-4.
              I discovered this accidentally when analyzing why certain features always
              took longer than estimated. Git analytics revealed that three supposedly
              independent modules had high co-change coupling—they couldn't be modified
              independently in practice, even though the architecture said they could.
              This explained why every "simple" change rippled through the system.
              The problem wasn't developer skill; it was architectural coupling that
              our planning hadn't accounted for.

          section#try-git-spark.mb-5.bg-light.p-4.rounded-3
            h2.h3.mb-4.text-primary.d-flex.align-items-center
              i.bi.bi-rocket-takeoff.me-2
              | Try Git Spark: What I Built and Why

            .row.align-items-center.mb-4
              .col-lg-8
                p.lead.mb-3.
                  Despite failing to measure AI contributions, I succeeded in creating something
                  valuable: an honest Git analytics tool that respects what the data can and
                  cannot tell us. This is <strong>git-spark</strong>, my first npm package.

                p.mb-3.
                  It's early in git-spark's lifetime, and I expect it will evolve as I get
                  feedback from peers. The real question: Have I achieved the goal of making
                  an honest, trustworthy reporting tool that adds value without inventing
                  scores from data that isn't there?

              .col-lg-4
                .card.border-primary.shadow-sm
                  .card-body.text-center
                    i.bi.bi-box-seam.display-4.text-primary.mb-3
                    h5.card-title
                      a(href='https://www.npmjs.com/package/git-spark' target='_blank' rel='noopener noreferrer' aria-label='npm install git-spark') npm install git-spark
                    p.text-muted.small My first published package!

            .card.mb-4.border-success
              .card-header.bg-success.text-white
                h4.card-title.mb-0
                  i.bi.bi-star-fill.me-2
                  | The Git-Spark Promise: Honest Metrics

              .card-body
                p.mb-3.
                  After multiple rewrites and reality checks, here's what git-spark actually does:

                .row.g-4
                  .col-md-6
                    h5.h6.fw-bold.mb-3
                      i.bi.bi-check-circle.text-success.me-2
                      | What It Reports
                    ul.list-unstyled.mb-0
                      li.mb-2
                        i.bi.bi-arrow-right-circle.text-primary.me-2
                        | Observable patterns in commit history
                      li.mb-2
                        i.bi.bi-arrow-right-circle.text-primary.me-2
                        | File coupling and change frequencies
                      li.mb-2
                        i.bi.bi-arrow-right-circle.text-primary.me-2
                        | Author contribution distributions
                      li.mb-2
                        i.bi.bi-arrow-right-circle.text-primary.me-2
                        | Temporal development trends
                      li.mb-2
                        i.bi.bi-arrow-right-circle.text-primary.me-2
                        | Code structure evolution over time

                  .col-md-6
                    h5.h6.fw-bold.mb-3
                      i.bi.bi-shield-check.text-success.me-2
                      | What It Refuses to Infer
                    ul.list-unstyled.mb-0
                      li.mb-2
                        i.bi.bi-x-circle.text-danger.me-2
                        | Fake "health scores" or "productivity ratings"
                      li.mb-2
                        i.bi.bi-x-circle.text-danger.me-2
                        | Developer rankings or comparisons
                      li.mb-2
                        i.bi.bi-x-circle.text-danger.me-2
                        | Code quality judgments from LOC
                      li.mb-2
                        i.bi.bi-x-circle.text-danger.me-2
                        | AI contribution percentages (I learned!)
                      li.mb-2
                        i.bi.bi-x-circle.text-danger.me-2
                        | Anything not directly observable in Git

            .alert.alert-info.mb-4
              strong.d-block.mb-2
                i.bi.bi-lightbulb.me-2
                | What I Learned Building This
              p.mb-2.
                Creating an enterprise-worthy package—something I'd put my name on—taught me
                the critical importance of testing, validation, and organization. More importantly,
                it taught me the discipline of honest reporting.
              p.mb-0.
                Git-spark doesn't tell you whether your repository is "healthy" or your team
                is "productive." Instead, it shows you patterns and lets you interpret them
                with your domain knowledge. That's not a limitation—it's a feature.

            .card.mb-4.bg-gradient-primary.text-white
              .card-body
                h4.card-title.mb-3
                  i.bi.bi-lightning-charge-fill.me-2
                  | Quick Start

                pre.bg-dark.text-light.p-3.rounded.mb-3
                  code.
                    # Install globally
                    npm install -g git-spark

                    # Or use with npx (no install needed)
                    npx git-spark analyze

                    # Analyze your repository
                    cd your-project
                    git-spark analyze --output report.json

                p.mb-0.
                  Get transparent insights into your repository's activity patterns
                  in seconds. No magic formulas, no fake scores—just observable facts
                  from your Git history.

            .row.g-4.mb-4
              .col-md-6
                .card.h-100.border-info
                  .card-body
                    h5.card-title
                      i.bi.bi-book.me-2
                      | Documentation & Examples
                    p.card-text.
                      See how git-spark works, what metrics it calculates, and why it
                      refuses to calculate others. Transparency starts with documentation.
                    a.btn.btn-outline-primary.btn-sm(
                      href='https://markhazleton.github.io/git-spark/',
                      target='_blank',
                      rel='noopener noreferrer',
                      aria-label='View git-spark documentation'
                    )
                      i.bi.bi-arrow-right-circle.me-2
                      | View Documentation

              .col-md-6
                .card.h-100.border-info
                  .card-body
                    h5.card-title
                      i.bi.bi-github.me-2
                      | Open Source on GitHub
                    p.card-text.
                      Built in the open. See exactly how it works. Contribute improvements,
                      report issues, or just browse the code to understand the approach.
                    a.btn.btn-outline-dark.btn-sm(
                      href='https://github.com/MarkHazleton/git-spark',
                      target='_blank',
                      rel='noopener noreferrer',
                      aria-label='View git-spark on GitHub'
                    )
                      i.bi.bi-github.me-2
                      | View on GitHub

            .alert.alert-warning.d-flex.align-items-start
              i.bi.bi-info-circle-fill.fs-4.me-3.mt-1
              div
                h5.alert-heading.mb-2 Still Evolving
                p.mb-0.
                  This is version 1.x—early but functional. I'm actively seeking feedback
                  on what's valuable and what needs rework. If you try it, let me know what
                  you discover. Does it help you understand your development patterns?
                  Are there honest metrics I'm missing? Is there a better way to present
                  the data without inventing meaning?

          section#conclusion.mb-5
            h2.h3.mb-4.text-primary.d-flex.align-items-center
              i.bi.bi-check-circle.me-2
              | Conclusion: What I Learned by Failing Successfully

            p.lead.mb-4.
              I set out to answer a simple question: "How much of my code is generated by
              AI prompts?" I built an entire analytics tool, published my first npm package,
              and learned more about Git internals than I ever expected. And I still can't
              answer that question.

            p.mb-4.
              But that failure taught me something more valuable: the discipline of honest
              measurement. Not every question has a data-driven answer. Not every metric
              is meaningful. And tools that claim to measure everything often measure nothing
              reliably.

            .card.mb-4.bg-light
              .card-body
                h3.h5.fw-bold.mb-3
                  i.bi.bi-award.me-2
                  | What This Journey Actually Produced

                .row.g-4
                  .col-md-6
                    h4.h6.text-success.mb-3 Technical Achievements
                    ul.mb-0
                      li.mb-2 My first published npm package
                      li.mb-2 Understanding of Git internals
                      li.mb-2 Experience with testing and validation
                      li.mb-2 Skills in package organization
                      li.mb-2 Real-world TypeScript/JavaScript development

                  .col-md-6
                    h4.h6.text-primary.mb-3 Conceptual Insights
                    ul.mb-0
                      li.mb-2 Limits of Git-based metrics
                      li.mb-2 Difference between motion and progress
                      li.mb-2 Value of honest vs. authoritative reporting
                      li.mb-2 Why AI contributions remain invisible
                      li.mb-2 What makes metrics trustworthy

            p.mb-4.
              Git-spark doesn't tell you if your team is productive. It doesn't measure
              code quality. It can't identify AI-generated code. What it does is report
              observable patterns in your repository's history and then—crucially—shuts up.
              No fake scores. No pretend insights. Just data and the humility to admit its
              limitations.

            .alert.alert-info.mb-4
              strong.d-block.mb-2
                i.bi.bi-lightbulb.me-2
                | The Real Question Changed
              p.mb-2.
                I started asking: <em>"Are we productive?"</em>
              p.mb-2.
                I learned to ask: <em>"Are we set up to succeed?"</em>
              p.mb-0.
                That second question requires context, judgment, and human interpretation—
                things no tool can provide. Git-spark gives you the data; you bring the wisdom.

            p.mb-4.
              So yes, I failed to measure AI contributions. But I succeeded in creating
              something I'm proud to put my name on: an honest reporter that respects both
              the data and your intelligence. As git-spark evolves with feedback from the
              community, that commitment to honesty remains the foundation.

            .card.mb-4.border-primary
              .card-header.bg-primary.text-white
                h4.card-title.mb-0
                  i.bi.bi-compass.me-2
                  | Where to Go from Here
              .card-body
                p.mb-3.
                  If you're struggling with similar questions about measuring development
                  productivity, AI contributions, or team performance:

                ol.mb-0
                  li.mb-3
                    strong Start with honesty:
                    |  What can you actually observe vs. what are you inferring?
                  li.mb-3
                    strong Question the scores:
                    |  Any tool claiming to measure "health" or "productivity" with a single number is oversimplifying.
                  li.mb-3
                    strong Try git-spark:
                    |  See what Git history actually reveals about your development patterns.
                  li.mb-3
                    strong Interpret with context:
                    |  Patterns mean different things for different teams. Your judgment matters more than any formula.
                  li.mb-3
                    strong Share your findings:
                    |  I'm still learning. What patterns do you find valuable? What am I missing?

            p.mb-4.
              Building git-spark taught me that the best metrics tools don't pretend to
              have all the answers. They provide honest data and trust you to ask better
              questions. That's the philosophy behind every line of code in git-spark,
              and I hope it's useful to others wrestling with these same measurement challenges.

            .text-center.mt-5
              a.btn.btn-primary.btn-lg.me-3(
                href='https://github.com/MarkHazleton/git-spark',
                target='_blank',
                rel='noopener noreferrer',
                role='button',
                aria-label='Try git-spark on GitHub'
              )
                i.bi.bi-github.me-2
                | Try git-spark
              a.btn.btn-outline-primary.btn-lg(
                href='/#contact',
                role='button',
                aria-label='Get in touch to discuss engineering metrics'
              )
                i.bi.bi-chat-dots.me-2
                | Discuss Metrics

