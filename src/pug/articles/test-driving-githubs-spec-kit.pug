extends ../layouts/modern-layout

//- Import standardized article mixins
include ../modules/article-mixins

block layout-content
  br
  // Hero Section
  section.bg-gradient-primary.py-5
    .container
      .row.align-items-center
        .col-lg-10.mx-auto.text-center
          h1.display-4.fw-bold.mb-3
            i.bi.bi-lightbulb.me-3
            | GitHub Spec Kit
          h2.mb-4 Test Driving GitHub Spec Kit: From Three-Ring Binders to Living Documentation

  //- Main Article Content
  article#main-article
    .container
      .row
        .col-lg-8.mx-auto

          p.lead.mb-4.text-center.mt-4.
            Using structured specifications to guide AI code generation with measurable results.

          //- Table of contents
          +tableOfContents([
            { href: '#executive-summary', text: 'Executive Summary' },
            { href: '#from-vibe-coding', text: 'The Problem: Specs Always Become Outdated' },
            { href: '#what-is-spec-kit', text: 'What Is GitHub Spec Kit?' },
            { href: '#case-study', text: 'Case Study: WebSpark.HttpClientUtility' },
            { href: '#results', text: 'Results: Zero Documentation Debt' },
            { href: '#ai-struggles', text: 'The Feedback Loop in Practice' },
            { href: '#workflow', text: 'Following the Spec Kit Flow' },
            { href: '#conclusion', text: 'When to Use Spec Kit' }
          ])

          section#executive-summary.mb-5
            h2.h3.mb-4
              i.bi.bi-file-earmark-text.me-2
              | Executive Summary

            p.lead.mb-4.
              Every developer knows the pattern: Design document says one thing, code does another, six months later nobody knows which is correct. We tried waterfall's rigid specs and agile's no-specs approach. Both failed for the same reason—humans won't maintain documentation when it's divorced from implementation.

            p.mb-4.
              GitHub Spec Kit offers a third option: let AI agents maintain the feedback loop we never could. When you fix bugs or tweak implementations, you tell the agent to update the spec. Documentation evolves to match reality instead of becoming outdated fiction.

            .card.mb-4
              .card-header.bg-light
                h5.card-title.mb-0
                  i.bi.bi-trophy.me-2
                  | What You'll Learn
              .card-body
                ul.mb-0
                  li
                    strong The post-implementation feedback loop:
                    |  How AI agents keep specs synchronized with reality when you make changes
                  li
                    strong Zero documentation debt:
                    |  Why the ROI isn't speed—it's having specs that are still accurate a year later
                  li
                    strong When to use it:
                    |  Libraries, APIs, multi-year projects where institutional knowledge matters
                  li
                    strong When to skip it:
                    |  Throwaway prototypes or solo projects you'll rewrite in 6 months
                  li
                    strong Real metrics:
                    |  Implementation time stayed the same, but documentation stayed accurate

            .card.mb-4
              .card-header.bg-light
                h5.card-title.mb-0
                  i.bi.bi-people.me-2
                  | Who This Is For
              .card-body
                ul.mb-0
                  li
                    strong Solutions Architects:
                    |  Translate business requirements into technology with precision
                  li
                    strong Development Teams:
                    |  Escape the prompt-generate-debug cycle with structured workflows
                  li
                    strong Engineering Leaders:
                    |  Build institutional knowledge that scales beyond individual contributors
                  li
                    strong .NET Developers:
                    |  Practical patterns for NuGet packages, documentation, and quality enforcement

            .alert.alert-info.mb-0
              h6.alert-heading
                i.bi.bi-link-45deg.me-2
                | References
              ul.mb-0
                li
                  | GitHub Spec Kit: 
                  a(href='https://github.com/github/spec-kit', target='_blank', rel='noopener') https://github.com/github/spec-kit
                li
                  | GitHub Copilot: 
                  a(href='https://github.com/features/copilot', target='_blank', rel='noopener') https://github.com/features/copilot
                li
                  | WebSpark.HttpClientUtility Repository: 
                  a(href='https://github.com/markhazleton/WebSpark.HttpClientUtility', target='_blank', rel='noopener') Real-world example project
                li
                  | GitHub Actions: 
                  a(href='https://docs.github.com/actions', target='_blank', rel='noopener') https://docs.github.com/actions
                li
                  | NuGet Publishing: 
                  a(href='https://learn.microsoft.com/nuget/create-packages/publish-a-package', target='_blank', rel='noopener') https://learn.microsoft.com/nuget/create-packages/publish-a-package

          section#from-vibe-coding.mb-5
            h2.h3.mb-4
              i.bi.bi-arrow-right-circle.me-2
              | The Problem: Specs Always Become Outdated

            p.mb-3.
              You write a detailed specification. Implementation starts. Then reality hits: edge cases appear, requirements shift, better solutions emerge. You fix the code, ship the feature, and move on. Six months later, a new developer reads your spec and wastes three hours implementing something that doesn't match what you actually built.

            p.mb-3.
              This isn't laziness—it's human nature. Manually keeping documentation synchronized with code requires discipline that nobody maintains under deadline pressure. Waterfall tried to solve this with rigid up-front specs. Agile gave up and said "the code is the documentation." Both approaches fail when institutional knowledge matters.

            h3.h5.mb-3 The Root Cause: Humans Won't Update Docs Post-Implementation

            p.mb-3.
              Here's the brutal truth: after you spend 7 hours implementing a feature and tweaking it until it works, you're not going back to update a spec document. You're moving to the next task. The spec becomes fiction, tribal knowledge accumulates, and the next developer repeats your mistakes.

            h3.h5.mb-3 The Solution: AI Agents in the Feedback Loop

            p.mb-3.
              GitHub Spec Kit doesn't eliminate iteration—it ensures iteration improves documentation instead of destroying it. After <code>/speckit.implement</code> finishes and you fix the inevitable bugs, you tell the agent: "Update SPEC.md to reflect the relativePath solution we actually shipped." Takes 20 minutes. The spec evolves to match reality.

            p.mb-3.
              Here's the key insight: GitHub Spec Kit value isn't faster development—it's zero documentation debt. When the next developer (or future you) reads the spec, it describes what actually works, not what you planned before reality intervened.

            h3.h5.mb-3 What This Means in Practice

            p.mb-0.
              I completed two production features using this approach. Implementation took the same 7 hours it always does. But updating specs to match what I actually built took 20 minutes instead of never happening. Traditional approach: 7 hours implementation, specs become outdated instantly, next developer wastes 3 hours reconciling code vs. docs. GitHub Spec Kit advantage: 7 hours implementation + 20 minutes documentation = zero documentation debt.

          section#what-is-spec-kit.mb-5
            h2.h3.mb-4
              i.bi.bi-info-circle.me-2
              | What Is GitHub Spec Kit?

            p.mb-3.
              GitHub Spec Kit structures AI development into phases that generate persistent documentation. Instead of prompting until code works, you run commands that create markdown artifacts: <code>/speckit.specify</code> captures requirements in SPEC.md, <code>/speckit.plan</code> determines technical approach in PLAN.md, <code>/speckit.tasks</code> breaks work into TASKS.md, and <code>/speckit.implement</code> generates code.

            p.mb-3.
              These files live alongside your code in <code>specifications/001-feature-name/</code>. The structure forces you to think before coding—but more importantly, the files become living documentation. When implementation deviates from the plan (and it always does), you tell the AI to update the specs to match reality.

            p.mb-3.
              A <code>/speckit.constitution</code> file defines project-wide standards once, guiding all generation. Optional commands like <code>/speckit.clarify</code> catch ambiguities before implementation. The full toolkit is open-source (MIT) at <a href="https://github.com/github/spec-kit" target="_blank" rel="noopener">github/spec-kit</a>.

            .alert.alert-info.mb-4
              h6.alert-heading.mb-2
                i.bi.bi-lightbulb.me-2
                | Why This Matters for Documentation Sync
              p.mb-0.
                This structure solves a problem traditional specs couldn't: <strong>maintaining sync between documentation and code.</strong> My waterfall-era specs at EDS became outdated the moment developers started coding. Agile threw out specs entirely rather than maintain them. GitHub Spec Kit offers a third path—AI agents that update specs when implementation changes. Here's what happened when I tested that theory on a production NuGet package.

          section#case-study.mb-5
            h2.h3.mb-4
              i.bi.bi-diagram-3.me-2
              | Case Study: WebSpark.HttpClientUtility — A Production .NET NuGet Package

            p.mb-3.
              I applied Spec Kit to WebSpark.HttpClientUtility, a .NET 8/9 NuGet package with decorator-pattern HTTP client utilities. Results from two specs:

            .row.g-3.mb-4
              .col-md-6
                .card.h-100.border-primary
                  .card-header.bg-primary.text-white
                    h5.card-title.mb-0
                      i.bi.bi-file-text.me-2
                      | Spec 001: Static Documentation Site
                  .card-body
                    p Built complete Eleventy-based documentation website
                    ul.small.mb-0
                      li 6 pages with responsive design
                      li Live NuGet API integration
                      li GitHub Pages deployment
                      li Build time: 0.4 seconds

              .col-md-6
                .card.h-100.border-success
                  .card-header.bg-success.text-white
                    h5.card-title.mb-0
                      i.bi.bi-check-circle.me-2
                      | Spec 002: Zero Compiler Warnings
                  .card-body
                    p Achieved professional quality baseline
                    ul.small.mb-0
                      li 0 warnings, 0 errors
                      li 520/520 tests passing
                      li TreatWarningsAsErrors enabled
                      li XML docs for all public APIs

            p.mb-0.
              I'll walk through both specs, showing how Spec Kit moved me from ambiguous goals to shipped releases (v1.5.0 and v1.5.1).

          section#repository-layout.mb-5
            h2.h3.mb-4
              i.bi.bi-folder.me-2
              | Repository Layout

            p.mb-3.
              Here's the repository structure that emerged from the spec-driven process. Note the distinction between <code>.specify/</code> (Spec Kit framework and templates) and <code>specifications/</code> (your actual project specs). Each spec directory contains the complete spec-plan-tasks workflow artifacts:

            .alert.alert-info.mb-3
              h6.alert-heading.mb-2
                i.bi.bi-info-circle.me-2
                | Understanding the Structure
              ul.mb-0.small
                li
                  strong .specify/:
                  |  Framework files (constitution, templates, scripts)
                li
                  strong specifications/:
                  |  Your actual project specs with full workflow artifacts
                li
                  strong Why 1,648 lines of tasks.md?
                  |  Documentation site spec broke down into granular, testable tasks across HTML, CSS, JS, CI/CD
                li
                  strong copilot-instructions.md (244 lines):
                  |  Project-specific AI guidance including coding standards, architecture decisions, and common patterns

            pre.language-plaintext.bg-dark.text-light.p-3.rounded
              code.language-plaintext.text-light.
                WebSpark.HttpClientUtility/
                ├─ .specify/                      # Spec Kit framework
                │  ├─ memory/constitution.md      # Project principles
                │  ├─ templates/                  # Spec templates
                │  └─ scripts/                    # Automation scripts
                ├─ specifications/
                │  ├─ 001-static-documentation-site/
                │  │  ├─ spec.md                  # 724 lines
                │  │  ├─ plan.md                  # 835 lines
                │  │  ├─ tasks.md                 # 1,648 lines
                │  │  └─ data-model.md
                │  └─ 002-clean-compiler-warnings/
                │     ├─ spec.md                  # 120 lines
                │     ├─ plan.md                  # 258 lines
                │     └─ tasks.md                 # 332 lines
                ├─ src/                           # Library source
                │  └─ WebSpark.HttpClientUtility/
                │     ├─ ClientService/
                │     ├─ Crawler/
                │     ├─ MemoryCache/
                │     └─ Streaming/
                ├─ test/                          # 520 tests (×2 frameworks)
                │  └─ WebSpark.HttpClientUtility.Test/
                ├─ docs/                          # Generated documentation
                │  ├─ index.html
                │  ├─ getting-started/
                │  ├─ api/
                │  └─ examples/
                ├─ .github/
                │  ├─ workflows/
                │  │  ├─ dotnet.yml              # CI/CD pipeline
                │  │  └─ publish-docs.yml        # Doc deployment
                │  └─ copilot-instructions.md    # 244 lines
                └─ Directory.Build.props         # Solution-wide config

          section#the-spec.mb-5
            h2.h3.mb-4
              i.bi.bi-file-text.me-2
              | The Spec: Spec 002 - Clean Compiler Warnings

            p.mb-3.
              Below is the actual spec that drove me from "unknown number of warnings" to zero warnings with enforcement enabled. It's intentionally explicit and measurable.

            pre.language-markdown.bg-dark.text-light.p-3.rounded
              code.language-markdown.text-light.
                # Spec 002: Clean Compiler Warnings

                ## Summary
                Achieve zero compiler warnings across all three projects in the WebSpark.HttpClientUtility solution and enable TreatWarningsAsErrors for CI/CD enforcement.

                Target frameworks: net8.0, net9.0
                Projects: Library, Test, Web App

                ## Goals
                - Zero compiler warnings in Release and Debug configurations
                - Enable TreatWarningsAsErrors solution-wide
                - Maintain 100% test pass rate (520 tests × 2 frameworks)
                - Comprehensive XML documentation for all public APIs
                - Professional quality baseline for NuGet package

                ## Non-Goals
                - Suppress warnings without fixing root causes
                - Compromise API design to avoid warnings
                - Skip test documentation (treat tests as product)
                - Delay enforcement—enable immediately after cleanup

                ## Constraints
                - Cannot break existing public API contracts
                - Cannot reduce test coverage
                - Must target both net8.0 and net9.0
                - Must pass all 520 existing tests
                - Changes must be backward compatible

                ## Current State Analysis Required
                1. Run `dotnet build -c Release -v detailed > build_warnings.txt`
                2. Categorize warnings by type (CS1591, CS8602, CA2007, etc.)
                3. Prioritize: Public API docs > Null safety > Code analysis
                4. Document baseline count per category

                ## Acceptance Criteria
                - `dotnet build` produces 0 warnings and 0 errors
                - All 520 tests pass on net8.0 and net9.0
                - `TreatWarningsAsErrors` enabled in Directory.Build.props
                - XML docs for all public classes, methods, properties
                - Null reference warnings resolved (not suppressed)
                - Code analysis rules properly configured in .editorconfig

                ## File Plan
                - src/WebSpark.HttpClientUtility/**/*.cs (add XML docs, null checks)
                - test/WebSpark.HttpClientUtility.Test/**/*.cs (document test intent)
                - Directory.Build.props (enable TreatWarningsAsErrors)
                - .editorconfig (configure analyzer severities)
                - Build verification scripts

                ## Done Definition
                - Build log shows "0 Warning(s)"
                - Test output shows "520 passed"
                - CI/CD pipeline passes with TreatWarningsAsErrors
                - No #pragma warning disable directives added
                - Documentation complete for all public surface area

            p.mb-3.
              Notice how the spec avoids prescribing HOW to fix warnings—it defines the target state and constraints, letting the implementer (human or AI) determine the optimal approach.

            .alert.alert-warning.mb-0
              h6.alert-heading.mb-2
                i.bi.bi-exclamation-triangle.me-2
                | What I Learned: Spec Weaknesses
              p.mb-2 This spec worked well, but I made mistakes that cost time:
              ul.mb-0
                li
                  strong Too vague on "unknown baseline":
                  |  Should have run the audit FIRST and documented exact warning counts by category
                li
                  strong Missing priority order:
                  |  AI tried to fix everything simultaneously. Should have specified: "Fix CS1591 docs first, then null safety, then code analysis"
                li
                  strong No time estimate:
                  |  Without "Target: 4 hours" in the spec, I lost focus during implementation
                li
                  strong Lesson:
                  |  Even good specs have gaps. The feedback loop caught these issues, and I updated the spec after implementation to reflect what actually worked.

          section#implementation.mb-5
            h2.h3.mb-4
              i.bi.bi-code-slash.me-2
              | Driving Implementation with the Spec

            p.mb-3.
              With SPEC.md in place, Copilot had a clear target and generated the implementation according to the spec's requirements. The actual code changes involved adding XML documentation, implementing null guards, and configuring analyzer rules.

            .alert.alert-info.mb-4
              h6.alert-heading.mb-2
                i.bi.bi-github.me-2
                | View Implementation Details
              p.mb-0
                | For complete before/after code changes, see the 
                a.text-decoration-none(href='https://github.com/markhazleton/WebSpark.HttpClientUtility' target='_blank' rel='noopener') WebSpark.HttpClientUtility repository
                |  and review the commit history for Spec 001 (documentation) and Spec 002 (zero warnings).

          section#tests.mb-5
            h2.h3.mb-4
              i.bi.bi-check2-square.me-2
              | Tests as Acceptance Criteria

            p.mb-3.
              Writing tests from the spec gave Copilot unambiguous targets. The existing 520-test suite (260 tests × 2 frameworks) served as acceptance criteria, ensuring no regressions while adding XML documentation and null guards.

            .alert.alert-success.mb-4
              h6.alert-heading.mb-2
                i.bi.bi-check-circle-fill.me-2
                | Test Coverage
              ul.mb-0
                li
                  strong 520 tests passing
                  |  (260 tests across .NET 8.0 and 9.0)
                li
                  strong Zero test failures
                  |  throughout implementation
                li
                  strong XML documentation tests
                  |  validate all public APIs documented
                li
                  strong Parameter validation tests
                  |  ensure ArgumentNullException coverage

            p.mb-0.
              The spec's constraint "Must pass all 520 existing tests" made the test suite non-negotiable, preventing shortcuts that might have broken existing functionality.

          section#packaging.mb-5
            h2.h3.mb-4
              i.bi.bi-box-seam.me-2
              | Packaging for NuGet

            p.mb-4.
              One of the most valuable outcomes of the Spec Kit approach was integrating CI/CD directly into the "Done Definition." GitHub Actions workflows became gatekeepers that validated every change before allowing new versions to ship.

            .row.g-3.mb-4
              .col-lg-6
                .card.border-primary.h-100
                  .card-header.bg-light
                    h5.card-title.mb-0
                      i.bi.bi-play-circle.me-2.text-primary
                      | CI Pipeline (Continuous Integration)
                  .card-body
                    p.mb-3 Runs on every push and pull request to validate code quality:
                    ul.mb-0
                      li
                        strong Restore dependencies
                        |  - Ensure all packages resolve correctly
                      li
                        strong Build solution
                        |  - Compile with TreatWarningsAsErrors enabled
                      li
                        strong Run 520 tests
                        |  - Execute full test suite across net8.0 and net9.0
                      li
                        strong Code coverage
                        |  - Track test coverage metrics
                      li
                        strong Fail fast
                        |  - Block PRs if any step fails

              .col-lg-6
                .card.border-success.h-100
                  .card-header.bg-light
                    h5.card-title.mb-0
                      i.bi.bi-box-seam.me-2.text-success
                      | Publish Pipeline (Release)
                  .card-body
                    p.mb-3 Triggered by version tags (e.g., v1.5.0) to deploy to NuGet.org:
                    ul.mb-0
                      li
                        strong Build Release configuration
                        |  - Full optimization enabled
                      li
                        strong Pack NuGet package
                        |  - Generate .nupkg with metadata
                      li
                        strong Run final tests
                        |  - Last validation before publish
                      li
                        strong Push to NuGet.org
                        |  - Automated deployment with API key
                      li
                        strong Skip duplicates
                        |  - Prevent accidental republish

            .alert.alert-info.mb-4
              h6.alert-heading.mb-2
                i.bi.bi-shield-check.me-2
                | Quality Gates in Action
              p.mb-2 The CI/CD pipeline enforces the spec's constraints automatically:
              ul.mb-0
                li
                  strong Zero warnings requirement
                  |  - Build fails if warnings appear (TreatWarningsAsErrors)
                li
                  strong Test coverage mandate
                  |  - 520 tests must pass before any merge
                li
                  strong API contract validation
                  |  - Tests prevent breaking changes
                li
                  strong Manual release control
                  |  - No NuGet publish without explicit version tag

            p.mb-0.
              This automation simplified the release process dramatically. Instead of manually running tests, checking warnings, packing, and publishing, a single 
              code git tag v1.5.1
              |  command triggers the entire validated pipeline. The GitHub Spec Kit "Done Definition" became executable infrastructure, not just documentation. For complete workflow details, see the 
              a.text-decoration-none(href='https://github.com/markhazleton/WebSpark.HttpClientUtility/tree/main/.github/workflows' target='_blank' rel='noopener') .github/workflows directory
              |  in the repository.

          section#results.mb-5
            h2.h3.mb-4
              i.bi.bi-graph-up.me-2
              | Results: What Changed with Spec Kit

            p.mb-3.
              Over the course of spec-driven development on WebSpark.HttpClientUtility, I achieved measurable results across two major specifications. The key insight: implementation time stayed the same, but documentation stayed accurate.

            .alert.alert-success.mb-4
              h5.alert-heading
                i.bi.bi-check-circle-fill.me-2
                | Quantitative Outcomes
              ul.mb-0
                li
                  strong 136 files changed 
                  | (29,141 insertions, 3,167 deletions)
                li
                  strong 2 specs completed 
                  | (Spec 001: Documentation Site, Spec 002: Zero Warnings)
                li
                  strong 2 releases shipped 
                  | (v1.5.0: Documentation, v1.5.1: Quality)
                li
                  strong 0 warnings, 0 errors 
                  | (from unknown baseline)
                li
                  strong 520/520 tests passing 
                  | (260 tests × 2 frameworks: net8.0 + net9.0)
                li
                  strong 20 minutes to sync specs 
                  | (vs. never updating them in traditional approach)

            h3.h6.mb-3 The Real ROI: Zero Documentation Debt

            p.mb-3.
              Implementation took 7 hours (3 hours for documentation site, 4 hours for warning cleanup). In a traditional approach, that would be 7 hours of implementation with specs immediately becoming outdated. With Spec Kit, I spent an additional 20 minutes having the agent update specs to match what I actually built.

            p.mb-3.
              The payoff: next developer (or future me) doesn't waste 3 hours reconciling outdated specs with actual code. They read accurate documentation that describes what actually works. Over a year with multiple developers, this compounds significantly.

            h3.h6.mb-3 Spec 001: Static Documentation Site

            .row.g-3.mb-4
              .col-md-4
                .card.border-primary.h-100
                  .card-header.bg-primary.text-white
                    strong Speed
                  .card-body
                    p.display-6.text-primary.mb-2 3 hours
                    p.small.text-muted.mb-0 From spec to deployed site

              .col-md-4
                .card.border-success.h-100
                  .card-header.bg-success.text-white
                    strong Quality
                  .card-body
                    p.display-6.text-success.mb-2 95+
                    p.small.text-muted.mb-0 Lighthouse performance score

              .col-md-4
                .card.border-info.h-100
                  .card-header.bg-info.text-white
                    strong Scope
                  .card-body
                    p.display-6.text-info.mb-2 6 pages
                    p.small.text-muted.mb-0 With live NuGet API integration

            h3.h6.mb-3 Spec 002: Clean Compiler Warnings

            .row.g-3.mb-4
              .col-md-4
                .card.border-primary.h-100
                  .card-header.bg-primary.text-white
                    strong Warnings Fixed
                  .card-body
                    p.display-6.text-primary.mb-2 Unknown → 0
                    p.small.text-muted.mb-0 All categories addressed

              .col-md-4
                .card.border-success.h-100
                  .card-header.bg-success.text-white
                    strong Test Pass Rate
                  .card-body
                    p.display-6.text-success.mb-2 100%
                    p.small.text-muted.mb-0 520/520 tests across 2 frameworks

              .col-md-4
                .card.border-info.h-100
                  .card-header.bg-info.text-white
                    strong Time to Green
                  .card-body
                    p.display-6.text-info.mb-2 4 hours
                    p.small.text-muted.mb-0 Including docs and enforcement

            h3.h6.mb-3 Comparison: Spec Kit vs. Ad Hoc Approach

            .table-responsive.mb-3
              table.table.table-bordered.table-striped(aria-label='Spec Kit vs Ad Hoc comparison')
                thead.table-light
                  tr
                    th Metric
                    th With Spec Kit
                    th Typical Ad Hoc
                    th Long-Term Impact
                tbody
                  tr
                    td Implementation time
                    td 7 hours
                    td 7 hours
                    td No difference
                  tr
                    td Documentation sync time
                    td 20 minutes (AI-assisted)
                    td Never happens
                    td Zero debt vs. compounding confusion
                  tr
                    td Spec accuracy after 6 months
                    td Matches implementation
                    td Outdated fiction
                    td New developers trust docs
                  tr
                    td Onboarding cost
                    td Read accurate specs
                    td Reverse-engineer from code
                    td 3 hours saved per developer
                  tr
                    td AI hallucinations
                    td Minimal (spec-constrained)
                    td Frequent
                    td 80% reduction in rework
                  tr
                    td Test stability
                    td 100% pass rate maintained
                    td Regression issues common
                    td Zero regressions

            p.mb-0.
              The spec-driven approach doesn't make you faster—it makes your documentation trustworthy. When you return to the codebase a year later, or when a new team member joins, the specs accurately describe what was built and why.

          section#ai-struggles.mb-5
            h2.h3.mb-4
              i.bi.bi-arrow-repeat.me-2
              | The Feedback Loop in Practice: Wrong → Fix → Update → Sync

            p.mb-3.
              Here's what makes GitHub Spec Kit different: when AI generates wrong code, you don't just fix the code—you update the specs to reflect what actually works.

            .row.g-3.mb-4
              .col-md-6
                .card.border-secondary.h-100
                  .card-header.bg-light
                    h6.card-title.mb-0
                      i.bi.bi-x-circle.me-2.text-danger
                      | Traditional Process
                  .card-body.small
                    ol.mb-0
                      li Developer fixes bug
                      li Knowledge lives in their head
                      li Documentation drifts
                      li Next developer repeats mistake

              .col-md-6
                .card.border-success.h-100
                  .card-header.bg-light
                    h6.card-title.mb-0
                      i.bi.bi-check-circle.me-2.text-success
                      | Spec Kit Process
                  .card-body.small
                    ol.mb-0
                      li AI generates from spec
                      li Human fixes code
                      li AI updates spec/plan/tasks
                      li Knowledge persists in repository

            h3.h5.mb-3 Three Cycles That Became Institutional Knowledge

            .card.mb-3
              .card-header.bg-light
                h6.card-title.mb-0
                  i.bi.bi-1-circle.me-2
                  | Path Resolution: Spec Said One Thing, Reality Required Another
              .card-body
                ul.list-unstyled.mb-0
                  li
                    strong AI generated:
                    |  Absolute paths using pathPrefix config (standard Eleventy approach)
                  li
                    strong What broke:
                    |  GitHub Pages subdirectory deployment
                  li
                    strong I fixed it:
                    |  Custom <code>relativePath</code> filter that calculates paths dynamically
                  li
                    strong Then I closed the loop:
                    |  "Update SPEC.md and PLAN.md to document why pathPrefix failed and what works instead"
                  li.mt-2
                    strong Result:
                    |  SPEC.md now says "No environment-specific configuration." PLAN.md shows pathPrefix crossed out with the working alternative. Next developer won't try pathPrefix because the spec explains why it doesn't work.

            .card.mb-3
              .card-header.bg-light
                h6.card-title.mb-0
                  i.bi.bi-2-circle.me-2
                  | Warning Suppression: Spec Was Too Vague
              .card-body
                ul.list-unstyled.mb-0
                  li
                    strong AI generated:
                    |  <code>#pragma warning disable</code> directives (fastest solution)
                  li
                    strong Spec said:
                    |  "No suppressions" but didn't say HOW to fix properly
                  li
                    strong I fixed it:
                    |  200+ XML docs, null guards with <code>ArgumentNullException.ThrowIfNull()</code>
                  li
                    strong Then I closed the loop:
                    |  "Update SPEC.md with specific examples of acceptable vs. unacceptable fixes"
                  li.mt-2
                    strong Result:
                    |  SPEC.md now has a "✅ DO / ❌ DON'T" section. PLAN.md has a 5-step remediation strategy. TASKS.md breaks it into auditable chunks. Future features inherit this standard.

            .card.mb-3
              .card-header.bg-light
                h6.card-title.mb-0
                  i.bi.bi-3-circle.me-2
                  | Test Documentation: Spec Didn't Ask, AI Didn't Deliver
              .card-body
                ul.list-unstyled.mb-0
                  li
                    strong AI generated:
                    |  Documented library code, skipped test methods entirely
                  li
                    strong Spec said:
                    |  "520 tests passing" but not "tests need documentation"
                  li
                    strong I fixed it:
                    |  Added XML docs to 260 test methods explaining WHAT and WHY
                  li
                    strong Then I closed the loop:
                    |  "Update SPEC.md to require test documentation. Add principle to CONSTITUTION.md: 'Tests are product documentation.'"
                  li.mt-2
                    strong Result:
                    |  Every future spec inherits "tests need docs" standard. AI reads the constitution before generating code. The team's quality bar persists beyond individual developers.

            h3.h5.mb-3 Why This Matters

            p.mb-0.
              In waterfall, specs froze at design and diverged immediately. In agile, we stopped writing specs because maintaining them was impossible. GitHub Spec Kit closes the loop: when implementation teaches you something, you spend 5 minutes having AI update the specs. The path resolution lesson, the warning fix patterns, the test documentation standard—all permanent institutional knowledge, not tribal knowledge that walks out the door.

          section#faq.mb-5
            h2.h3.mb-4
              i.bi.bi-question-circle.me-2
              | Frequently Asked Questions

            .accordion#faqAccordion
              .accordion-item
                h3.accordion-header#faq1
                  button.accordion-button.collapsed(
                    type='button'
                    data-bs-toggle='collapse'
                    data-bs-target='#collapse1'
                    aria-expanded='false'
                    aria-controls='collapse1'
                    aria-label='Does this only work with GitHub Copilot?'
                  )
                    i.bi.bi-question-circle-fill.me-2
                    | Does this only work with GitHub Copilot?
                .accordion-collapse.collapse#collapse1(
                  aria-labelledby='faq1'
                  data-bs-parent='#faqAccordion'
                )
                  .accordion-body.
                    No. The pattern is model-agnostic. Any LLM benefits from structured specs and tests.

              .accordion-item
                h3.accordion-header#faq2
                  button.accordion-button.collapsed(
                    type='button'
                    data-bs-toggle='collapse'
                    data-bs-target='#collapse2'
                    aria-expanded='false'
                    aria-controls='collapse2'
                    aria-label='Is this just test-driven development?'
                  )
                    i.bi.bi-question-circle-fill.me-2
                    | Isn't this just test-driven development?
                .accordion-collapse.collapse#collapse2(
                  aria-labelledby='faq2'
                  data-bs-parent='#faqAccordion'
                )
                  .accordion-body.
                    It's complementary. Spec Kit codifies requirements and examples up front, then TDD validates them. The twist is that you're writing for humans and an AI partner simultaneously.

              .accordion-item
                h3.accordion-header#faq3
                  button.accordion-button.collapsed(
                    type='button'
                    data-bs-toggle='collapse'
                    data-bs-target='#collapse3'
                    aria-expanded='false'
                    aria-controls='collapse3'
                    aria-label='What if my problem is too open-ended for a spec?'
                  )
                    i.bi.bi-question-circle-fill.me-2
                    | What if my problem is too open-ended for a spec?
                .accordion-collapse.collapse#collapse3(
                  aria-labelledby='faq3'
                  data-bs-parent='#faqAccordion'
                )
                  .accordion-body.
                    Break it into spec-able slices. Use research spikes to learn, then spec the actionable parts.

              .accordion-item
                h3.accordion-header#faq4
                  button.accordion-button.collapsed(
                    type='button'
                    data-bs-toggle='collapse'
                    data-bs-target='#collapse4'
                    aria-expanded='false'
                    aria-controls='collapse4'
                    aria-label='What if Copilot still gets it wrong?'
                  )
                    i.bi.bi-question-circle-fill.me-2
                    | What if Copilot still gets it wrong?
                .accordion-collapse.collapse#collapse4(
                  aria-labelledby='faq4'
                  data-bs-parent='#faqAccordion'
                )
                  .accordion-body.
                    Tighten the spec, add failing tests for the misbehavior, and iterate. Avoid changing code and spec in opposite directions.

          section#workflow.mb-5
            h2.h3.mb-4
              i.bi.bi-arrow-repeat.me-2
              | Following the Spec Kit Flow

            p.mb-4.
              Ready to try it yourself? Here's the recommended workflow with the slash commands that guide Copilot through each phase. Each command has a specific purpose and builds on the previous steps.

            .table-responsive.mb-4
              table.table.table-bordered.table-hover(aria-label='Spec Kit workflow commands')
                thead.table-light
                  tr
                    th.text-center Step
                    th Command
                    th Purpose
                    th When to Use
                tbody
                  tr
                    td.text-center.fw-bold 1
                    td
                      code.text-primary /speckit.constitution
                    td Define your quality principles and coding standards
                    td Once per project - sets foundational guidelines
                  tr
                    td.text-center.fw-bold 2
                    td
                      code.text-primary /speckit.specify
                    td Declare WHAT to build and WHY it matters
                    td Start of each feature/spec - defines the goal
                  tr
                    td.text-center.fw-bold 3
                    td
                      code.text-primary /speckit.clarify
                    td Answer ambiguities and edge cases
                    td After specify - usually just once to refine requirements
                  tr
                    td.text-center.fw-bold 4
                    td
                      code.text-primary /speckit.plan
                    td Determine HOW to implement (tools, versions, approach)
                    td Before coding - establishes technical strategy
                  tr
                    td.text-center.fw-bold 5
                    td
                      code.text-primary /speckit.tasks
                    td Break work into discrete, testable chunks
                    td After planning - creates actionable task list
                  tr
                    td.text-center.fw-bold 6
                    td
                      code.text-primary /speckit.analyze
                    td Optional sanity check before execution
                    td For complex specs - validates approach before commit
                  tr
                    td.text-center.fw-bold 7
                    td
                      code.text-primary /speckit.implement
                    td Execute the plan with Copilot
                    td Final step - let AI generate the code

            .alert.alert-info.mb-4
              h6.alert-heading.mb-2
                i.bi.bi-info-circle.me-2
                | Example Flow: Warning Cleanup Spec
              p.mb-2.
                For a spec like "eliminate all compiler warnings", you'd follow this sequence:
              ol.mb-0
                li
                  strong /speckit.constitution:
                  |  "We treat warnings as errors. No pragma suppressions without justification."
                li
                  strong /speckit.specify:
                  |  "WHAT: Zero warnings with TreatWarningsAsErrors enabled. WHY: Professional code quality and prevent tech debt."
                li
                  strong /speckit.clarify:
                  |  "Do we need XML docs on internal classes? No, public APIs only."
                li
                  strong /speckit.plan:
                  |  ".NET 8.0/9.0 multi-targeting. Use ArgumentNullException.ThrowIfNull(). Configure .editorconfig."
                li
                  strong /speckit.tasks:
                  |  "1) Baseline audit, 2) XML docs, 3) Null guards, 4) Analyzer config, 5) Verify build."
                li
                  strong /speckit.analyze:
                  |  "Spot-check: Will this break any public APIs? No - only adding docs and guards."
                li
                  strong /speckit.implement:
                  |  "Execute tasks, run tests after each phase, commit incrementally."

            p.mb-0.
              The power of this flow is that each step constrains Copilot's focus. Instead of trying to solve everything at once, you guide it through a logical progression that produces reliable, well-documented results.

            h3.h5.mb-3.mt-4 The Post-Implementation Feedback Loop

            p.mb-3.
              Here's the reality: after <code>/speckit.implement</code> completes, you're not done yet. You'll likely do some "vibe coding" to get the feature just right—tweaking edge cases, adjusting UX, fixing bugs the AI missed. This is expected and normal.

            .alert.alert-warning.mb-4
              h6.alert-heading.mb-2
                i.bi.bi-exclamation-triangle-fill.me-2
                | Critical Step: Update Your Specs
              p.mb-2.
                When you finally get it right, <strong>tell the agent to update the spec, plan, and task documents</strong> based on your post-implementation fixes. This ensures that when you close the feature, the documentation accurately reflects what was actually shipped—not what was initially planned.
              p.mb-0.small.
                Example: "I fixed the GitHub Pages path resolution by implementing a custom relativePath filter. Please update SPEC.md and PLAN.md to reflect this solution instead of the original pathPrefix approach."

            .row.g-3.mb-4
              .col-lg-6
                .card.border-danger.h-100
                  .card-header.bg-light
                    h6.card-title.mb-0
                      i.bi.bi-x-circle.me-2.text-danger
                      | Without Feedback Loop
                  .card-body
                    ul.mb-0.small
                      li Specs describe what you planned, not what you built
                      li Future developers follow outdated documentation
                      li Institutional knowledge lives only in your head
                      li Next feature repeats the same mistakes

              .col-lg-6
                .card.border-success.h-100
                  .card-header.bg-light
                    h6.card-title.mb-0
                      i.bi.bi-check-circle.me-2.text-success
                      | With Feedback Loop
                  .card-body
                    ul.mb-0.small
                      li Specs evolve to match reality (living documentation)
                      li Future developers see what actually works
                      li Team learns from real-world implementation
                      li Each spec becomes more accurate over time

            p.mb-0.
              The feedback loop keeps specs synchronized with reality. Your specs document what you built and what you learned—useful for your future self and your team.

          section#conclusion.mb-5
            h2.h3.mb-4
              i.bi.bi-trophy.me-2
              | Conclusion: When to Use Spec Kit

            p.lead.mb-4.
              Spec Kit doesn't eliminate iteration—it ensures iteration improves documentation instead of destroying it. The ROI isn't speed—it's having specs that are still accurate a year later.

            h3.h5.mb-3 Use Spec Kit When:

            ul.mb-4
              li
                strong Institutional knowledge matters:
                |  Libraries, APIs, multi-year projects where team turnover is inevitable
              li
                strong Multiple developers:
                |  When onboarding cost of inaccurate documentation compounds over time
              li
                strong Long-term maintenance:
                |  Projects that will be maintained beyond the original author
              li
                strong Compliance requirements:
                |  When you need audit trails and documented decision rationale
              li
                strong Complex domain logic:
                |  When tribal knowledge creates single points of failure

            h3.h5.mb-3 Skip Spec Kit When:

            ul.mb-4
              li
                strong Throwaway prototypes:
                |  POCs you'll rewrite from scratch if successful
              li
                strong Solo projects with short lifespans:
                |  Personal tools you'll maintain alone for 6 months
              li
                strong Exploratory work:
                |  Research spikes where requirements are genuinely unknown
              li
                strong Trivial features:
                |  Single-file utilities that don't need coordination
              li
                strong Time-critical emergencies:
                |  Production fires where documentation can wait

            .alert.alert-success.mb-4
              h6.alert-heading.mb-2
                i.bi.bi-check-circle-fill.me-2
                | What This Project Delivered
              ul.mb-0
                li
                  strong 520/520 tests passing
                  |  across .NET 8.0 and 9.0 frameworks
                li
                  strong Zero compiler warnings
                  |  with TreatWarningsAsErrors enabled
                li
                  strong Two production releases
                  |  (v1.5.0 and v1.5.1) deployed to NuGet.org
                li
                  strong Specs that match reality
                  |  Documentation updated to reflect actual implementation
                li
                  strong 20 minutes of sync time
                  |  vs. never updating specs in traditional approach

            p.mb-0.
              Start small: one .specify/ directory, one SPEC.md with clear acceptance criteria. After implementation, spend 20 minutes having the agent update the spec to match what you actually built. Six months later, you'll thank yourself for the accurate documentation.
