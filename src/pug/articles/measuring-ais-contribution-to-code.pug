extends ../layouts/modern-layout

block layout-content
  br
  section.bg-gradient-primary.py-5
    .container
      .row.align-items-center
        .col-lg-10.mx-auto.text-center
          h1.display-4.fw-bold.mb-3
            i.bi.bi-lightbulb.me-3
            | Measuring AI's Contribution to Code
          h2.h3.mb-4 The Attribution Problem in AI-Assisted Development
          p.lead.mb-5.
            Artificial Intelligence is reshaping the software development landscape
            by enhancing productivity, improving code quality, and fostering innovation.
            This article delves into the metrics and tools used to measure AI's impact on coding.

          // Article Metadata
          .mb-4.d-flex.justify-content-center.align-items-center.gap-4
            span
              i.bi.bi-person.me-1
              | Mark Hazleton
            span
              i.bi.bi-calendar.me-1
              | September 2025
            span
              i.bi.bi-tag.me-1
              | AI, Development, Metrics

  // Main Article Content
  article#main-article
    .container
      .row
        .col-lg-9.mx-auto
          // Article Content Section
          section#article-content.mb-5

            // Opening Question
            h2.h3.mb-3.text-primary.d-flex.align-items-center
              i.bi.bi-lightning-charge.me-2
              | How a Simple Question Exposed a Complex Reality

            p.lead.mb-4.
              The question hit me during what should have been a routine executive demo.
              I'd just finished presenting a new application we'd built—nothing groundbreaking,
              but functional and delivered quickly. The demo had gone well and I was happy I made my time window provided.
              Then can a question from an executive, "How much of this code was written by AI?"
            p.
              I hesitated. It seemed like such a straightforward question,
              but as I opened my mouth to answer, I realized I genuinely didn't know.
              Was it the prompts I'd crafted in GitHub Copilot's agent mode?
              The code completions that appeared as I typed?
              The entire functions Claude generated from my specifications?
              The debugging cycles where I'd iterate with AI to fix issues?
              My hedged response—something about "significant AI assistance while maintaining human oversight"
              —earned me a pointed follow-up: "That sounds like you're avoiding the question."
            p.
              They were right. I was avoiding it, but not out of evasiveness.
              I was avoiding it because the question, despite appearing simple,
              had exposed a fundamental measurement problem that I'd never seriously considered.
            p.
              Later that evening, I attempted to quantify the AI contribution to that application.
              I estimated roughly 90% of the final code had originated from AI prompts and assistance
              rather than manual typing. But even as I calculated that percentage, I knew it was meaningless.
              What did "AI-generated" really mean when I had crafted every prompt,
              reviewed every output, debugged every integration,
              and taken responsibility for every architectural decision?
            p.
              This led me down a rabbit hole of measurement complexity that I suspect many developers
              are quietly grappling with. In our rush to adopt transformative AI tools,
              we've created a new category of work that defies traditional software metrics.
              Git commits look identical whether they originated from human keystrokes or AI generation.
              Lines of code tell us nothing about cognitive effort or creative contribution.
              Traditional productivity measures crumble when the fundamental nature of the work has changed.
            p.
              More troubling, I realized that the question itself carried an implicit judgment:
              if 90% was "AI-generated," was the developer 90% less valuable?
              Nothing could be further from the truth, yet the framing almost demanded that conclusion.
              This article is my attempt to work through the measurement challenge that executive's
              question exposed. It's about why traditional code metrics fail in an AI-augmented world,
              what new approaches might work better, and perhaps most importantly,
              how we can measure AI's impact without accidentally diminishing the substantial human expertise
              required to orchestrate these powerful tools effectively.
            p.
              The answer to "How much code was AI-generated?" turns out to be far more complex—and far more
              interesting—than any percentage could capture.


            // Table of Contents
            nav#table-of-contents.mb-5(aria-label='Table of Contents')
              .card.bg-light
                .card-header
                  h3.card-title.mb-0.fw-bold
                    i.bi.bi-list-ul.me-2
                    | Table of Contents
                .card-body
                  ul.list-group.list-group-flush
                    li.list-group-item: a.text-decoration-none(href='#vanishing-trail') The Vanishing Trail of AI Assistance
                    li.list-group-item: a.text-decoration-none(href='#git-commits') Why Git Commits Tell Us Nothing
                    li.list-group-item: a.text-decoration-none(href='#spectrum-assistance') The Spectrum of AI Assistance
                    li.list-group-item: a.text-decoration-none(href='#metrics-framework') Proposed Metrics Framework
                    li.list-group-item: a.text-decoration-none(href='#enterprise-challenge') The Enterprise Measurement Challenge
                    li.list-group-item: a.text-decoration-none(href='#skill-statistics') The Skill Behind the Statistics
                    li.list-group-item: a.text-decoration-none(href='#developer-paradox') The Senior Developer Paradox
                    li.list-group-item: a.text-decoration-none(href='#enterprise-response') The Enterprise Response
                    li.list-group-item: a.text-decoration-none(href='#skills-matter') The Skills That Still Matter
                    li.list-group-item: a.text-decoration-none(href='#recommendations') Recommendations for Technical Leaders

            section#vanishing-trail.mb-5
              h2.h3.mb-4.text-primary.d-flex.align-items-center
                i.bi.bi-eye-slash.me-2
                | The Vanishing Trail of AI Assistance
              p.mb-4.
                When a software executive asks "How much of this code was written by AI?", they're asking what seems
                like a straightforward question. But in practice, it's become one of the most difficult metrics
                to accurately measure in modern software development.
                Today's development environment creates a perfect storm of attribution complexity that makes traditional
                code metrics nearly useless for understanding AI contribution.

            section#git-commits.mb-5
              .card.mb-4
                .card-header.bg-warning.text-dark
                  h3.card-title.mb-0.fw-bold
                    i.bi.bi-git.me-2
                    | Why Git Commits Tell Us Nothing
                .card-body
                  .row
                    .col-lg-8
                      ol.mb-0
                        li GitHub Copilot suggests code completions as you type
                        li IntelliSense auto-completes method signatures and imports
                        li AI agents generate entire functions from prompts in 'Agent Mode'
                        li AI agents let you discuss approaches and design decisions in 'Ask Mode'
                        li Code formatters restructure the output
                        li Linters suggest improvements
                        li The developer reviews and refines everything
                        li Git commit records the final result
                    .col-lg-4
                      .alert.alert-warning.mb-0
                        p.mb-0
                          i.bi.bi-exclamation-triangle.me-2
                          strong Key Insight:
                          | At commit time, all code appears identical regardless of origin. A function generated entirely by AI looks exactly the same as one painstakingly typed by hand.

            section#spectrum-assistance.mb-5
              h2.h3.mb-4.text-primary.d-flex.align-items-center
                i.bi.bi-bar-chart.me-2
                | The Spectrum of AI Assistance
              p.mb-4.
                Part of what makes the "How much was AI-generated?" question so difficult to answer is that it assumes
                a binary world that doesn't exist. In reality, AI assistance exists on a spectrum,
                and most modern development involves multiple levels simultaneously within the same project, file,
                or even function.
              p.mb-4.
                When we try to assign a single percentage to AI contribution,
                we're collapsing this spectrum into an oversimplified metric.
                A more honest assessment requires understanding the different types of AI assistance
                and recognizing that they often layer on top of each other in ways that make clean attribution
                nearly impossible.
              p.lead.mb-4.
                The binary question "AI-generated or not?" fails to capture the nuanced reality of modern development:

              .row.g-4.mb-4
                .col-lg-9
                  h4.mb-3
                    i.bi.bi-1-circle.me-2
                    | Level 1: Autocomplete Enhancement
                  ul.mb-0
                    li Traditional IntelliSense completing `console.log()`
                    li Copilot suggesting variable names
                .col-lg-3
                  .card.bg-light.mb-3
                    .card-body.text-center
                      .fw-bold.text-muted AI Contribution
                      .display-6.text-primary 10–20%

              .row.g-4.mb-4
                .col-lg-9
                  h4.mb-3
                    i.bi.bi-2-circle.me-2
                    | Level 2: Code Completion
                  ul.mb-0
                    li Copilot generating entire method bodies
                    li Repetitive patterns like error handling
                .col-lg-3
                  .card.bg-light.mb-3
                    .card-body.text-center
                      .fw-bold.text-muted AI Contribution
                      .display-6.text-primary 40–60%

              .row.g-4.mb-4
                .col-lg-9
                  h4.mb-3
                    i.bi.bi-3-circle.me-2
                    | Level 3: Conversational Generation
                  ul.mb-0
                    li Prompting Claude or GPT to write specific functions
                    li AI agents creating entire components from requirements
                .col-lg-3
                  .card.bg-light.mb-3
                    .card-body.text-center
                      .fw-bold.text-muted AI Contribution
                      .display-6.text-warning 70–90%

              .row.g-4.mb-4
                .col-lg-9
                  h4.mb-3
                    i.bi.bi-4-circle.me-2
                    | Level 4: Architecture Generation
                  ul.mb-0
                    li AI designing entire application structures
                    li Generating multiple interconnected files
                .col-lg-3
                  .card.bg-light.mb-3
                    .card-body.text-center
                      .fw-bold.text-muted AI Contribution
                      .display-6.text-warning 70–90%

              p.text-muted.mb-4.
                The same 100-line file might contain elements from all four levels, making percentage calculations meaningless.

            section#metrics-framework.mb-5
              h2.h3.mb-4.text-primary.d-flex.align-items-center
                i.bi.bi-graph-up.me-2
                | Proposed Metrics Framework
              p.lead.mb-4.
                Given the attribution complexity, we can shift our focus from trying to measure
                "How much code was AI-generated?"
                to "How much did AI accelerate or improve our development process?"
                Here are some potential metrics that could provide
                meaningful insights into AI's impact without getting bogged down in impossible attribution.
              .row.g-4.mb-4
                .col-lg-6
                  .card.h-100
                    .card-header.bg-success.text-white
                      h4.card-title.mb-0
                        i.bi.bi-speedometer2.me-2
                        | 1. Development Velocity Metrics
                    .card-body
                      p
                        strong Measure:
                        | Story points delivered per sprint, features shipped per quarter
                      p
                        strong Rationale:
                        | If AI is truly accelerating development, velocity should increase
                      p
                        strong Limitation:
                        | Doesn't isolate AI impact from other productivity factors

                .col-lg-6
                  .card.h-100
                    .card-header.bg-info.text-white
                      h4.card-title.mb-0
                        i.bi.bi-clock.me-2
                        | 2. Time-to-First-Working-Prototype
                    .card-body
                      p
                        strong Measure:
                        | Hours from requirements to functioning demo
                      p
                        strong Rationale:
                        | AI excels at rapid prototyping and proof-of-concept development
                      p
                        strong Limitation:
                        | May not reflect production-ready code quality

              .row.g-4.mb-4
                .col-lg-6
                  .card.h-100
                    .card-header.bg-primary.text-white
                      h4.card-title.mb-0
                        i.bi.bi-chat-dots.me-2
                        | 3. Prompt-to-Code Ratio
                    .card-body
                      p
                        strong Measure:
                        | Lines of natural language prompts vs. lines of generated code
                      p
                        strong Rationale:
                        | Higher ratios indicate more efficient AI utilization
                      p
                        strong Limitation:
                        | Requires tracking prompts across multiple tools

                .col-lg-6
                  .card.h-100
                    .card-header.bg-secondary.text-white
                      h4.card-title.mb-0
                        i.bi.bi-eye.me-2
                        | 4. Code Review Patterns
                    .card-body
                      p
                        strong Measure:
                        | Types and frequency of changes during human review
                      p
                        strong Rationale:
                        | Pure AI code requires different review patterns than human-written code
                      p
                        strong Limitation:
                        | Requires structured review tagging

              .card.mb-4
                .card-header.bg-warning.text-dark
                  h4.card-title.mb-0
                    i.bi.bi-bug.me-2
                    | 5. Debugging Session Analysis
                .card-body
                  p
                    strong Measure:
                    | Time spent debugging AI-generated vs. human-written code sections
                  p
                    strong Rationale:
                    | Different code origins may have different defect patterns
                  p
                    strong Limitation:
                    | Requires sophisticated tooling to track code origins

            section#enterprise-challenge.mb-5
              h2.h3.mb-4.text-primary.d-flex.align-items-center
                i.bi.bi-building.me-2
                | The Enterprise Measurement Challenge

              p.mb-4.
                Organizations need practical approaches to measure AI's impact on development processes.
                Here are four key measurement strategies that enterprises can implement:

              dl.mb-4
                dt.fw-bold.mb-2 Developer Self-Reporting
                dd.mb-3.
                  Ask developers to estimate AI contribution for each feature or sprint. While subjective, it provides directional insight.

                dt.fw-bold.mb-2 Tool Integration Metrics
                dd.mb-3.
                  Measure acceptance rates of AI suggestions, prompt frequency, and time spent in AI-assisted vs. manual coding modes.

                dt.fw-bold.mb-2 Comparative Development Studies
                dd.mb-3.
                  Run parallel development efforts with and without AI tools on similar features, measuring delivery time and quality.

                dt.fw-bold.mb-2 Code Complexity Analysis
                dd.mb-3.
                  AI-generated code often has different complexity patterns than human code. Static analysis might reveal these signatures.

            section#skill-statistics.mb-5
              h2.h3.mb-4.text-primary.d-flex.align-items-center
                i.bi.bi-mortarboard.me-2
                | The Skill Behind the Statistics

              h3.h4.mb-3 The Hidden Complexity of AI-Assisted Development

              p.mb-4.
                Developers using AI aren't being replaced—they're being amplified. The skills required for effective AI-assisted development are sophisticated and often invisible in traditional metrics:

              .row.mb-4
                .col-md-6
                  ul.mb-3
                    li Prompt Engineering Mastery
                    li Context Window Management
                    li Code Quality Assessment
                .col-md-6
                  ul.mb-3
                    li Architecture and Integration
                    li Debugging AI Patterns

            section#developer-paradox.mb-5
              h2.h3.mb-4.text-primary.d-flex.align-items-center
                i.bi.bi-person-workspace.me-2
                | The Senior Developer Paradox

              p.lead.mb-4.
                Senior developers excel with AI tools not despite their experience, but because of it.
                The messy, frustrating journey of learning to code—debugging obscure errors,
                wrestling with poorly documented APIs, and owning catastrophic mistakes—builds
                the intuition that makes AI assistance truly powerful.

              h3.h4.mb-3 The Hard-Won Experience Advantage

              p.mb-3.
                Experienced developers can leverage AI effectively because they've internalized what quality code looks like:

              ul.mb-4
                li Recognize when AI-generated solutions will cause maintainability problems
                li Spot architectural flaws before they compound into technical debt
                li Understand the performance and security implications of suggested patterns
                li Know when to reject AI suggestions that seem clever but are fundamentally wrong

              h3.h4.mb-3 The Friction Paradox

              p.mb-4.
                By removing friction from the development process, we risk losing the very struggles
                that teach developers humility, ownership, and deep system understanding.
                When AI generates code that "just works," junior developers may miss the hard lessons
                that come from making mistakes and having to fix them personally.

              h3.h4.mb-3 Resisting the "Blame the AI" Game

              p.mb-4.
                The most dangerous trend is developers who distance themselves from AI-assisted output.
                Whether code comes from human keystrokes or AI generation, the developer who commits it
                owns it completely—warts and all. The moment we start saying "the AI wrote that buggy function,"
                we've lost the fundamental accountability that separates professional developers from code generators.

            section#enterprise-response.mb-5
              h2.h3.mb-4.text-primary.d-flex.align-items-center
                i.bi.bi-building-gear.me-2
                | The Enterprise Response: Built-in Expertise

              p.lead.mb-4.
                Smart enterprises aren't asking "How do we measure AI contribution?" but rather
                "How do we build systems that amplify human expertise while maintaining accountability?"
                The solution lies in embedding expert knowledge into development processes rather than
                turning everything over to AI.

              h3.h4.mb-3 Systemic Expertise Integration

              p.mb-4.
                Forward-thinking organizations are building expertise directly into their development workflow
                through intelligent systems that guide rather than replace human decision-making:

              dl.mb-4
                dt.fw-bold.mb-2 AI-Powered Quality Gates
                dd.mb-3.
                  Automated systems that apply enterprise coding standards and architectural patterns,
                  catching issues before they reach production while teaching developers best practices.

                dt.fw-bold.mb-2 Contextual Code Review
                dd.mb-3.
                  AI-assisted review processes that highlight potential issues while requiring human
                  judgment for approval, maintaining the critical human oversight in the development process.

                dt.fw-bold.mb-2 Embedded Architecture Guidance
                dd.mb-3.
                  Systems that suggest architectural patterns and warn about anti-patterns in real-time,
                  helping developers make better decisions without removing their agency.

                dt.fw-bold.mb-2 Continuous Learning Integration
                dd.mb-3.
                  Platforms that capture institutional knowledge and make it accessible during development,
                  ensuring that hard-won organizational expertise isn't lost to AI automation.

              p.text-muted.mb-4.
                These approaches maintain the essential human element while scaling expertise across the organization,
                avoiding the trap of complete AI dependency.

            section#skills-matter.mb-5
              h2.h3.mb-4.text-primary.d-flex.align-items-center
                i.bi.bi-star.me-2
                | The Skills That Still Matter

              p.lead.mb-4.
                In an AI-augmented development world, success isn't about writing more code—it's about
                becoming more discerning. The developers who thrive are those who can distinguish
                quality AI assistance from AI slop and orchestrate these powerful tools effectively.

              h3.h4.mb-3 Essential Skills for AI-Augmented Development

              dl.mb-4
                dt.fw-bold.mb-2 System Architecture Thinking
                dd.mb-3.
                  The ability to see how components fit together and identify when AI suggestions
                  will create architectural debt or violate system boundaries.

                dt.fw-bold.mb-2 Requirements Translation
                dd.mb-3.
                  Converting business needs into precise technical specifications that AI can work with,
                  while understanding what gets lost in translation.

                dt.fw-bold.mb-2 Quality Pattern Recognition
                dd.mb-3.
                  Instantly recognizing good code patterns versus AI-generated code that looks correct
                  but introduces subtle bugs or maintainability issues.

                dt.fw-bold.mb-2 Integration Intuition
                dd.mb-3.
                  Understanding how AI-generated components will behave in real systems under load,
                  with real data, and real user behavior.

                dt.fw-bold.mb-2 Performance and Security Instincts
                dd.mb-3.
                  The hard-earned ability to spot performance bottlenecks and security vulnerabilities
                  that AI tools might miss or inadvertently introduce.

              p.mb-4.
                These skills aren't replaceable by AI because they represent the accumulated wisdom
                of dealing with the messy reality of software systems. They're what separate developers
                who use AI effectively from those who become dependent on it.

            section#recommendations.mb-5
              h2.h3.mb-4.text-primary.d-flex.align-items-center
                i.bi.bi-clipboard-check.me-2
                | Recommendations for Technical Leaders

              p.lead.mb-4.
                Rather than trying to precisely measure AI's contribution to code, focus on building
                systems and practices that amplify your team's effectiveness while maintaining the
                essential human elements that ensure quality and accountability.

              h3.h4.mb-3 Start Here: Immediate Actions

              ol.mb-4
                li.mb-3
                  strong Focus on velocity, not attribution.
                  | Track how quickly your team delivers features and solves problems, not which tool generated what code.

                li.mb-3
                  strong Build quality gates, not barriers.
                  | Implement AI-assisted code review and automated testing that catches issues while teaching best practices.

                li.mb-3
                  strong Survey your developers regularly.
                  | Ask about their AI tool usage, what's working, what isn't, and where they need support.

                li.mb-3
                  strong Measure business outcomes.
                  | Are you shipping faster? Are customers happier? Are bugs decreasing? These matter more than code origin.

              h3.h4.mb-3 Invest for the Future

              p.mb-4.
                As AI tools mature, position your organization to leverage them effectively while preserving
                the developer expertise that makes the difference between good AI assistance and AI dependency.
              p.mb-2.
                The future of software development metrics isn't about attribution—it's about acceleration.
                Rather than trying to precisely measure how much code AI "wrote," we should focus on measuring
                how much faster, better, and more innovative our development processes have become.

            // Conclusion Section
            .alert.alert-primary.mb-5
              p.mb-0
                small.fst-italic.
                  This piece began as a back-and-forth with Claude—just me trying to untangle my thoughts
                  and draft a follow-up email to the exec who sparked the original question.
                  From there, I layered in more prompts, rewrites, and a few rabbit holes of exploration.
                  Eventually, I handed it off to a custom GPT I built, which shaped it into something blog-worthy.
                  Then GitHub Copilot helped me mop up the AI slop: bloated formatting, awkward phrasing,
                  and the occasional hallucinated flourish.
                  So while I can't tell you exactly how much of this article was written by AI, I can say with certainty—it wouldn't exist without it.

            // Back to Top Link
            .text-center.mb-4
              a(href='#table-of-contents' class='btn btn-outline-primary')
                i.bi.bi-arrow-up-circle.me-2
                | Back to Table of Contents
