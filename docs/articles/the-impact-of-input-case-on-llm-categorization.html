<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="author" content="Mark Hazleton" />
        <meta name="robots" content="index, follow" />
        <title>The Impact of Input Case on LLM Categorization</title>
        <meta name="description" content="Discover how input case affects tokenization and categorization in Large Language Models (LLMs). Understand case sensitivity in NLP tasks and model robustness." />
        <meta name="keywords" content="LLM, input case, tokenization, NLP, categorization agents, BERT cased uncased, GPT case sensitivity, sentiment analysis, topic classification" />
        <meta name="author" content="Mark Hazleton" />
        <link rel="canonical" href="https://markhazleton.com/articles/the-impact-of-input-case-on-llm-categorization.html" />
        <meta property="og:title" content="The Impact of Input Case on LLM Categorization" />
        <meta property="og:description" content="Discover how input case affects tokenization and categorization in Large Language Models (LLMs). Understand case sensitivity in NLP tasks and model robustness.." />
        <meta property="og:type" content="article" />
        <meta property="og:url" content="https://markhazleton.com/articles/the-impact-of-input-case-on-llm-categorization.html" />
        <meta property="og:image" content="https://img.youtube.com/vi/2hI79aKyaK0/maxresdefault.jpg" />
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:title" content="The Impact of Input Case on LLM Categorization" />
        <meta name="twitter:description" content="Discover how input case affects tokenization and categorization in Large Language Models (LLMs). Understand case sensitivity in NLP tasks and model robustness." />
        <meta name="twitter:image" content="https://img.youtube.com/vi/2hI79aKyaK0/maxresdefault.jpg" />
        <link rel="shortcut icon" href="/assets/img/favicon.ico" />
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <script type="application/ld+json">
            {
                "@context": "http://schema.org",
                "@type": "Person",
                "name": "Mark Hazleton",
                "familyName": "Hazleton",
                "givenName": "Mark",
                "jobTitle": "Solutions Architect",
                "alumniOf": "University of North Texas",
                "affiliation": {
                    "@type": "Organization",
                    "name": "Control Origins"
                },
                "sameAs": ["https://www.linkedin.com/in/markhazleton/", "https://github.com/markhazleton/", "https://twitter.com/markhazleton/", "https://www.youtube.com/c/MarkHazleton/", "https://markhazleton.brandyourself.com/", "https://www.postman.com/markhazleton/", "https://stackoverflow.com/users/479571/markhazleton/", "https://www.slideshare.net/markhazleton/", "https://hub.docker.com/u/markhazleton/", "https://www.polywork.com/markhazleton/", "https://www.codeproject.com/Members/MarkHazleton/", "https://markhazleton.wordpress.com/", "https://learn.microsoft.com/en-us/users/mark-hazleton/", "https://app.pluralsight.com/profile/markhazletonCEC/", "https://app.pluralsight.com/profile/markhazleton/", "https://www.instagram.com/markhazleton/", "https://storybird.ai/u/markhazleton/", "https://www.pinterest.com/markhazleton/"],
                "url": "https://markhazleton.com"
            }
        </script>
        <script type="application/ld+json">
            {
                "@context": "https://schema.org",
                "@type": "Organization",
                "url": "https://markhazleton.com",
                "logo": "https://markhazleton.com/assets/img/MarkHazleton.jpg"
            }
        </script>
        <meta name="seobility" content="f80235aca1a812e0afde44f0142c825b" />
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link rel="stylesheet" href="/css/styles.css?version=20250413" />
        <!-- Global site tag (gtag.js) - Google Analytics-->
        <script src="https://www.googletagmanager.com/gtag/js?id=G-L8GVZNDH0B" async></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag() {
                dataLayer.push(arguments);
            }
            gtag('js', new Date());
            gtag('config', 'G-L8GVZNDH0B');
        </script>
        <script>
            (function (c, l, a, r, i, t, y) {
                c[a] =
                    c[a] ||
                    function () {
                        (c[a].q = c[a].q || []).push(arguments);
                    };
                t = l.createElement(r);
                t.async = 1;
                t.src = 'https://www.clarity.ms/tag/' + i + '?ref=bwt';
                y = l.getElementsByTagName(r)[0];
                y.parentNode.insertBefore(t, y);
            })(window, document, 'clarity', 'script', 'd628hovv63');
        </script>
    </head>
    <body class="sidetracked-body" id="page-top">
        <nav class="navbar navbar-dark bg-primary p-1">
            <div class="container-fluid justify-content-end">
                <div class="social-icons d-flex">
                    <a class="social-icon" href="https://www.linkedin.com/in/markhazleton" target="_blank" rel="noopener noreferrer" title="LinkedIn Profile"><i class="fab fa-linkedin-in text-light me-2" style="font-size: 2rem"></i></a>
                    <a class="social-icon" href="https://github.com/markhazleton" target="_blank" rel="noopener noreferrer" title="GitHub Profile"><i class="fab fa-github text-light me-2" style="font-size: 2rem"></i></a>
                    <a class="social-icon" href="https://www.youtube.com/@MarkHazleton" target="_blank" rel="noopener noreferrer" title="YouTube Channel"><i class="fab fa-youtube text-light me-2" style="font-size: 2rem"></i></a>
                    <a class="social-icon" href="https://www.instagram.com/markhazleton/" target="_blank" rel="noopener noreferrer" title="Instagram Profile"><i class="fab fa-instagram text-light" style="font-size: 2rem"></i></a>
                </div>
            </div>
        </nav>
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="/#page-top" title="Mark Hazleton">
                <span class="d-block d-lg-none">Mark Hazleton</span>
                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="/assets/img/MarkHazleton.jpg" alt="..." /></span>
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav">
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="/#about">About</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="/#articles">Articles</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="/#projects">Projects</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="/#experience">Experience</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="https://webspark.markhazleton.com">WebSpark</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="/projectmechanics/">Project Mechanics</a></li>
                </ul>
            </div>
        </nav>
        <!-- Page Content-->
        <div class="container-fluid p-0 painteddesert-background">
            <article class="painteddesert-section painteddesert-section-background" id="post">
                <div class="painteddesert-section-content">
                    <h1>The Impact of Input Case on LLM Categorization Agents</h1>
                    <h2 class="subheading mb-3">How Case Sensitivity Affects Tokenization and Categorization in NLP</h2>
                    <p>Large Language Models (LLMs) have demonstrated remarkable capabilities in various Natural Language Processing (NLP) tasks, including text categorization. However, the seemingly minor detail of input case can significantly influence how these models process information, leading to variations in tokenization and, consequently, the categories they return. Understanding this sensitivity is crucial for developing robust and reliable categorization agents.</p>
                    <div class="card mb-4">
                        <div class="card-title bg-primary text-white p-2"><p class="lead">Deep Dive: LLM Prompt Case Sensitivity</p></div>
                        <div class="card-body">
                            <p class="card-text">Unlock the secrets of LLM prompt case sensitivity! This deep dive explores how the seemingly simple act of capitalization in your AI prompts can drastically impact the responses you receive from Large Language Models (LLMs). Discover why case sensitivity matters in prompt engineering, affecting tasks like sentiment analysis where emphasis can be key, and topic classification where proper nouns and acronyms are crucial. Learn best practices for case-specific prompt engineering to enhance the accuracy and clarity of your AI interactions and avoid unexpected results. Whether you're a beginner or an experienced prompt engineer, understanding LLM response variations based on prompt case is essential for maximizing AI utility.</p>
                            <div class="ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/2hI79aKyaK0?si=-ip3OjC9MjTSsDYA" title="Deep Dive: LLM Prompt Case Sensitivity" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></div>
                        </div>
                        <div class="card-footer"><p></p></div>
                    </div>
                    <h2 class="mb-3">Tokenization: The First Hurdle Influenced by Case</h2>
                    <p>At the core of how LLMs understand text lies tokenization, the process of breaking down raw text into smaller units called tokens. Tokenization, the process of breaking text into tokens, is profoundly impacted by input case. Case-sensitive tokenizers treat "Apple" and "apple" as distinct, while case-insensitive tokenizers merge them, affecting downstream tasks. The way an LLM's tokenizer handles case sensitivity plays a pivotal role in shaping the input it ultimately processes.</p>
                    <div class="row mb-4">
                        <div class="col-md-6">
                            <div class="card">
                                <div class="card-header"><h3 class="card-title">Case-Sensitive Tokenization</h3></div>
                                <div class="card-body"><p class="card-text">Many LLMs utilize case-sensitive tokenizers, where uppercase and lowercase forms of a word are treated as distinct tokens. For instance, in a case-sensitive model, “Apple” and “apple” would be assigned different token IDs and embedding vectors. This distinction allows the model to potentially capture nuances associated with capitalization, such as proper nouns or emphasis</p></div>
                            </div>
                        </div>
                        <div class="col-md-6">
                            <div class="card">
                                <div class="card-header"><h3 class="card-title">Case-Insensitive Tokenization</h3></div>
                                <div class="card-body"><p class="card-text">Some approaches employ case-insensitive tokenization, typically by converting all input text to lowercase before tokenization. In such systems, "Apple" and "apple" would be treated as identical tokens. While this simplifies the vocabulary and can aid generalization in some tasks, it also means the loss of any information conveyed through capitalization</p></div>
                            </div>
                        </div>
                    </div>
                    <p>Subword tokenizers like BERT's WordPiece can split "AIRPORT" into fragments (e.g., "AI", "##R"), complicating categorization compared to "airport" as a single token. The choice of tokenization directly dictates the sequence of tokens the LLM receives as input. Therefore, altering the case in the input can fundamentally change the representation the model uses for categorization. Furthermore, case changes can affect how subword tokenizers split words. For example, a word like “airport” might be a single token for BERT’s WordPiece tokenizer. However, if the input is “AIRPORT” (all caps), it could be broken down into multiple subword fragments like "AI", "##R", "##PO", "##RT". This fragmentation of all-caps words into "nonsense" subword pieces can complicate the model's understanding and potentially lead to different categorization outcomes</p>
                    <h2 class="mb-3">How Tokenization Changes Impact Categorization</h2>
                    <p>Tokenization changes ripple through LLM layers, altering internal representations and leading to different classification outputs. Models may activate different learned patterns based on input case. Since tokenization is the initial step in processing text for an LLM, any changes at this stage propagate through the model's layers, ultimately influencing the categorization output. If the input case alters the tokens, the model's internal representation of the text will be different. This means the model might activate different patterns and associations learned during its training, potentially leading to different classification decisions</p>
                    <h2 class="mb-3">Case Sensitivity Across Different LLM Architectures</h2>
                    <p>Case sensitivity varies across models like BERT, GPT, and others:</p>
                    <div class="accordion" id="caseSensitivityAccordion">
                        <div class="accordion-item">
                            <span class="accordion-header"><button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseOne">BERT Cased vs. Uncased</button></span>
                            <div class="accordion-collapse collapse" id="collapseOne">
                                <div class="accordion-body">
                                    <p>
                                        <strong>BERT-base-cased</strong>
                                        preserves capitalization, aiding tasks like NER, while
                                        <strong>BERT-base-uncased</strong>
                                        generalizes better by ignoring case. The uncased version was trained on lowercased text, making it inherently case-insensitive. In contrast, the cased version preserves the original casing and can leverage capitalization cues. This design choice leads to performance differences on various tasks. For instance,
                                        <strong>BERT-base-cased</strong>
                                        might perform better on Named Entity Recognition where capitalization is crucial, while
                                        <strong>BERT-base-uncased</strong>
                                        might generalize better for topic classification where the underlying meaning is less dependent on case.
                                    </p>
                                </div>
                            </div>
                        </div>
                        <div class="accordion-item">
                            <span class="accordion-header"><button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseTwo">GPT and Decoder Models</button></span>
                            <div class="accordion-collapse collapse" id="collapseTwo">
                                <div class="accordion-body"><p>GPT models (e.g., GPT-2, GPT-3) use case-sensitive BPE tokenization, yet show robustness to noisy input due to their training data scale. Models in the GPT family utilize byte-pair encoding (BPE) tokenization without lowercasing, making them inherently case-sensitive. They assign different token IDs even for the same word with different casing (e.g., "Hello" vs. "hello"). Despite this sensitivity, research suggests that decoder-only LLMs like GPT-2 can be relatively robust to noisy case changes, potentially due to their extensive training data and byte-level BPE handling variations smoothly</p></div>
                            </div>
                        </div>
                        <div class="accordion-item">
                            <span class="accordion-header"><button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseThree">Other Transformers</button></span>
                            <div class="accordion-collapse collapse" id="collapseThree">
                                <div class="accordion-body"><p>Many modern Transformers (RoBERTa, XLNet, T5) retain case information by default, impacting their categorization behaviors depending on the task. They typically retain case by default through subword tokenization or byte encodings. However, older models or those explicitly trained with case normalization (like some LSTM-based classifiers) will be case-insensitive.</p></div>
                            </div>
                        </div>
                    </div>
                    <h2 class="mb-3">Impact on Specific Categorization Tasks</h2>
                    <div class="row">
                        <div class="col-md-6">
                            <div class="card">
                                <div class="card-body">
                                    <h3 class="card-title">Sentiment Analysis</h3>
                                    <p class="card-text">Casing enhances sentiment detection by preserving cues like all-caps emphasis (e.g., "AMAZING!"). While the core sentiment often resides in the words themselves, casing can carry subtle sentiment cues. For example, all-caps words like "AMAZING!" can indicate intensified emotion. Lowercasing everything would lose this emphasis. Some sentiment analysis systems even explicitly boost sentiment intensity for all-caps words. Therefore, preserving case might be beneficial for sentiment classifiers to capture these nuances. However, one must also be aware that uncommon all-caps words might be split into subwords, which the model then needs to interpret.</p>
                                </div>
                            </div>
                        </div>
                        <div class="col-md-6">
                            <div class="card">
                                <div class="card-body">
                                    <h3 class="card-title">Topic Classification</h3>
                                    <p class="card-text">Case sensitivity helps identify acronyms and proper nouns ("WHO" vs. "who"), critical in topic categorization. Historically, making models case-insensitive was often considered acceptable, or even helpful, for topic classification, as the difference between "inflation" and "Inflation" is usually insignificant for determining the topic. Lowercasing can reduce data sparsity (treating "NASA" and "nasa" as the same). However, case can be crucial for identifying domain-specific terms, acronyms, and proper nouns that are strong indicators of a topic (e.g., "COVID-19", "UNICEF", "Python" vs. "python"). A case-sensitive model can differentiate between "who" (a pronoun) and "WHO" (World Health Organization). Modern transformer models used for topic classification generally retain case and perform well</p>
                                </div>
                            </div>
                        </div>
                    </div>
                    <h2 class="mb-3">Ensuring Robustness to Varying Input Case</h2>
                    <p>For robust agents, employ data augmentation with mixed-case samples. Experiment with both cased and uncased preprocessing depending on task needs. Ideally, a categorization agent should be robust to variations in user input case, including titles in all caps or random capitalization. If a model was primarily trained on well-cased text, it might misinterpret oddly cased input. Strategies to address this include data augmentation with mixed-case examples during training. For critical applications, it might be necessary to experiment with both case-sensitive and case-insensitive preprocessing or even use additional features to explicitly capture information lost through normalization.</p>
                    <h2 class="mb-3">Conclusion</h2>
                    <p>Input case significantly impacts LLM categorization agents by influencing the tokenization process. Case-sensitive tokenization preserves potential nuances but can lead to different tokens for the same word with different casing, potentially altering the model's internal representation and categorization output. Different LLM architectures exhibit varying degrees of case sensitivity based on their tokenizers and training. The importance of preserving case depends on the specific categorization task; it can be beneficial for capturing sentiment intensity and distinguishing topic-defining acronyms and proper nouns, but might be less critical for general topic identification.</p>
                    <p>Ultimately, understanding your model's tokenizer, the role of case in your specific categorization task, and experimenting with different preprocessing approaches are key to building reliable and robust LLM-based categorization agents. Evaluating your model's performance with both original-cased and lowercased input can reveal its sensitivity to case changes and guide you in making informed decisions about text preprocessing</p>
                </div>
            </article>
        </div>
        <footer class="navbar-dark bg-primary">
            <div class="row">
                <div class="col-1"><br /></div>
                <div class="col-10 text-left">
                    <br />
                    <!-- Ensure current article exists-->
                    <!-- Extract and clean currentKeywords-->
                    <!-- Only check for related articles if there are valid keywords-->
                    <!-- Shuffle related articles and limit to 3-->
                    <!-- Render current article's keywords as badges-->
                    <div class="keywords mt-3">
                        <p class="text-white">Hashtags:</p>
                        <div class="d-flex flex-wrap gap-2">
                            <span class="badge bg-primary text-uppercase">llm</span>
                            <span class="badge bg-primary text-uppercase">categorization</span>
                            <span class="badge bg-primary text-uppercase">case sensitivity</span>
                            <span class="badge bg-primary text-uppercase">tokenization</span>
                            <span class="badge bg-primary text-uppercase">nlp</span>
                            <span class="badge bg-primary text-uppercase">input case</span>
                            <span class="badge bg-primary text-uppercase">case-sensitive</span>
                            <span class="badge bg-primary text-uppercase">case-insensitive</span>
                            <span class="badge bg-primary text-uppercase">prompt engineering</span>
                            <span class="badge bg-primary text-uppercase">sentiment analysis</span>
                            <span class="badge bg-primary text-uppercase">topic classification</span>
                            <span class="badge bg-primary text-uppercase">bert</span>
                            <span class="badge bg-primary text-uppercase">gpt</span>
                            <span class="badge bg-primary text-uppercase">transformers</span>
                            <span class="badge bg-primary text-uppercase">subword tokenizers</span>
                            <span class="badge bg-primary text-uppercase">token ids</span>
                            <span class="badge bg-primary text-uppercase">embedding vectors</span>
                            <span class="badge bg-primary text-uppercase">bpe tokenization</span>
                            <span class="badge bg-primary text-uppercase">byte-pair encoding</span>
                            <span class="badge bg-primary text-uppercase">data augmentation</span>
                            <span class="badge bg-primary text-uppercase">mixed-case samples</span>
                            <span class="badge bg-primary text-uppercase">preprocessing</span>
                            <span class="badge bg-primary text-uppercase">robust agents</span>
                            <span class="badge bg-primary text-uppercase">sentiment detection</span>
                            <span class="badge bg-primary text-uppercase">acronyms</span>
                            <span class="badge bg-primary text-uppercase">proper nouns</span>
                            <span class="badge bg-primary text-uppercase">named entity recognition</span>
                            <span class="badge bg-primary text-uppercase">case normalization</span>
                            <span class="badge bg-primary text-uppercase">text preprocessing</span>
                            <span class="badge bg-primary text-uppercase">ai utility</span>
                            <span class="badge bg-primary text-uppercase">model performance</span>
                            <span class="badge bg-primary text-uppercase">classification decisions</span>
                            <span class="badge bg-primary text-uppercase">internal representations</span>
                            <span class="badge bg-primary text-uppercase">case changes</span>
                            <span class="badge bg-primary text-uppercase">subword fragments</span>
                            <span class="badge bg-primary text-uppercase">categorization tasks.</span>
                        </div>
                    </div>
                    <!-- Render related articles in a card layout-->
                    <div class="related-articles mt-4">
                        <h3 class="text-white">Related Articles</h3>
                        <div class="row row-cols-1 row-cols-md-2 row-cols-lg-3 g-4">
                            <div class="col">
                                <div class="card h-100 bg-dark text-white">
                                    <div class="card-body">
                                        <h5 class="card-title">
                                            <i class="bi bi-arrow-right-circle-fill me-2"></i>
                                            <a class="text-white" href="/articles/exploratory-data-analysis-eda-using-python.html" title="An in-depth guide to Data Sanity checks and Exploratory Data Analysis (EDA) using Python.">Exploratory Data Analysis (EDA) Using Python</a>
                                        </h5>
                                        <p class="card-text text-white">An in-depth guide to Data Sanity checks and Exploratory Data Analysis (EDA) using Python.</p>
                                    </div>
                                    <div class="card-footer"></div>
                                </div>
                            </div>
                            <div class="col">
                                <div class="card h-100 bg-dark text-white">
                                    <div class="card-body">
                                        <h5 class="card-title">
                                            <i class="bi bi-arrow-right-circle-fill me-2"></i>
                                            <a class="text-white" href="/articles/the-brain-behind-the-jshow-trivia-demo-on-webspark-j-show-builder-gpt.html" title="Discover the development journey of J-Show Builder GPT, an AI-powered tool that generates fun trivia games in a grid format for WebSpark’s JShow Trivia Demo.">The Brain Behind the JShow Trivia Demo on WebSpark J-Show Builder GPT</a>
                                        </h5>
                                        <p class="card-text text-white">Discover the development journey of J-Show Builder GPT, an AI-powered tool that generates fun trivia games in a grid format for WebSpark’s JShow Trivia Demo.</p>
                                    </div>
                                    <div class="card-footer"></div>
                                </div>
                            </div>
                            <div class="col">
                                <div class="card h-100 bg-dark text-white">
                                    <div class="card-body">
                                        <h5 class="card-title">
                                            <i class="bi bi-arrow-right-circle-fill me-2"></i>
                                            <a class="text-white" href="/articles/integrating-chat-completions-into-prompt-spark.html" title="Learn how chat completions enhances LLM interactions in the Prompt Spark project by enabling seamless chat functionalities for Core Spark Variants.">Integrating Chat Completion into Prompt Spark</a>
                                        </h5>
                                        <p class="card-text text-white">Learn how chat completions enhances LLM interactions in the Prompt Spark project by enabling seamless chat functionalities for Core Spark Variants.</p>
                                    </div>
                                    <div class="card-footer"></div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="mt-4"><p class="text-white"></p></div>
                </div>
            </div>
            <div class="row">
                <div class="col-1"><br /></div>
            </div>
            <div class="row">
                <div class="col-1"><br /></div>
                <div class="col-10 justify-content-end">
                    <br />
                    <div class="social-icons d-flex">
                        <a class="social-icon" href="https://www.linkedin.com/in/markhazleton" target="_blank" rel="noopener noreferrer" title="LinkedIn Profile"><i class="fab fa-linkedin-in"></i></a>
                        <a class="social-icon" href="https://github.com/markhazleton" target="_blank" rel="noopener noreferrer" title="GitHub Profile"><i class="fab fa-github"></i></a>
                        <a class="social-icon" href="https://www.youtube.com/@MarkHazleton" target="_blank" rel="noopener noreferrer" title="YouTube Channel"><i class="fab fa-youtube"></i></a>
                        <a class="social-icon" href="https://www.instagram.com/markhazleton/" target="_blank" rel="noopener noreferrer" title="Instagram Profile"><i class="fab fa-instagram"></i></a>
                        <a class="social-icon" href="https://markhazleton.com/rss.xml" target="_blank" rel="noopener noreferrer" title="RSS Feed"><i class="fas fa-rss"></i></a>
                        <br />
                    </div>
                </div>
            </div>
            <div class="row">
                <div class="col-12"><br /></div>
            </div>
        </footer>
        <!-- Core theme JS-->
        <script src="/js/scripts.js"></script>
    </body>
</html>
