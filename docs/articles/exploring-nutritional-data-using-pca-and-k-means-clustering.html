<!-- Modern Layout with Dynamic SEO - Centralizes all SEO meta tag generation-->
<!-- Uses articles.json data to automatically populate title, description, keywords-->
<!DOCTYPE html>
<html lang="en">
    <head>
        <!-- Essential Meta Tags-->
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta name="color-scheme" content="light dark" />
        <!-- Content Security Policy for Google Analytics support-->
        <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https://www.googletagmanager.com https://www.google-analytics.com; connect-src 'self' https://www.googletagmanager.com https://www.google-analytics.com https://analytics.google.com https://region1.google-analytics.com https://region1.analytics.google.com https://stats.g.doubleclick.net; img-src 'self' data: https://www.google-analytics.com https://www.googletagmanager.com; style-src 'self' 'unsafe-inline'; font-src 'self'; worker-src 'self'; frame-src 'none'; object-src 'none'; base-uri 'self'; form-action 'self'" />
        <!-- Theme color for supported browsers (Chrome, Safari, Edge)-->
        <meta name="theme-color" content="#2c3e50" media="(prefers-color-scheme: light)" />
        <meta name="theme-color" content="#34495e" media="(prefers-color-scheme: dark)" />
        <!-- Dynamic SEO Meta Tags from articles.json-->
        <title>Exploring Nutritional Data with K-means</title>
        <meta name="description" content="Discover how K-means clustering can analyze nutritional data, segmenting foods based on nutrient content. Learn techniques to enhance dietary insights." />
        <meta name="keywords" content="K-means clustering, nutritional data, data science, Google Colab, Mark Hazleton" />
        <meta name="author" content="Mark Hazleton" />
        <meta name="robots" content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1" />
        <!-- Page-specific metadata block (optional overrides)-->
        <!-- Canonical URL-->
        <link rel="canonical" href="https://markhazleton.com/articles/exploring-nutritional-data-using-pca-and-k-means-clustering.html" />
        <!-- Default canonical override block (for backward compatibility)-->
        <!-- Open Graph Meta Tags - Optimized for LinkedIn Sharing-->
        <meta property="og:title" content="Exploring Nutritional Data Using K-means Clustering" />
        <meta property="og:description" content="Discover how K-means clustering can analyze nutritional data, segmenting foods based on nutrient content. Learn techniques to enhance dietary insights." />
        <meta property="og:type" content="article" />
        <meta property="og:url" content="https://markhazleton.com/articles/exploring-nutritional-data-using-pca-and-k-means-clustering.html" />
        <meta property="og:image" content="https://markhazleton.com/assets/img/ScotlandRainbow.jpg" />
        <meta property="og:image:width" content="1200" />
        <meta property="og:image:height" content="630" />
        <meta property="og:image:alt" content="Exploring Nutritional Data Using K-means Clustering - Mark Hazleton" />
        <meta property="og:site_name" content="Mark Hazleton" />
        <meta property="og:locale" content="en_US" />
        <!-- LinkedIn-specific optimizations-->
        <meta property="article:author" content="Mark Hazleton" />
        <meta property="article:published_time" content="2024-10-04" />
        <meta property="article:tag" content="K-means clustering" />
        <meta property="article:tag" content="data science" />
        <meta property="article:tag" content="unsupervised learning" />
        <meta property="article:tag" content="Mark Hazleton" />
        <meta property="article:tag" content="clustering algorithm" />
        <meta property="article:tag" content="PCA" />
        <meta property="article:tag" content="data analysis" />
        <!-- YouTube video Open Graph tags-->
        <!-- Additional Open Graph overrides (for backward compatibility)-->
        <!-- Twitter Card Meta Tags-->
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:site" content="@markhazleton" />
        <meta name="twitter:creator" content="@markhazleton" />
        <meta name="twitter:title" content="Nutritional Data with K-means" />
        <meta name="twitter:description" content="Discover how K-means clustering can analyze nutritional data, segmenting foods based on nutrient content. Learn techniques to enhance dietary insights." />
        <meta name="twitter:image" content="https://markhazleton.com/assets/img/ScotlandRainbow.jpg" />
        <meta name="twitter:image:alt" content="Exploring Nutritional Data Using K-means Clustering - Mark Hazleton" />
        <!-- YouTube video Twitter Card tags-->
        <!-- Additional Twitter Card overrides (for backward compatibility)-->
        <!-- Performance optimization - No external CDN dependencies-->
        <!-- Favicon and app icons-->
        <link rel="shortcut icon" href="/assets/img/favicon.ico" />
        <link rel="icon" type="image/x-icon" href="/assets/img/favicon.ico" />
        <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
        <link rel="manifest" href="/site.webmanifest" />
        <!-- RSS Feeds - Articles and Projects-->
        <link rel="alternate" type="application/rss+xml" title="Mark Hazleton Articles" href="/rss.xml" />
        <link rel="alternate" type="application/rss+xml" title="Mark Hazleton Projects" href="/projects-rss.xml" />
        <!-- Modern Layout Styles (includes Bootstrap 5, Bootstrap Icons, Font Awesome, PrismJS)-->
        <link href="/css/modern-styles.css" rel="stylesheet" />
        <!-- Enhanced JSON-LD Structured Data for SEO-->
        <script type="application/ld+json">
            {
                "@context": "https://schema.org",
                "@type": "Person",
                "name": "Mark Hazleton",
                "givenName": "Mark",
                "familyName": "Hazleton",
                "jobTitle": "Solutions Architect",
                "description": "Solutions Architect passionate for solutions which make technology work for business",
                "url": "https://markhazleton.com",
                "image": {
                    "@type": "ImageObject",
                    "url": "https://markhazleton.com/assets/img/MarkHazleton.jpg",
                    "width": 400,
                    "height": 400
                },
                "sameAs": ["https://www.linkedin.com/in/markhazleton/", "https://github.com/markhazleton/", "https://twitter.com/markhazleton/", "https://www.youtube.com/@MarkHazleton"],
                "knowsAbout": [".NET Framework", "ASP.NET Core", "Microsoft Azure", "Project Management", "Web Development", "Solution Architecture", "Software Engineering", "Cloud Computing", "Artificial Intelligence", "Machine Learning"],
                "alumniOf": {
                    "@type": "Organization",
                    "name": "University of North Texas"
                },
                "address": {
                    "@type": "PostalAddress",
                    "addressLocality": "Wichita",
                    "addressRegion": "KS",
                    "addressCountry": "US"
                },
                "mainEntityOfPage": {
                    "@type": "WebPage",
                    "@id": "https://markhazleton.com/"
                }
            }
        </script>
        <!-- Additional structured data for specific pages-->
        <!-- Additional page-specific CSS-->
        <!-- Include LinkedIn sharing mixins-->
        <!-- Global site tag (gtag.js) - Google Analytics-->
        <script src="https://www.googletagmanager.com/gtag/js?id=G-L8GVZNDH0B" async></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag() {
                dataLayer.push(arguments);
            }
            gtag('js', new Date());
            gtag('config', 'G-L8GVZNDH0B');
        </script>
    </head>
    <body>
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top" role="navigation" aria-label="Main navigation">
            <div class="container-fluid">
                <a class="navbar-brand" href="/#" aria-label="Mark Hazleton homepage">Mark Hazleton</a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
                <div class="collapse navbar-collapse" id="navbarNav">
                    <ul class="navbar-nav me-auto">
                        <li class="nav-item"><a class="nav-link active" href="/#about" aria-current="page">About Mark</a></li>
                        <li class="nav-item"><a class="nav-link" href="/projects.html">Projects</a></li>
                        <li class="nav-item"><a class="nav-link" href="/articles.html">Articles</a></li>
                    </ul>
                    <!-- Search Box - Inside collapsible navbar-->
                    <form class="d-flex me-3 my-2 my-lg-0" onsubmit="return searchArticles(event)">
                        <div class="input-group search-box">
                            <input class="form-control" id="headerSearchInput" type="search" placeholder="Search articles..." aria-label="Search articles" autocomplete="off" />
                            <button class="btn btn-outline-light" type="submit" aria-label="Search"><i class="fas fa-search"></i></button>
                        </div>
                    </form>
                    <!-- Social Icons - Inside collapsible navbar-->
                    <div class="d-flex social-icons">
                        <a href="https://www.linkedin.com/in/markhazleton/" target="_blank" rel="noopener noreferrer" title="LinkedIn">
                            <i class="fab fa-linkedin-in"></i>
                            <span class="visually-hidden">LinkedIn Profile</span>
                        </a>
                        <a href="https://github.com/markhazleton/" target="_blank" rel="noopener noreferrer" title="GitHub">
                            <i class="fab fa-github"></i>
                            <span class="visually-hidden">GitHub Profile</span>
                        </a>
                        <a href="https://www.youtube.com/@MarkHazleton" target="_blank" rel="noopener noreferrer" title="YouTube">
                            <i class="fab fa-youtube"></i>
                            <span class="visually-hidden">YouTube Channel</span>
                        </a>
                        <a href="/rss.xml" target="_blank" rel="noopener noreferrer" title="Articles RSS Feed">
                            <i class="fas fa-rss"></i>
                            <span class="visually-hidden">Articles RSS Feed</span>
                        </a>
                        <a href="/projects-rss.xml" target="_blank" rel="noopener noreferrer" title="Projects RSS Feed">
                            <i class="fas fa-rss text-warning"></i>
                            <span class="visually-hidden">Projects RSS Feed</span>
                            <i class="fab fa-youtube"></i>
                        </a>
                    </div>
                </div>
            </div>
        </nav>
        <br />
        <br />
        <!-- Hero Section-->
        <section class="bg-gradient-primary py-5">
            <div class="container">
                <div class="row align-items-center">
                    <div class="col-lg-10 mx-auto text-center">
                        <h1 class="display-4 fw-bold mb-3">
                            <i class="bi bi-cloud me-3"></i>
                            Exploreing Nutritional Data Using PCA and K-Means Clustering
                        </h1>
                        <h2 class="subheading mb-4">Understanding K-means Clustering</h2>
                    </div>
                </div>
            </div>
        </section>
        <!-- Main Article Content-->
        <article id="main-article">
            <div class="container">
                <div class="row">
                    <p class="lead">Clustering, from a data science perspective, refers to the process of grouping a set of objects or data points in such a way that items in the same group (called a cluster) are more similar to each other than to those in other groups. It’s a key technique in unsupervised learning, where the algorithm identifies patterns and relationships in data without predefined labels or categories. The goal of clustering is to uncover hidden structures in data by organizing it into meaningful groups based on their similarities. This approach is widely used in various fields, including customer segmentation, market analysis, image recognition, and medical diagnostics, helping data scientists make sense of complex datasets and draw valuable insights.</p>
                    <p>Let's look at K-means clustering, a popular algorithm for partitioning data into clusters based on their similarities.</p>
                    <dl>
                        <dt>K</dt>
                        <dd>The "K" in K-means refers to the number of clusters you want to divide your data into. For example, if you choose K = 3, the algorithm will create three groups based on the similarities in the data.</dd>
                        <dt>Means</dt>
                        <dd>The "means" part refers to the centroids or averages of the clusters. In K-means clustering, each cluster has a central point, which is the average (or mean) of all the data points in that cluster. The algorithm calculates these means and adjusts them as it organizes the data into clusters.</dd>
                        <dt>Clustering</dt>
                        <dd>Clustering is the process of grouping similar data points together. In K-means clustering, data points are grouped into K clusters based on their similarity to the centroids. The goal is to minimize the distance between data points and their assigned cluster’s centroid.</dd>
                    </dl>
                    <p>By iteratively adjusting the clusters and centroids, K-means efficiently finds patterns and groups within data, making it one of the most widely used techniques for discovering hidden structures in datasets.</p>
                    <p>K-means Clustering is a method used to group data points into clusters based on their similarities. Each cluster has a center point called a centroid, and data points are grouped by how close they are to this centroid.</p>
                    <p>K-means clustering is a popular unsupervised machine learning algorithm that partitions data into 𝑘 clusters, where each point belongs to the cluster with the nearest mean (centroid). The algorithm iterates to minimize the variance within clusters, producing an optimal clustering result. This article will walk through its key concepts, applications, and provide answers to common questions.</p>
                    <div class="card">
                        <div class="card-header"><h5 class="card-title">Student Grade Example</h5></div>
                        <div class="card-body">
                            <p class="card-text">Let’s imagine you’re a teacher with a class of students, and you want to group them based on their grades in subjects like Math, Science, English, Drama, Band, Choir, History, and P.E. You don’t know in advance which students are good at which subjects, so you use K-means clustering to let the data guide you. Using K-means clustering, we can find groups of students who are similar in their performance in these subjects, but unlike other methods, this is an unsupervised approach. Since K-means is an unsupervised clustering algorithm, we don't start with any assumptions about which students belong to which group. Instead, we let the data, in this case the students' grades, lead us to the grouping. The algorithm analyzes their performance across all subjects and forms clusters based on similarities in their grades.</p>
                            <dl>
                                <dt>Start with a Guess</dt>
                                <dd>At the beginning, you randomly place three imaginary "centers" (these are like starting points) anywhere in the grade data. You don't know yet which students belong to which group, so these centers are just rough guesses.</dd>
                                <dt>Group By grades</dt>
                                <dd>Next, you look at each student’s grades and see which center they are closest to. For example, a student who has high grades in Math and Science might be grouped with the "math and science" center, while a student with high Drama and Band grades might be closer to the "arts" center.</dd>
                                <dt>Move the Center</dt>
                                <dd>After assigning students to the nearest group, you update each center by moving it to the average of the grades in its group. For example, if the "math and science" group now has several students, the center moves to the average of their Math and Science grades.</dd>
                                <dt>Repeat</dt>
                                <dd>You keep repeating this process, assigning students to the nearest center and updating the centers, until the centers no longer move. At this point, the algorithm has found stable groups of students based on their grades. You keep repeating this process—grouping students by the nearest center, moving the centers to the average of each group—until the centers stop moving. This means the groups are now stable, and you’ve found clusters of students based on their grades.</dd>
                            </dl>
                            <p>In this way, K-means doesn’t start with any assumptions about which students are good in which subjects. The algorithm uses the grade data to create groups naturally, allowing you to find patterns in how students perform across subjects. Whether they are strong in math or enjoy the arts, the data leads to the final grouping! This is the power of unsupervised learning. Here is a simple example of how K-means clustering might group students based on their grades in different subjects:</p>
                            <ul>
                                <li>
                                    <strong>Group 1:</strong>
                                    Students with high grades in Math and Science might be grouped together. This could indicate a group of students who are strong in math and science subjects.
                                </li>
                                <li>
                                    <strong>Group 2:</strong>
                                    Students with high grades in Drama, Band, and Choir might naturally group together. These students may enjoy the arts or theatre-related activities.
                                </li>
                                <li>
                                    <strong>Group 3:</strong>
                                    Students with high grades in P.E. and History could form a group, reflecting students who excel in physical activities or sports.
                                </li>
                            </ul>
                            <p class="card-text">In this way, K-means clustering allows the data itself to define the groups, helping teachers see patterns in student strengths without making any assumptions beforehand. K-means Clustering helps to identify patterns and group similar data points, making it easier to analyze and understand large datasets.</p>
                        </div>
                        <div class="card-footer"><a class="btn btn-primary" href="https://en.wikipedia.org/wiki/K-means_clustering" target="_blank" rel="nofollow noopener noreferrer" title="Learn more about K-means Clustering on Wikipedia" alt="Wikipedia on K-means Clustering">Wikipedia on K-means Clustering</a></div>
                    </div>
                    <h2 class="subheading mb-3">K-Means Clustering Nutrient Data</h2>
                    <p>
                        I recently analyzed a nutritional dataset from Kaggle using Google Colab, a platform that lets you write and execute Python code in a browser-based Jupyter notebook. The goal was to understand nutrient patterns across various foods and to segment these items based on their nutrient content. You can find the full code and analysis in my Google Drive folder
                        <a href="https://drive.google.com/drive/folders/1cF49bLIgTwHMNwo7TxSjMw_8m-yw2cBg?usp=sharing" target="_blank" rel="nofollow noopener noreferrer" title="Google Drive Folder with Code and Analysis" alt="Google Drive Folder">here.</a>
                    </p>
                    <p>Kaggle is an excellent resource for data scientists and enthusiasts to find and share datasets. It offers a vast collection of datasets across various domains, making it a great place to practice data analysis and machine learning techniques. You can find datasets on topics ranging from healthcare to finance, and even niche areas like nutritional data. Kaggle also hosts competitions where you can test your skills against others and learn from the community.</p>
                    <p>I got my hands on a dataset containing nutritional values for common foods and products, including protein, fat, vitamin C, and fiber content.</p>
                    <p>I have already performed data exploration and manipulation steps, so let's dive into the key concepts of K-means clustering. We have a clean dataset with the nutrient values of various foods, and we are ready to apply K-means clustering to segment these foods based on their nutrient content.</p>
                    <dl>
                        <dt>Random Initialization</dt>
                        <dd>
                            K-means starts by randomly selecting 𝑘 initial centroids. Since this process is random, different runs of K-means can yield different results. This issue can be reduced by running the algorithm multiple times and selecting the result with the lowest sum of squared errors (SSE).
                            <pre class="language-csharp"><code class="language-csharp"><span class="token comment">// Example pseudocode for random initialization</span>
Initialize k <span class="token class-name">random</span> centroids<span class="token punctuation">;</span>
<span class="token keyword">while</span> <span class="token keyword">not</span> converged<span class="token punctuation">:</span>
  Assign points to the <span class="token class-name">nearest</span> centroid<span class="token punctuation">;</span>
  Update <span class="token class-name">centroid</span> locations<span class="token punctuation">;</span>
End <span class="token keyword">while</span><span class="token punctuation">;</span>
</code></pre>
                        </dd>
                        <dt>Cluster Assignment and Centroid Update</dt>
                        <dd>
                            After initialization, the algorithm assigns data points to the nearest centroid based on Euclidean distance. The centroids are then updated by calculating the mean of the points in each cluster. This process is repeated until the centroids stabilize.
                            <pre class="language-csharp"><code class="language-csharp"><span class="token comment">// Pseudocode for cluster assignment and update</span>
<span class="token keyword">for</span> <span class="token class-name">each</span> point <span class="token keyword">in</span> dataset<span class="token punctuation">:</span>
  Assign point to <span class="token class-name">nearest</span> centroid<span class="token punctuation">;</span>
<span class="token class-name">Update</span> centroids<span class="token punctuation">;</span>
Repeat until centroids no <span class="token class-name">longer</span> change<span class="token punctuation">;</span>
</code></pre>
                        </dd>
                        <dt>Sum of Squared Errors (SSE)</dt>
                        <dd>
                            K-means aims to minimize the sum of squared errors (SSE), which is the total squared distance between points and their centroids. The lower the SSE, the better the clustering result.
                            <pre class="language-csharp"><code class="language-csharp"><span class="token comment">// Formula for SSE</span>
SSE <span class="token operator">=</span> <span class="token function">sum</span><span class="token punctuation">(</span><span class="token punctuation">(</span>x <span class="token operator">-</span> centroid<span class="token punctuation">)</span><span class="token operator">^</span><span class="token number">2</span> <span class="token keyword">for</span> <span class="token class-name">all</span> x <span class="token keyword">in</span> cluster<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
                        </dd>
                        <dt>Choosing the Optimal Number of Clusters</dt>
                        <dd>Two common methods for selecting the optimal number of clusters are: - **Elbow Method:** Plot SSE for different values of 𝑘 and look for the "elbow," where adding more clusters no longer significantly reduces SSE. - **Silhouette Score:** Measures how similar a point is to its own cluster compared to other clusters. The higher the score, the better the clustering.</dd>
                        <dt>Scaling Data for K-Means</dt>
                        <dd>Scaling is critical in K-means to ensure that all features contribute equally to the distance calculations. Data should be scaled, especially when the features have different units or scales, to prevent larger-scale features from dominating the clustering.</dd>
                    </dl>
                    <div class="row">
                        <div class="col-12">
                            <div class="accordion" id="faqAccordion">
                                <div class="accordion-item">
                                    <span class="accordion-header" id="headingOne"><button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseOne" aria-expanded="false" aria-controls="collapseOne">Q1: Why might different runs of K-means produce slightly different results?</button></span>
                                    <div class="accordion-collapse collapse" id="collapseOne" aria-labelledby="headingOne" data-bs-parent="#faqAccordion"><div class="accordion-body">K-means relies on random initialization of centroids, which can lead to different clustering results. Running the algorithm multiple times and selecting the result with the lowest SSE can address this issue.</div></div>
                                </div>
                                <div class="accordion-item">
                                    <span class="accordion-header" id="headingTwo"><button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseTwo" aria-expanded="false" aria-controls="collapseTwo">Q2: Does K-means automatically scale the data during the iterations?</button></span>
                                    <div class="accordion-collapse collapse" id="collapseTwo" aria-labelledby="headingTwo" data-bs-parent="#faqAccordion"><div class="accordion-body">No, K-means does not scale data automatically. It is necessary to scale the data manually before applying the algorithm.</div></div>
                                </div>
                                <div class="accordion-item">
                                    <span class="accordion-header" id="headingThree"><button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseThree" aria-expanded="false" aria-controls="collapseThree">Q3: How does K-means compute the centroids for clusters?</button></span>
                                    <div class="accordion-collapse collapse" id="collapseThree" aria-labelledby="headingThree" data-bs-parent="#faqAccordion"><div class="accordion-body">The centroids are calculated by taking the mean of all points assigned to each cluster after every iteration.</div></div>
                                </div>
                                <div class="accordion-item">
                                    <span class="accordion-header" id="headingFour"><button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseFour" aria-expanded="false" aria-controls="collapseFour">Q4: Why is it important to specify the number of clusters in K-means?</button></span>
                                    <div class="accordion-collapse collapse" id="collapseFour" aria-labelledby="headingFour" data-bs-parent="#faqAccordion"><div class="accordion-body">K-means requires specifying the number of clusters (𝑘) beforehand, as this determines how many centroids are initialized and how the data is partitioned.</div></div>
                                </div>
                                <div class="accordion-item">
                                    <span class="accordion-header" id="headingFive"><button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseFive" aria-expanded="false" aria-controls="collapseFive">Q5: What happens if the number of clusters is equal to the number of data points?</button></span>
                                    <div class="accordion-collapse collapse" id="collapseFive" aria-labelledby="headingFive" data-bs-parent="#faqAccordion"><div class="accordion-body">If 𝑘 equals the number of data points, each point will be its own cluster, and the SSE will be zero.</div></div>
                                </div>
                                <div class="accordion-item">
                                    <span class="accordion-header" id="headingSix"><button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSix" aria-expanded="false" aria-controls="collapseSix">Q6: Is K-means sensitive to outliers?</button></span>
                                    <div class="accordion-collapse collapse" id="collapseSix" aria-labelledby="headingSix" data-bs-parent="#faqAccordion"><div class="accordion-body">Yes, K-means can be sensitive to outliers, as they can heavily influence the centroids. Handling outliers before applying K-means is important, or consider using other algorithms like K-medoids.</div></div>
                                </div>
                                <div class="accordion-item">
                                    <span class="accordion-header" id="headingSeven"><button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSeven" aria-expanded="false" aria-controls="collapseSeven">Q7: What kind of distance metric does K-means use?</button></span>
                                    <div class="accordion-collapse collapse" id="collapseSeven" aria-labelledby="headingSeven" data-bs-parent="#faqAccordion"><div class="accordion-body">K-means typically uses Euclidean distance for measuring the distance between points and centroids.</div></div>
                                </div>
                                <div class="accordion-item">
                                    <span class="accordion-header" id="headingEight"><button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseEight" aria-expanded="false" aria-controls="collapseEight">Q8: Can K-means be used for non-spherical clusters?</button></span>
                                    <div class="accordion-collapse collapse" id="collapseEight" aria-labelledby="headingEight" data-bs-parent="#faqAccordion"><div class="accordion-body">K-means works best for spherical clusters with relatively uniform variance. For non-spherical clusters, other algorithms like DBSCAN may be more appropriate.</div></div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <dl>
                        <dt>Dimensionality Reduction with PCA</dt>
                        <dd><p>I applied Principal Component Analysis (PCA) to reduce the complexity of the data. PCA transforms the data into new dimensions, called principal components, which capture the most significant patterns. This allowed me to visualize nutrient relationships in just two dimensions, preserving about 68% of the data’s variance, or information.</p></dd>
                        <dt>Segmentation Using K-means Clustering</dt>
                        <dd><p>I applied K-means clustering, a technique that divides data into clusters based on their similarity. To determine the optimal number of clusters, I plotted an elbow curve using the inertia (sum of squared distances within clusters). The “elbow point” at three clusters indicated a good balance. I confirmed this choice with the silhouette score, which measures how well-separated the clusters are; a score closer to 1 suggests distinct and well-defined clusters.</p></dd>
                    </dl>
                    <h3>Key Concepts</h3>
                    <p>This project provided valuable experience with PCA and K-means clustering. While Google Colab allowed me to work interactively, PCA simplified the data by highlighting major patterns, and K-means grouped the data into meaningful clusters, which could be helpful for future analyses like customer segmentation or recommendation systems.</p>
                    <div class="card">
                        <div class="card-header"><h3 class="card-title">Principal Component Analysis (PCA)</h3></div>
                        <div class="card-body"></div>
                        <p>Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms data into a smaller number of uncorrelated variables (principal components) that capture the main patterns in the data.</p>
                        <p>Imagine you have a lot of data, like test scores in math, science, history, and art. It can be hard to see the big picture when you have many different variables, especially if there are a lot of patterns and connections between them. PCA is a tool that helps you simplify this information. It takes a big set of data with many variables and reduces it to a smaller set of "components" that still capture the most important information.</p>
                        <dl>
                            <dt>Principal Component Analysis (PCA)</dt>
                            <dd></dd>
                            <p>Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms data into a smaller number of uncorrelated variables (principal components) that capture the main patterns in the data.</p>
                            <p>Imagine you have a lot of data, like test scores in math, science, history, and art. It can be hard to see the big picture when you have many different variables, especially if there are a lot of patterns and connections between them. PCA is a tool that helps you simplify this information. It takes a big set of data with many variables and reduces it to a smaller set of "components" that still capture the most important information.</p>
                            <dt>How PCA Works</dt>
                            <dd></dd>
                            <p>PCA works by looking for patterns in the data. It finds directions (or components) in which the data varies the most. These components are like new axes, and they represent combinations of the original variables. Here’s a step-by-step look at how PCA simplifies data:</p>
                            <ol>
                                <li>Find the Directions of Maximum Variance: PCA looks for the direction in the data where the points are spread out the most. This direction is called the first principal component. It shows the main pattern in the data.</li>
                                <li>Add More Components: After finding the first principal component, PCA looks for the next direction where the data varies the most, but at a right angle to the first one. This is the second principal component. These components are always at right angles (90 degrees) to each other, making them uncorrelated.</li>
                                <li>Keep the Important Components: You can add as many components as there are variables, but most of the time, only the first few components capture the main patterns. By keeping only the most important ones, you reduce the data’s complexity.</li>
                            </ol>
                            <dt>Example with Simple Data</dt>
                            <dd></dd>
                            <p>Imagine a dataset with students' scores in Math and Science. If there’s a strong relationship (students who do well in Math also do well in Science), PCA would combine these scores into one main component that represents both subjects.</p>
                            <ol>
                                <li>Original Data: Math and Science Scores</li>
                                <li>First Principal Component: Overall Academic Ability</li>
                                <li>Second Principal Component: Difference Between Math and Science</li>
                            </ol>
                            <p>So, instead of tracking two scores separately, PCA would give you an “Overall Academic Ability” score that captures the main pattern, simplifying the data.</p>
                            <dt>Why PCA is Useful</dt>
                            <dd></dd>
                            <p>PCA has several benefits for analyzing data:</p>
                            <ol>
                                <li>Reduces Complexity: PCA simplifies data by reducing the number of variables you need to look at. You keep the main patterns and ignore the rest, making the data easier to understand.</li>
                                <li>Finds Patterns: PCA reveals hidden patterns by focusing on the directions where the data varies the most.</li>
                                <li>Helps with Visualization: If you have data with many variables, PCA can reduce it to two or three components, making it possible to plot and visualize.</li>
                            </ol>
                        </dl>
                        <p>In summary, PCA is a way to look at the big picture of your data by focusing on what’s most important. It turns a complex dataset into something simpler, allowing you to see patterns and relationships that would otherwise be hidden.</p>
                        <div class="card-footer"><a class="btn btn-primary" href="https://en.wikipedia.org/wiki/Principal_component_analysis" target="_blank" rel="nofollow noopener noreferrer" title="Learn more about Principal Component Analysis on Wikipedia" alt="Wikipedia on PCA">Wikipedia on PCA</a></div>
                        <br />
                        <hr />
                        <br />
                        <div class="card">
                            <div class="card-header"><h3 class="card-title">Principal Components</h3></div>
                            <div class="card-body"></div>
                            <p>Principal Components are the new variables created by PCA that represent the directions of maximum variance in the data. Each principal component is a linear combination of the original variables, and they are uncorrelated with each other.</p>
                            <p>Here’s a more detailed look at Principal Components:</p>
                            <dl>
                                <dt>First Principal Component</dt>
                                <dd></dd>
                                <p>The first principal component captures the largest amount of variance in the data. It is the direction in which the data varies the most. This component often represents the most significant pattern in the dataset.</p>
                                <dt>Second Principal Component</dt>
                                <dd></dd>
                                <p>The second principal component captures the second largest amount of variance, but it is orthogonal (at a right angle) to the first component. This ensures that it represents a different pattern in the data.</p>
                                <dt>Subsequent Principal Components</dt>
                                <dd></dd>
                                <p>Each subsequent principal component captures the next highest variance while being orthogonal to all previous components. These components continue to represent new patterns in the data, but each captures less variance than the previous one.</p>
                                <dt>Eigenvalues and Eigenvectors</dt>
                                <dd></dd>
                                <p>Principal components are derived from the eigenvectors of the covariance matrix of the data. The eigenvalues associated with these eigenvectors indicate the amount of variance captured by each principal component.</p>
                                <dt>Interpretation</dt>
                                <dd></dd>
                                <p>Interpreting principal components involves looking at the coefficients of the original variables in each component. These coefficients indicate the contribution of each variable to the component, helping to understand the underlying patterns.</p>
                            </dl>
                            <p>In summary, Principal Components are essential in PCA as they transform the data into a new set of variables that capture the most significant patterns, making it easier to analyze and visualize complex datasets.</p>
                            <div class="card-footer"><a class="btn btn-primary" href="https://en.wikipedia.org/wiki/Principal_component_analysis#Details" target="_blank" rel="nofollow noopener noreferrer" title="Learn more about Principal Components on Wikipedia" alt="Wikipedia on Principal Components">Wikipedia on Principal Components</a></div>
                        </div>
                        <br />
                        <hr />
                        <br />
                        <h2>Glossary of Terms</h2>
                        <dl>
                            <dt>Google Colab</dt>
                            <dd></dd>
                            A free, browser-based environment for writing and running Python code in Jupyter notebooks, particularly popular for data science projects.
                            <dt>Jupyter Notebook</dt>
                            <dd></dd>
                            An interactive coding environment that allows you to combine code, visualizations, and text, commonly used in data science.
                            <dt>Dendrogram</dt>
                            <dd></dd>
                            A tree-like diagram used to illustrate the arrangement of clusters formed by hierarchical clustering.
                            <dt>Elbow Curve</dt>
                            <dd></dd>
                            A plot used to determine the optimal number of clusters in K-means by finding a point where the decrease in variance slows, resembling an "elbow."
                            <dt>Inertia</dt>
                            <dd></dd>
                            The sum of squared distances between data points and their cluster centroids. Lower inertia indicates tighter clusters.
                            <dt>Silhouette Score</dt>
                            <dd></dd>
                            A measure of how similar each point is to its cluster compared to other clusters, with values close to 1 indicating well-separated clusters.
                            <dt>Dimensionality Reduction</dt>
                            <dd></dd>
                            The process of reducing the number of variables in a dataset while retaining as much information as possible.
                        </dl>
                    </div>
                </div>
            </div>
        </article>
        <!-- Auto-include floating LinkedIn share for article pages-->
        <!-- Check if first parameter is an article object-->
        <div class="floating-linkedin-share position-fixed" style="bottom: 20px; right: 20px; z-index: 1000">
            <a class="btn btn-linkedin btn-lg rounded-circle shadow" href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fmarkhazleton.com%2Farticles%2Fexploring-nutritional-data-using-pca-and-k-means-clustering.html&amp;title=Exploring%20Nutritional%20Data%20Using%20K-means%20Clustering&amp;summary=Discover%20how%20K-means%20clustering%20can%20be%20used%20to%20analyze%20nutritional%20data%2C%20segmenting%20foods%20based%20on%20their%20nutrient%20content.&amp;source=Mark%20Hazleton" target="_blank" rel="noopener noreferrer" title="Share on LinkedIn" onclick="return openLinkedInShare(this.href);"><i class="fab fa-linkedin-in"></i></a>
        </div>
        <!-- Footer-->
        <footer>
            <div class="container">
                <div class="row">
                    <div class="col-md-6 mb-4">
                        <h5 class="mb-3">Mark Hazleton</h5>
                        <p class="text-light">Solutions Architect passionate for solutions which make technology work for business. Lifelong learner, not sidetracked by sizzle.</p>
                        <div class="social-icons">
                            <a href="https://www.linkedin.com/in/markhazleton/" target="_blank" rel="noopener noreferrer" aria-label="LinkedIn">
                                <i class="fab fa-linkedin-in"></i>
                                <span class="visually-hidden">LinkedIn Profile</span>
                            </a>
                            <a href="https://github.com/markhazleton/" target="_blank" rel="noopener noreferrer" aria-label="GitHub">
                                <i class="fab fa-github"></i>
                                <span class="visually-hidden">GitHub Profile</span>
                            </a>
                            <a href="https://www.youtube.com/@MarkHazleton" target="_blank" rel="noopener noreferrer" aria-label="YouTube">
                                <i class="fab fa-youtube"></i>
                                <span class="visually-hidden">YouTube Channel</span>
                            </a>
                            <a href="/rss.xml" target="_blank" rel="noopener noreferrer" aria-label="Articles RSS Feed">
                                <i class="fas fa-rss"></i>
                                <span class="visually-hidden">Articles RSS Feed</span>
                            </a>
                            <a href="/projects-rss.xml" target="_blank" rel="noopener noreferrer" aria-label="Projects RSS Feed">
                                <i class="fas fa-rss"></i>
                                <span class="visually-hidden">Projects RSS Feed.text-warning</span>
                            </a>
                        </div>
                    </div>
                    <div class="col-md-3 mb-4">
                        <h6 class="mb-3">Quick Links</h6>
                        <ul class="list-unstyled footer-links">
                            <li><a href="/#about">About</a></li>
                            <li><a href="/#experience">Experience</a></li>
                            <li><a href="/projects.html">Projects</a></li>
                            <li><a href="/articles.html">Articles</a></li>
                        </ul>
                    </div>
                    <div class="col-md-3 mb-4">
                        <h6 class="mb-3">Resources</h6>
                        <ul class="list-unstyled footer-links">
                            <li><a href="/projectmechanics/">Project Mechanics</a></li>
                            <li><a href="https://webspark.markhazleton.com">WebSpark</a></li>
                            <li><a href="/rss.xml">Articles RSS Feed</a></li>
                            <li><a href="/projects-rss.xml">Projects RSS Feed</a></li>
                            <li><a href="/sitemap.xml">Sitemap</a></li>
                        </ul>
                    </div>
                </div>
                <hr class="my-4" />
                <div class="row">
                    <div class="col-12 text-center"><p class="mb-0">&copy; 2025 Mark Hazleton. All rights reserved.</p></div>
                </div>
            </div>
        </footer>
        <script src="/js/scripts.js"></script>
    </body>
</html>
