<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="author" content="Mark Hazleton" />
        <meta name="robots" content="index, follow" />
        <title>Exploring Data Using K-means Clustering</title>
        <meta name="description" content="A Data Scientist explores nutritional data patterns using K-means Clustering on Google Colab, segmenting foods by nutrient content." />
        <meta name="keywords" content="PCA, K-means Clustering, Nutritional Data, Google Colab, Data Science, Jupyter Notebook" />
        <meta name="author" content="Mark Hazleton" />
        <link rel="canonical" href="https://markhazleton.com/articles/exploring-nutritional-data-using-pca-and-k-means-clustering.html" />
        <link rel="shortcut icon" href="/assets/img/favicon.ico" />
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <script type="application/ld+json">
            {
                "@context": "http://schema.org",
                "@type": "Person",
                "name": "Mark Hazleton",
                "familyName": "Hazleton",
                "givenName": "Mark",
                "jobTitle": "Solutions Architect",
                "alumniOf": "University of North Texas",
                "affiliation": {
                    "@type": "Organization",
                    "name": "Control Origins"
                },
                "sameAs": ["https://www.linkedin.com/in/markhazleton/", "https://github.com/markhazleton/", "https://twitter.com/markhazleton/", "https://www.youtube.com/c/MarkHazleton/", "https://markhazleton.brandyourself.com/", "https://www.postman.com/markhazleton/", "https://stackoverflow.com/users/479571/markhazleton/", "https://www.slideshare.net/markhazleton/", "https://hub.docker.com/u/markhazleton/", "https://www.polywork.com/markhazleton/", "https://www.codeproject.com/Members/MarkHazleton/", "https://markhazleton.wordpress.com/", "https://learn.microsoft.com/en-us/users/mark-hazleton/", "https://app.pluralsight.com/profile/markhazletonCEC/", "https://app.pluralsight.com/profile/markhazleton/", "https://www.instagram.com/markhazleton/", "https://storybird.ai/u/markhazleton/", "https://www.pinterest.com/markhazleton/"],
                "url": "https://markhazleton.com"
            }
        </script>
        <script type="application/ld+json">
            {
                "@context": "https://schema.org",
                "@type": "Organization",
                "url": "https://markhazleton.com",
                "logo": "https://markhazleton.com/assets/img/MarkHazleton.jpg"
            }
        </script>
        <meta name="seobility" content="f80235aca1a812e0afde44f0142c825b" />
        <!-- No external font dependencies - using system fonts-->
        <!-- Core theme CSS (includes Bootstrap)-->
        <link rel="stylesheet" href="/css/styles.css?version=20250704" />
        <!-- Global site tag (gtag.js) - Google Analytics-->
        <script src="https://www.googletagmanager.com/gtag/js?id=G-L8GVZNDH0B" async></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag() {
                dataLayer.push(arguments);
            }
            gtag('js', new Date());
            gtag('config', 'G-L8GVZNDH0B');
        </script>
        <script>
            (function (c, l, a, r, i, t, y) {
                c[a] =
                    c[a] ||
                    function () {
                        (c[a].q = c[a].q || []).push(arguments);
                    };
                t = l.createElement(r);
                t.async = 1;
                t.src = 'https://www.clarity.ms/tag/' + i + '?ref=bwt';
                y = l.getElementsByTagName(r)[0];
                y.parentNode.insertBefore(t, y);
            })(window, document, 'clarity', 'script', 'd628hovv63');
        </script>
    </head>
    <body class="sidetracked-body" id="page-top">
        <nav class="navbar navbar-dark bg-primary p-1">
            <div class="container-fluid justify-content-end">
                <div class="social-icons d-flex">
                    <a class="social-icon" href="https://www.linkedin.com/in/markhazleton" target="_blank" rel="noopener noreferrer" title="LinkedIn Profile"><i class="fab fa-linkedin-in text-light me-2" style="font-size: 2rem"></i></a>
                    <a class="social-icon" href="https://github.com/markhazleton" target="_blank" rel="noopener noreferrer" title="GitHub Profile"><i class="fab fa-github text-light me-2" style="font-size: 2rem"></i></a>
                    <a class="social-icon" href="https://www.youtube.com/@MarkHazleton" target="_blank" rel="noopener noreferrer" title="YouTube Channel"><i class="fab fa-youtube text-light me-2" style="font-size: 2rem"></i></a>
                    <a class="social-icon" href="https://www.instagram.com/markhazleton/" target="_blank" rel="noopener noreferrer" title="Instagram Profile"><i class="fab fa-instagram text-light" style="font-size: 2rem"></i></a>
                </div>
            </div>
        </nav>
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="/#page-top" title="Mark Hazleton">
                <span class="d-block d-lg-none">Mark Hazleton</span>
                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="/assets/img/MarkHazleton.jpg" alt="..." /></span>
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon">#navbarResponsive.collapse.navbar-collapse</span>
                <ul class="navbar-nav">
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="/#about">About</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="/articles.html">Articles</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="/#projects">Projects</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="/#experience">Experience</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="https://webspark.markhazleton.com">WebSpark</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="/projectmechanics/">Project Mechanics</a></li>
                </ul>
                <!-- Search Box (for larger screens)-->
                <div class="d-none d-lg-flex ms-auto">
                    <form class="d-flex" onsubmit="return searchArticles(event)">
                        <div class="input-group search-box">
                            <input class="form-control" id="headerSearchInput" type="search" placeholder="Search articles..." aria-label="Search articles" autocomplete="off" />
                            <button class="btn btn-outline-light" type="submit" aria-label="Search"><i class="fas fa-search"></i></button>
                        </div>
                    </form>
                </div>
            </button>
        </nav>
        <!-- Page Content-->
        <div class="container-fluid p-0 painteddesert-background">
            <article class="painteddesert-section painteddesert-section-background" id="post">
                <div class="painteddesert-section-content">
                    <h1>Exploring Data K-means Clustering</h1>
                    <h2 class="subheading mb-3">Understanding K-means Clustering</h2>
                    <p>Clustering, from a data science perspective, refers to the process of grouping a set of objects or data points in such a way that items in the same group (called a cluster) are more similar to each other than to those in other groups. It’s a key technique in unsupervised learning, where the algorithm identifies patterns and relationships in data without predefined labels or categories. The goal of clustering is to uncover hidden structures in data by organizing it into meaningful groups based on their similarities. This approach is widely used in various fields, including customer segmentation, market analysis, image recognition, and medical diagnostics, helping data scientists make sense of complex datasets and draw valuable insights.</p>
                    <p>Let's look at K-means clustering, a popular algorithm for partitioning data into clusters based on their similarities.</p>
                    <dl>
                        <dt>K</dt>
                        <dd>The "K" in K-means refers to the number of clusters you want to divide your data into. For example, if you choose K = 3, the algorithm will create three groups based on the similarities in the data.</dd>
                        <dt>Means</dt>
                        <dd>The "means" part refers to the centroids or averages of the clusters. In K-means clustering, each cluster has a central point, which is the average (or mean) of all the data points in that cluster. The algorithm calculates these means and adjusts them as it organizes the data into clusters.</dd>
                        <dt>Clustering</dt>
                        <dd>Clustering is the process of grouping similar data points together. In K-means clustering, data points are grouped into K clusters based on their similarity to the centroids. The goal is to minimize the distance between data points and their assigned cluster’s centroid.</dd>
                    </dl>
                    <p>By iteratively adjusting the clusters and centroids, K-means efficiently finds patterns and groups within data, making it one of the most widely used techniques for discovering hidden structures in datasets.</p>
                    <p>K-means Clustering is a method used to group data points into clusters based on their similarities. Each cluster has a center point called a centroid, and data points are grouped by how close they are to this centroid.</p>
                    <p>K-means clustering is a popular unsupervised machine learning algorithm that partitions data into 𝑘 clusters, where each point belongs to the cluster with the nearest mean (centroid). The algorithm iterates to minimize the variance within clusters, producing an optimal clustering result. This article will walk through its key concepts, applications, and provide answers to common questions.</p>
                    <div class="card">
                        <div class="card-header"><h5 class="card-title">Student Grade Example</h5></div>
                        <div class="card-body">
                            <p class="card-text">Let’s imagine you’re a teacher with a class of students, and you want to group them based on their grades in subjects like Math, Science, English, Drama, Band, Choir, History, and P.E. You don’t know in advance which students are good at which subjects, so you use K-means clustering to let the data guide you. Using K-means clustering, we can find groups of students who are similar in their performance in these subjects, but unlike other methods, this is an unsupervised approach. Since K-means is an unsupervised clustering algorithm, we don't start with any assumptions about which students belong to which group. Instead, we let the data, in this case the students' grades, lead us to the grouping. The algorithm analyzes their performance across all subjects and forms clusters based on similarities in their grades.</p>
                            <dl>
                                <dt>Start with a Guess</dt>
                                <dd>At the beginning, you randomly place three imaginary "centers" (these are like starting points) anywhere in the grade data. You don't know yet which students belong to which group, so these centers are just rough guesses.</dd>
                                <dt>Group By grades</dt>
                                <dd>Next, you look at each student’s grades and see which center they are closest to. For example, a student who has high grades in Math and Science might be grouped with the "math and science" center, while a student with high Drama and Band grades might be closer to the "arts" center.</dd>
                                <dt>Move the Center</dt>
                                <dd>After assigning students to the nearest group, you update each center by moving it to the average of the grades in its group. For example, if the "math and science" group now has several students, the center moves to the average of their Math and Science grades.</dd>
                                <dt>Repeat</dt>
                                <dd>You keep repeating this process, assigning students to the nearest center and updating the centers, until the centers no longer move. At this point, the algorithm has found stable groups of students based on their grades. You keep repeating this process—grouping students by the nearest center, moving the centers to the average of each group—until the centers stop moving. This means the groups are now stable, and you’ve found clusters of students based on their grades.</dd>
                            </dl>
                            <p>In this way, K-means doesn’t start with any assumptions about which students are good in which subjects. The algorithm uses the grade data to create groups naturally, allowing you to find patterns in how students perform across subjects. Whether they are strong in math or enjoy the arts, the data leads to the final grouping! This is the power of unsupervised learning. Here is a simple example of how K-means clustering might group students based on their grades in different subjects:</p>
                            <ul>
                                <li>
                                    <strong>Group 1:</strong>
                                    Students with high grades in Math and Science might be grouped together. This could indicate a group of students who are strong in math and science subjects.
                                </li>
                                <li>
                                    <strong>Group 2:</strong>
                                    Students with high grades in Drama, Band, and Choir might naturally group together. These students may enjoy the arts or theatre-related activities.
                                </li>
                                <li>
                                    <strong>Group 3:</strong>
                                    Students with high grades in P.E. and History could form a group, reflecting students who excel in physical activities or sports.
                                </li>
                            </ul>
                            <p class="card-text">In this way, K-means clustering allows the data itself to define the groups, helping teachers see patterns in student strengths without making any assumptions beforehand. K-means Clustering helps to identify patterns and group similar data points, making it easier to analyze and understand large datasets.</p>
                        </div>
                        <div class="card-footer"><a class="btn btn-primary" href="https://en.wikipedia.org/wiki/K-means_clustering" target="_blank" rel="nofollow" title="Learn more about K-means Clustering on Wikipedia" alt="Wikipedia on K-means Clustering">Wikipedia on K-means Clustering</a></div>
                    </div>
                    <h2 class="subheading mb-3">K-Means Clustering Nutrient Data</h2>
                    <p>
                        I recently analyzed a nutritional dataset from Kaggle using Google Colab, a platform that lets you write and execute Python code in a browser-based Jupyter notebook. The goal was to understand nutrient patterns across various foods and to segment these items based on their nutrient content. You can find the full code and analysis in my Google Drive folder
                        <a href="https://drive.google.com/drive/folders/1cF49bLIgTwHMNwo7TxSjMw_8m-yw2cBg?usp=sharing" target="_blank" rel="nofollow" title="Google Drive Folder with Code and Analysis" alt="Google Drive Folder">here.</a>
                    </p>
                    <p>Kaggle is an excellent resource for data scientists and enthusiasts to find and share datasets. It offers a vast collection of datasets across various domains, making it a great place to practice data analysis and machine learning techniques. You can find datasets on topics ranging from healthcare to finance, and even niche areas like nutritional data. Kaggle also hosts competitions where you can test your skills against others and learn from the community.</p>
                    <p>I got my hands on a dataset containing nutritional values for common foods and products, including protein, fat, vitamin C, and fiber content.</p>
                    <p>I have already performed data exploration and manipulation steps, so let's dive into the key concepts of K-means clustering. We have a clean dataset with the nutrient values of various foods, and we are ready to apply K-means clustering to segment these foods based on their nutrient content.</p>
                    <dl>
                        <dt>Random Initialization</dt>
                        <dd>
                            K-means starts by randomly selecting 𝑘 initial centroids. Since this process is random, different runs of K-means can yield different results. This issue can be reduced by running the algorithm multiple times and selecting the result with the lowest sum of squared errors (SSE).
                            <pre class="language-csharp"><code class="language-csharp"><span class="token comment">// Example pseudocode for random initialization</span>
Initialize k <span class="token class-name">random</span> centroids<span class="token punctuation">;</span>
<span class="token keyword">while</span> <span class="token keyword">not</span> converged<span class="token punctuation">:</span>
  Assign points to the <span class="token class-name">nearest</span> centroid<span class="token punctuation">;</span>
  Update <span class="token class-name">centroid</span> locations<span class="token punctuation">;</span>
End <span class="token keyword">while</span><span class="token punctuation">;</span>
</code></pre>
                        </dd>
                        <dt>Cluster Assignment and Centroid Update</dt>
                        <dd>
                            After initialization, the algorithm assigns data points to the nearest centroid based on Euclidean distance. The centroids are then updated by calculating the mean of the points in each cluster. This process is repeated until the centroids stabilize.
                            <pre class="language-csharp"><code class="language-csharp"><span class="token comment">// Pseudocode for cluster assignment and update</span>
<span class="token keyword">for</span> <span class="token class-name">each</span> point <span class="token keyword">in</span> dataset<span class="token punctuation">:</span>
  Assign point to <span class="token class-name">nearest</span> centroid<span class="token punctuation">;</span>
<span class="token class-name">Update</span> centroids<span class="token punctuation">;</span>
Repeat until centroids no <span class="token class-name">longer</span> change<span class="token punctuation">;</span>
</code></pre>
                        </dd>
                        <dt>Sum of Squared Errors (SSE)</dt>
                        <dd>
                            K-means aims to minimize the sum of squared errors (SSE), which is the total squared distance between points and their centroids. The lower the SSE, the better the clustering result.
                            <pre class="language-csharp"><code class="language-csharp"><span class="token comment">// Formula for SSE</span>
SSE <span class="token operator">=</span> <span class="token function">sum</span><span class="token punctuation">(</span><span class="token punctuation">(</span>x <span class="token operator">-</span> centroid<span class="token punctuation">)</span><span class="token operator">^</span><span class="token number">2</span> <span class="token keyword">for</span> <span class="token class-name">all</span> x <span class="token keyword">in</span> cluster<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
                        </dd>
                        <dt>Choosing the Optimal Number of Clusters</dt>
                        <dd>Two common methods for selecting the optimal number of clusters are: - **Elbow Method:** Plot SSE for different values of 𝑘 and look for the "elbow," where adding more clusters no longer significantly reduces SSE. - **Silhouette Score:** Measures how similar a point is to its own cluster compared to other clusters. The higher the score, the better the clustering.</dd>
                        <dt>Scaling Data for K-Means</dt>
                        <dd>Scaling is critical in K-means to ensure that all features contribute equally to the distance calculations. Data should be scaled, especially when the features have different units or scales, to prevent larger-scale features from dominating the clustering.</dd>
                    </dl>
                    <div class="row">
                        <div class="col-12">
                            <div class="accordion" id="faqAccordion">
                                <div class="accordion-item">
                                    <span class="accordion-header" id="headingOne"><button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseOne" aria-expanded="false" aria-controls="collapseOne">Q1: Why might different runs of K-means produce slightly different results?</button></span>
                                    <div class="accordion-collapse collapse" id="collapseOne" aria-labelledby="headingOne" data-bs-parent="#faqAccordion"><div class="accordion-body">K-means relies on random initialization of centroids, which can lead to different clustering results. Running the algorithm multiple times and selecting the result with the lowest SSE can address this issue.</div></div>
                                </div>
                                <div class="accordion-item">
                                    <span class="accordion-header" id="headingTwo"><button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseTwo" aria-expanded="false" aria-controls="collapseTwo">Q2: Does K-means automatically scale the data during the iterations?</button></span>
                                    <div class="accordion-collapse collapse" id="collapseTwo" aria-labelledby="headingTwo" data-bs-parent="#faqAccordion"><div class="accordion-body">No, K-means does not scale data automatically. It is necessary to scale the data manually before applying the algorithm.</div></div>
                                </div>
                                <div class="accordion-item">
                                    <span class="accordion-header" id="headingThree"><button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseThree" aria-expanded="false" aria-controls="collapseThree">Q3: How does K-means compute the centroids for clusters?</button></span>
                                    <div class="accordion-collapse collapse" id="collapseThree" aria-labelledby="headingThree" data-bs-parent="#faqAccordion"><div class="accordion-body">The centroids are calculated by taking the mean of all points assigned to each cluster after every iteration.</div></div>
                                </div>
                                <div class="accordion-item">
                                    <span class="accordion-header" id="headingFour"><button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseFour" aria-expanded="false" aria-controls="collapseFour">Q4: Why is it important to specify the number of clusters in K-means?</button></span>
                                    <div class="accordion-collapse collapse" id="collapseFour" aria-labelledby="headingFour" data-bs-parent="#faqAccordion"><div class="accordion-body">K-means requires specifying the number of clusters (𝑘) beforehand, as this determines how many centroids are initialized and how the data is partitioned.</div></div>
                                </div>
                                <div class="accordion-item">
                                    <span class="accordion-header" id="headingFive"><button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseFive" aria-expanded="false" aria-controls="collapseFive">Q5: What happens if the number of clusters is equal to the number of data points?</button></span>
                                    <div class="accordion-collapse collapse" id="collapseFive" aria-labelledby="headingFive" data-bs-parent="#faqAccordion"><div class="accordion-body">If 𝑘 equals the number of data points, each point will be its own cluster, and the SSE will be zero.</div></div>
                                </div>
                                <div class="accordion-item">
                                    <span class="accordion-header" id="headingSix"><button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSix" aria-expanded="false" aria-controls="collapseSix">Q6: Is K-means sensitive to outliers?</button></span>
                                    <div class="accordion-collapse collapse" id="collapseSix" aria-labelledby="headingSix" data-bs-parent="#faqAccordion"><div class="accordion-body">Yes, K-means can be sensitive to outliers, as they can heavily influence the centroids. Handling outliers before applying K-means is important, or consider using other algorithms like K-medoids.</div></div>
                                </div>
                                <div class="accordion-item">
                                    <span class="accordion-header" id="headingSeven"><button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSeven" aria-expanded="false" aria-controls="collapseSeven">Q7: What kind of distance metric does K-means use?</button></span>
                                    <div class="accordion-collapse collapse" id="collapseSeven" aria-labelledby="headingSeven" data-bs-parent="#faqAccordion"><div class="accordion-body">K-means typically uses Euclidean distance for measuring the distance between points and centroids.</div></div>
                                </div>
                                <div class="accordion-item">
                                    <span class="accordion-header" id="headingEight"><button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseEight" aria-expanded="false" aria-controls="collapseEight">Q8: Can K-means be used for non-spherical clusters?</button></span>
                                    <div class="accordion-collapse collapse" id="collapseEight" aria-labelledby="headingEight" data-bs-parent="#faqAccordion"><div class="accordion-body">K-means works best for spherical clusters with relatively uniform variance. For non-spherical clusters, other algorithms like DBSCAN may be more appropriate.</div></div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <dl>
                        <dt>Dimensionality Reduction with PCA</dt>
                        <dd><p>I applied Principal Component Analysis (PCA) to reduce the complexity of the data. PCA transforms the data into new dimensions, called principal components, which capture the most significant patterns. This allowed me to visualize nutrient relationships in just two dimensions, preserving about 68% of the data’s variance, or information.</p></dd>
                        <dt>Segmentation Using K-means Clustering</dt>
                        <dd><p>I applied K-means clustering, a technique that divides data into clusters based on their similarity. To determine the optimal number of clusters, I plotted an elbow curve using the inertia (sum of squared distances within clusters). The “elbow point” at three clusters indicated a good balance. I confirmed this choice with the silhouette score, which measures how well-separated the clusters are; a score closer to 1 suggests distinct and well-defined clusters.</p></dd>
                    </dl>
                    <h3>Key Concepts</h3>
                    <p>This project provided valuable experience with PCA and K-means clustering. While Google Colab allowed me to work interactively, PCA simplified the data by highlighting major patterns, and K-means grouped the data into meaningful clusters, which could be helpful for future analyses like customer segmentation or recommendation systems.</p>
                    <div class="card">
                        <div class="card-header"><h3 class="card-title">Principal Component Analysis (PCA)</h3></div>
                        <div class="card-body"></div>
                        <p>Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms data into a smaller number of uncorrelated variables (principal components) that capture the main patterns in the data.</p>
                        <p>Imagine you have a lot of data, like test scores in math, science, history, and art. It can be hard to see the big picture when you have many different variables, especially if there are a lot of patterns and connections between them. PCA is a tool that helps you simplify this information. It takes a big set of data with many variables and reduces it to a smaller set of "components" that still capture the most important information.</p>
                        <dl>
                            <dt>Principal Component Analysis (PCA)</dt>
                            <dd></dd>
                            <p>Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms data into a smaller number of uncorrelated variables (principal components) that capture the main patterns in the data.</p>
                            <p>Imagine you have a lot of data, like test scores in math, science, history, and art. It can be hard to see the big picture when you have many different variables, especially if there are a lot of patterns and connections between them. PCA is a tool that helps you simplify this information. It takes a big set of data with many variables and reduces it to a smaller set of "components" that still capture the most important information.</p>
                            <dt>How PCA Works</dt>
                            <dd></dd>
                            <p>PCA works by looking for patterns in the data. It finds directions (or components) in which the data varies the most. These components are like new axes, and they represent combinations of the original variables. Here’s a step-by-step look at how PCA simplifies data:</p>
                            <ol>
                                <li>Find the Directions of Maximum Variance: PCA looks for the direction in the data where the points are spread out the most. This direction is called the first principal component. It shows the main pattern in the data.</li>
                                <li>Add More Components: After finding the first principal component, PCA looks for the next direction where the data varies the most, but at a right angle to the first one. This is the second principal component. These components are always at right angles (90 degrees) to each other, making them uncorrelated.</li>
                                <li>Keep the Important Components: You can add as many components as there are variables, but most of the time, only the first few components capture the main patterns. By keeping only the most important ones, you reduce the data’s complexity.</li>
                            </ol>
                            <dt>Example with Simple Data</dt>
                            <dd></dd>
                            <p>Imagine a dataset with students' scores in Math and Science. If there’s a strong relationship (students who do well in Math also do well in Science), PCA would combine these scores into one main component that represents both subjects.</p>
                            <ol>
                                <li>Original Data: Math and Science Scores</li>
                                <li>First Principal Component: Overall Academic Ability</li>
                                <li>Second Principal Component: Difference Between Math and Science</li>
                            </ol>
                            <p>So, instead of tracking two scores separately, PCA would give you an “Overall Academic Ability” score that captures the main pattern, simplifying the data.</p>
                            <dt>Why PCA is Useful</dt>
                            <dd></dd>
                            <p>PCA has several benefits for analyzing data:</p>
                            <ol>
                                <li>Reduces Complexity: PCA simplifies data by reducing the number of variables you need to look at. You keep the main patterns and ignore the rest, making the data easier to understand.</li>
                                <li>Finds Patterns: PCA reveals hidden patterns by focusing on the directions where the data varies the most.</li>
                                <li>Helps with Visualization: If you have data with many variables, PCA can reduce it to two or three components, making it possible to plot and visualize.</li>
                            </ol>
                        </dl>
                        <p>In summary, PCA is a way to look at the big picture of your data by focusing on what’s most important. It turns a complex dataset into something simpler, allowing you to see patterns and relationships that would otherwise be hidden.</p>
                        <div class="card-footer"><a class="btn btn-primary" href="https://en.wikipedia.org/wiki/Principal_component_analysis" target="_blank" rel="nofollow" title="Learn more about Principal Component Analysis on Wikipedia" alt="Wikipedia on PCA">Wikipedia on PCA</a></div>
                        <br />
                        <hr />
                        <br />
                        <div class="card">
                            <div class="card-header"><h3 class="card-title">Principal Components</h3></div>
                            <div class="card-body"></div>
                            <p>Principal Components are the new variables created by PCA that represent the directions of maximum variance in the data. Each principal component is a linear combination of the original variables, and they are uncorrelated with each other.</p>
                            <p>Here’s a more detailed look at Principal Components:</p>
                            <dl>
                                <dt>First Principal Component</dt>
                                <dd></dd>
                                <p>The first principal component captures the largest amount of variance in the data. It is the direction in which the data varies the most. This component often represents the most significant pattern in the dataset.</p>
                                <dt>Second Principal Component</dt>
                                <dd></dd>
                                <p>The second principal component captures the second largest amount of variance, but it is orthogonal (at a right angle) to the first component. This ensures that it represents a different pattern in the data.</p>
                                <dt>Subsequent Principal Components</dt>
                                <dd></dd>
                                <p>Each subsequent principal component captures the next highest variance while being orthogonal to all previous components. These components continue to represent new patterns in the data, but each captures less variance than the previous one.</p>
                                <dt>Eigenvalues and Eigenvectors</dt>
                                <dd></dd>
                                <p>Principal components are derived from the eigenvectors of the covariance matrix of the data. The eigenvalues associated with these eigenvectors indicate the amount of variance captured by each principal component.</p>
                                <dt>Interpretation</dt>
                                <dd></dd>
                                <p>Interpreting principal components involves looking at the coefficients of the original variables in each component. These coefficients indicate the contribution of each variable to the component, helping to understand the underlying patterns.</p>
                            </dl>
                            <p>In summary, Principal Components are essential in PCA as they transform the data into a new set of variables that capture the most significant patterns, making it easier to analyze and visualize complex datasets.</p>
                            <div class="card-footer"><a class="btn btn-primary" href="https://en.wikipedia.org/wiki/Principal_component_analysis#Details" target="_blank" rel="nofollow" title="Learn more about Principal Components on Wikipedia" alt="Wikipedia on Principal Components">Wikipedia on Principal Components</a></div>
                        </div>
                        <br />
                        <hr />
                        <br />
                        <h2>Glossary of Terms</h2>
                        <dl>
                            <dt>Google Colab</dt>
                            <dd></dd>
                            A free, browser-based environment for writing and running Python code in Jupyter notebooks, particularly popular for data science projects.
                            <dt>Jupyter Notebook</dt>
                            <dd></dd>
                            An interactive coding environment that allows you to combine code, visualizations, and text, commonly used in data science.
                            <dt>Dendrogram</dt>
                            <dd></dd>
                            A tree-like diagram used to illustrate the arrangement of clusters formed by hierarchical clustering.
                            <dt>Elbow Curve</dt>
                            <dd></dd>
                            A plot used to determine the optimal number of clusters in K-means by finding a point where the decrease in variance slows, resembling an "elbow."
                            <dt>Inertia</dt>
                            <dd></dd>
                            The sum of squared distances between data points and their cluster centroids. Lower inertia indicates tighter clusters.
                            <dt>Silhouette Score</dt>
                            <dd></dd>
                            A measure of how similar each point is to its cluster compared to other clusters, with values close to 1 indicating well-separated clusters.
                            <dt>Dimensionality Reduction</dt>
                            <dd></dd>
                            The process of reducing the number of variables in a dataset while retaining as much information as possible.
                        </dl>
                    </div>
                </div>
            </article>
        </div>
        <footer class="navbar-dark bg-primary">
            <div class="row">
                <div class="col-1"><br /></div>
                <div class="col-10 text-left">
                    <br />
                    <!-- Ensure current article exists-->
                    <!-- Extract and clean currentKeywords-->
                    <!-- Only check for related articles if there are valid keywords-->
                    <!-- Shuffle related articles and limit to 3-->
                    <!-- Render current article's keywords as badges-->
                    <div class="keywords mt-3">
                        <p class="text-white">Hashtags:</p>
                        <div class="d-flex flex-wrap gap-2">
                            <span class="badge bg-primary text-uppercase">k-means clustering</span>
                            <span class="badge bg-primary text-uppercase">unsupervised learning</span>
                            <span class="badge bg-primary text-uppercase">clustering algorithm</span>
                            <span class="badge bg-primary text-uppercase">centroids</span>
                            <span class="badge bg-primary text-uppercase">data points</span>
                            <span class="badge bg-primary text-uppercase">customer segmentation</span>
                            <span class="badge bg-primary text-uppercase">market analysis</span>
                            <span class="badge bg-primary text-uppercase">image recognition</span>
                            <span class="badge bg-primary text-uppercase">medical diagnostics</span>
                            <span class="badge bg-primary text-uppercase">partitioning data</span>
                            <span class="badge bg-primary text-uppercase">clusters</span>
                            <span class="badge bg-primary text-uppercase">similarities</span>
                            <span class="badge bg-primary text-uppercase">variance</span>
                            <span class="badge bg-primary text-uppercase">student grades</span>
                            <span class="badge bg-primary text-uppercase">random initialization</span>
                            <span class="badge bg-primary text-uppercase">sum of squared errors</span>
                            <span class="badge bg-primary text-uppercase">sse</span>
                            <span class="badge bg-primary text-uppercase">elbow method</span>
                            <span class="badge bg-primary text-uppercase">silhouette score</span>
                            <span class="badge bg-primary text-uppercase">scaling data</span>
                            <span class="badge bg-primary text-uppercase">euclidean distance</span>
                            <span class="badge bg-primary text-uppercase">non-spherical clusters</span>
                            <span class="badge bg-primary text-uppercase">dbscan</span>
                            <span class="badge bg-primary text-uppercase">outliers</span>
                            <span class="badge bg-primary text-uppercase">dimensionality reduction</span>
                            <span class="badge bg-primary text-uppercase">principal component analysis</span>
                            <span class="badge bg-primary text-uppercase">pca</span>
                            <span class="badge bg-primary text-uppercase">principal components</span>
                            <span class="badge bg-primary text-uppercase">eigenvalues</span>
                            <span class="badge bg-primary text-uppercase">eigenvectors</span>
                            <span class="badge bg-primary text-uppercase">google colab</span>
                            <span class="badge bg-primary text-uppercase">jupyter notebook</span>
                            <span class="badge bg-primary text-uppercase">dendrogram</span>
                            <span class="badge bg-primary text-uppercase">elbow curve</span>
                            <span class="badge bg-primary text-uppercase">inertia</span>
                            <span class="badge bg-primary text-uppercase">nutrient data</span>
                            <span class="badge bg-primary text-uppercase">kaggle</span>
                            <span class="badge bg-primary text-uppercase">data exploration</span>
                            <span class="badge bg-primary text-uppercase">machine learning</span>
                            <span class="badge bg-primary text-uppercase">data analysis</span>
                            <span class="badge bg-primary text-uppercase">recommendation systems</span>
                            <span class="badge bg-primary text-uppercase">visualization</span>
                            <span class="badge bg-primary text-uppercase">data patterns</span>
                            <span class="badge bg-primary text-uppercase">nutrient segmentation</span>
                            <span class="badge bg-primary text-uppercase">optimal clustering</span>
                            <span class="badge bg-primary text-uppercase">cluster assignment</span>
                            <span class="badge bg-primary text-uppercase">centroid update</span>
                            <span class="badge bg-primary text-uppercase">data scaling</span>
                            <span class="badge bg-primary text-uppercase">data transformation</span>
                            <span class="badge bg-primary text-uppercase">pattern recognition.</span>
                        </div>
                    </div>
                    <!-- Render related articles in a card layout-->
                    <div class="related-articles mt-4">
                        <h3 class="text-white">Related Articles</h3>
                        <div class="row row-cols-1 row-cols-md-2 row-cols-lg-3 g-4">
                            <div class="col">
                                <div class="card h-100 bg-dark text-white">
                                    <div class="card-body">
                                        <h5 class="card-title">
                                            <i class="bi bi-arrow-right-circle-fill me-2"></i>
                                            <a class="text-white" href="/articles/an-introduction-to-neural-networks.html" title="A beginner-friendly introduction to neural networks, explaining key concepts and their role in artificial intelligence.">An Introduction to Neural Networks</a>
                                        </h5>
                                        <p class="card-text text-white">A beginner-friendly introduction to neural networks, explaining key concepts and their role in artificial intelligence.</p>
                                    </div>
                                    <div class="card-footer"></div>
                                </div>
                            </div>
                            <div class="col">
                                <div class="card h-100 bg-dark text-white">
                                    <div class="card-body">
                                        <h5 class="card-title">
                                            <i class="bi bi-arrow-right-circle-fill me-2"></i>
                                            <a class="text-white" href="/articles/english-is-the-new-programming-language-of-choice.html" title="Explore the pivotal role of English in the evolution of the Microsoft Stack, specifically within .NET technologies such as C#, F#, VB.NET, and SQL. Understand why English is becoming as crucial as traditional programming languages in software development.">English is the New Programming Language of Choice</a>
                                        </h5>
                                        <p class="card-text text-white">Explore the pivotal role of English in the evolution of the Microsoft Stack, specifically within .NET technologies such as C#, F#, VB.NET, and SQL. Understand why English is becoming as crucial as traditional programming languages in software development.</p>
                                    </div>
                                    <div class="card-footer"></div>
                                </div>
                            </div>
                            <div class="col">
                                <div class="card h-100 bg-dark text-white">
                                    <div class="card-body">
                                        <h5 class="card-title">
                                            <i class="bi bi-arrow-right-circle-fill me-2"></i>
                                            <a class="text-white" href="/articles/ai-observability-is-no-joke.html" title="A  humorous look at AI observability through the lens of a joke-fetching agent. Learn why knowing what your AI did matters.">AI Observability Is No Joke</a>
                                        </h5>
                                        <p class="card-text text-white">A humorous look at AI observability through the lens of a joke-fetching agent. Learn why knowing what your AI did matters.</p>
                                    </div>
                                    <div class="card-footer"></div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="mt-4"><p class="text-white"></p></div>
                </div>
            </div>
            <div class="row">
                <div class="col-1"><br /></div>
            </div>
            <div class="row">
                <div class="col-1"><br /></div>
                <div class="col-10 justify-content-end">
                    <br />
                    <div class="social-icons d-flex">
                        <a class="social-icon" href="https://www.linkedin.com/in/markhazleton" target="_blank" rel="noopener noreferrer" title="LinkedIn Profile"><i class="fab fa-linkedin-in"></i></a>
                        <a class="social-icon" href="https://github.com/markhazleton" target="_blank" rel="noopener noreferrer" title="GitHub Profile"><i class="fab fa-github"></i></a>
                        <a class="social-icon" href="https://www.youtube.com/@MarkHazleton" target="_blank" rel="noopener noreferrer" title="YouTube Channel"><i class="fab fa-youtube"></i></a>
                        <a class="social-icon" href="https://www.instagram.com/markhazleton/" target="_blank" rel="noopener noreferrer" title="Instagram Profile"><i class="fab fa-instagram"></i></a>
                        <a class="social-icon" href="https://markhazleton.com/rss.xml" target="_blank" rel="noopener noreferrer" title="RSS Feed"><i class="fas fa-rss"></i></a>
                        <br />
                    </div>
                </div>
            </div>
            <div class="row">
                <div class="col-12"><br /></div>
            </div>
        </footer>
        <!-- Core theme JS-->
        <script src="/js/scripts.js"></script>
    </body>
</html>
